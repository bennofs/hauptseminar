* 1: why machine learning
** what is the type of this variable: var counter
*** classical approaches focused on correctness like type inference may not be able to solve this
*** solution is never going to be 100% exact
*** but still useful: recomendations, or replacing heuristics and checking later
** machine learning provides a way to solve this in a well-founded way: natural code hypothesis
#+BEGIN_QUOTE
*The naturalness hypothesis*

Software is a form of human communication;
software corpora have similar statistical properties to natural language corpora;
and these properties can be exploited to build better software engineering tools.
#+END_QUOTE
* 2: models
** sequence based
** ast based
** graph based
** behaviour based (dynamic)
* 3: graph based embedding
** inference graph
** graph neural networks
* a different angle: using runtime data
** API path mining (multiple)
** Loop approximation (Yao et al)
** Dypro, Liger (Wang et al)
* evaluation: tasks
hard to compare different models because they focus on different tasks
(BACKUP: table with models and tasks)

** method name prediction
** type prediction
** loop invariant generation
** semantic label prediction
* evaluation: type prediction
** task: type prediction
** papers: typilus, code2vec, bielik adversarial
* evaluation: method name prediction
* evaluation: semantic label prediction (CoSET)
* evaluation: loop invariant generation
* research directions
** understand *how*/*what* models learn
*** are some models better for some tasks?
*** dependence on language features?
*** do adversarial examples differ?
*** a unified benchmark/task set would be nice (COSET is unfortunately not public yet)
** new architectures
*** general pretrained models that can be finetuned? (cf BERT, GPT, XLNet)
*** combine dynamic with static features?
* conclusion
** active research in structured models
** lots of open questions
** more data would enable better evaluation (how about a google recaptcha for algorithms?)
** but still, many cool applications (maybe too many?)
* literature
:PROPERTIES:
:CATEGORY: hauptseminar-papers
:END:
** DONE Learning Nonlinear Loop Invariants with Gated Continous Logic Networks :dynamic:model:
Verifying real-world programs often requires inferring loop
invariants with nonlinear constraints. This is especially true
in programs that perform many numerical operations, such
as control systems for avionics or industrial plants. Recently,
data-driven methods for loop invariant inference have shown
promise, especially on linear loop invariants. However, ap-
plying data-driven inference to nonlinear loop invariants is
challenging due to the large numbers of and large magni-
tudes of high-order terms, the potential for overfitting on
a small number of samples, and the large space of possible
nonlinear inequality bounds.

In this paper, we introduce a new neural architecture for
general SMT learning, the Gated Continuous Logic Network
(G-CLN), and apply it to nonlinear loop invariant learning.
G-CLNs extend the Continuous Logic Network (CLN) archi-
tecture with gating units and dropout, which allow the model
to robustly learn general invariants over large numbers of
terms. To address overfitting that arises from finite program
sampling, we introduce fractional sampling—a sound relax-
ation of loop semantics to continuous functions that facili-
tates unbounded sampling on the real domain. We addition-
ally design a new CLN activation function, the Piecewise
Biased Quadratic Unit (PBQU), for naturally learning tight
inequality bounds.

We incorporate these methods into a nonlinear loop in-
variant inference system that can learn general nonlinear
loop invariants. We evaluate our system on a benchmark of
nonlinear loop invariants and show it solves 26 out of 27
problems, 3 more than prior work, with an average runtime
of 53.3 seconds. We further demonstrate the generic learning
ability of G-CLNs by solving all 124 problems in the linear
Code2Inv benchmark. We also perform a quantitative stabil-
ity evaluation and show G-CLNs have a convergence rate
of 97.5% on quadratic problems, a 39.2% improvement over
CLN models.
*** motivation: program verification requires loop invariants
*** research area: loop invariant inference using data-driven methods
*** method
**** use ML technologies (blackbox optimization) to fit a GCLN to runtime data of loops
**** basic idea: train a "network" that automatically learns which terms are important
**** derive loop invariants from "learned" parameters
**** use SMT checker to verify (since not all learned invariants are valid)
*** comparision to previous works
**** more general: can be applied to arbitrary non-linear invariants
**** does not require templates
*** fractional sampling
** CLN2INV: Learning Loop Invariants with Continuous Logic Networks
Program verification offers a framework for ensuring program correctness and
therefore systematically eliminating different classes of bugs.
Inferring loop invariants is one of the main challenges behind automated verification
of real-world programs, which often contain many loops.

In this paper, we present the Continuous Logic Network (CLN),
a novel neural architecture for automatically learning loop invariants
directly from program execution traces. Unlike existing neural networks,
CLNs can learn precise and explicit representations of formulas in
Satisfiability Modulo Theories (SMT) for loop invariants from program execution traces.
We develop a new sound and complete semantic mapping for assigning SMT formulas
to continuous truth values that allows CLNs to be trained efficiently.

We use CLNs to implement a new inference system for loop invariants, CLN2INV, that
significantly outperforms existing approaches on the popular Code2Inv dataset.
CLN2INV is the first tool to solve all 124 theoretically solvable problems in the
Code2Inv dataset. Moreover, CLN2INV takes only 1.1 second on average for each
problem, which is 40× faster than existing approaches. We further demonstrate
that CLN2INV can even learn 12 significantly more complex loop invariants than
the ones required for the Code2Inv dataset.
** DONE ML in compiler optimization :tuning:survey:
In the last decade, machine-learning-based
compilation has moved from an obscure research niche to a
mainstream activity.

In this paper, we describe the relationship
between machine learning and compiler optimization and
introduce the main concepts of features, models, training,
and deployment.

We then provide a comprehensive survey and provide a road map
for the wide variety of different research areas.
We conclude with a discussion on open issues in the area
and potential research directions.

This paper provides both an accessible introduction
to the fast moving area of machine-learning-based compilation
and a detailed bibliography of its main achievements.
*** describes models, feature engineering, applications and future directions
*** most usages seem to be focused on "tuning": tweaking the order or parameters of existing optimizations
*** not much references to work on program analysis for optimization using ML
** DONE A Survey of Machine Learning for Big Code and Naturalness :source:survey:
Research at the intersection of machine learning, programming languages,
and software engineering has recently taken important steps
in proposing learnable probabilistic models of source code
that exploit the abundance of patterns of code.

In this article, we survey this work. We contrast programming languages
against natural languages and discuss how these similarities and differences
drive the design of probabilistic models.

We present a taxonomy based on the underlying design principles of each model
and use it to navigate the literature. Then, we review how researchers have
adapted these models to application areas and discuss cross-cutting and
application-specific challenges and opportunities.
*** focused on Source Code as input (not dynamic features)
*** different kinds of models: code-generating models, representation models, pattern mining models
*** representation models seem interesting
*** program analysis references:
8 Learning to represent Programs with graphs
31 Learning Shape Analysis
38 Automatically generating features for learning program analysis heuristics for C-like languages
93 Using web corpus statistics for program analysis
106 Learning a classifier for false positive error reports emitted by static code analysis tools
115 Gated Graph Sequence Neural Networks
127 [[A User-Guided Approach to Program Analysis]]
147 Learning a strategy for adapting a program analysis via a Bayesian optimization
157 Deep Learning to find bugs
165 Predicting program properties from big code
** A User-Guided Approach to Program Analysis
Program analysis tools often produce undesirable output
due to various approximations. We present an approach
and a system Eugene that allows user feedback to guide
such approximations towards producing the desired output.
We formulate the problem of user-guided program analy-
sis in terms of solving a combination of hard rules and soft
rules: hard rules capture soundness while soft rules capture
degrees of approximations and preferences of users. Our
technique solves the rules using an off-the-shelf solver in a
manner that is sound (satisfies all hard rules), optimal (max-
imally satisfies soft rules), and scales to real-world analy-
ses and programs. We evaluate Eugene on two different
analyses with labeled output on a suite of seven Java pro-
grams of size 131–198 KLOC. We also report upon a user
study involving nine users who employ Eugene to guide an
information-flow analysis on three Java micro-benchmarks.
In our experiments, Eugene significantly reduces misclassi-
fied reports upon providing limited amounts of feedback.
** Probabilistic model for code with decision trees
In this paper we introduce a new approach for learning precise and general probabilistic models of code based on decision tree learning. Our approach directly benefits an emerging class of statistical programming tools which leverage probabilistic models of code learned over large codebases (e.g., GitHub) to make predictions about new programs (e.g., code completion, repair, etc).

The key idea is to phrase the problem of learning a probabilistic model of code as learning a decision tree in a domain specific language over abstract syntax trees (called TGen). This allows us to condition the prediction of a program element on a dynamically computed context. Further, our problem formulation enables us to easily instantiate known decision tree learning algorithms such as ID3, but also to obtain new variants we refer to as ID3+ and E13, not previously explored and ones that outperform ID3 in prediction accuracy.

Our approach is general and can be used to learn a probabilistic model of any programming language. We implemented our approach in a system called Deep3 and evaluated it for the challenging task of learning probabilistic models of JavaScript and Python. Our experimental results indicate that Deep3 predicts elements of JavaScript and Python code with precision above 82% and 69%, respectively. Further, Deep3 often significantly outperforms state-of-the-art approaches in overall prediction accuracy.
** DONE [[file:ref.bib::menendez17_alive_infer][Menendez & Nagarakatte 2017: Alive-Infer: data-driven precondition inference for peephole optimizations in LLVM]]
Peephole optimizations are a common source of compiler bugs.
Compiler developers typically transform an incorrect peephole optimization into a valid one by strengthening the precondition.
This process is challenging and tedious.

This paper proposes Alive-Infer, a data-driven approach that infers preconditions for peephole optimizations expressed in Alive.
Alive-Infer generates positive and negative examples for an optimization, enumerates predicates on-demand,
and learns a set of predicates that separate the positive and negative examples.
Alive-Infer repeats this process until it finds a precondition that ensures the validity of the optimization.
Alive-Infer reports both a weakest precondition and a set of succinct partial preconditions to the developer.

Our prototype generates preconditions that are weaker than LLVM’s preconditions for 73 optimizations in the Alive suite.
We also demonstrate the applicability of this technique to generalize 54 optimization patterns generated by Souper, an LLVM IR–based superoptimizer.
*** application of PIE to llvm with some real-world engineering challenges
** [[file:ref.bib::Raychev_2019][Raychev et al. 2019: Predicting program properties from “big code”]] :static:graphs:
We present a new approach for predicting program properties from large codebases (aka "Big Code"). Our approach learns a probabilistic model from "Big Code" and uses this model to predict properties of new, unseen programs.

The key idea of our work is to transform the program into a representation that allows us to formulate the problem of inferring program properties as structured prediction in machine learning. This enables us to leverage powerful probabilistic models such as Conditional Random Fields (CRFs) and perform joint prediction of program properties.

As an example of our approach, we built a scalable prediction engine called jsnice for solving two kinds of tasks in the context of JavaScript: predicting (syntactic) names of identifiers and predicting (semantic) type annotations of variables. Experimentally, JSNICE predicts correct names for 63% of name identifiers and its type annotation predictions are correct in 81% of cases. Since its public release at http://jsnice.org, JSNice has become a popular system with hundreds of thousands of uses.

By formulating the problem of inferring program properties as structured prediction, our work opens up the possibility for a range of new "Big Code" applications such as de-obfuscators, decompilers, invariant generators, and others.
*** using conditional random fields
*** represent program as inference network
** [[file:ref.bib::DBLP:journals/corr/abs-1910-07517][Yefet et al. 2019: Adversarial Examples for Models of Code]]
*** abstract
Neural models of code have shown impressive performance for tasks such as predicting method names and identifying certain kinds of bugs.
In this paper, we show that these models are vulnerable to adversarial examples, and introduce a novel approach for attacking trained models of code with adversarial examples.
The main idea is to force a given trained model to make an incorrect prediction as specified by the adversary by introducing small perturbations that do not change the program’s semantics.
To find such perturbations, we present a new technique for Discrete Adversarial Manipulation of Programs (DAMP).
DAMP works by deriving the desired prediction with respect to the model’s inp uts while holding the model weights constant, and following the gradients to slightly modify the input code.

We show that our DAMP attack is effective across three neural architectures: code2vec, GGNN, and GNN-FiLM, in both Java and C#.
We show that DAMP has up to 89% success rate in changing a prediction to the adversary’s choice (“targeted attack”),
and a success rate of up to 94% in changing a given prediction to any incorrect prediction (“non-targeted attack”).
To defend a model against such attacks, we examine a variety of possible defenses empirically and discuss their trade-offs.
We show that some of these defenses drop the success rate of the attacker drastically, with a minor penalty of 2% relative degradation in accuracy while not performing under attack.
*** two mutations: variable renaming (single var) or inserting a new, unused variable statement
*** code2vec: non-targeted robustness only 6%, targeted 10.39% - 97.89%, can achieve ~90% F1 without var names
*** ggnn and gnn-film: higher robustness (57%-87.62%), where gnn-film > ggnn
*** also describe defense
*** interesting: transferability of adversarial samples
**** "We also did not find significant evidence that adversarial examples transfer across models that were trained on the same dataset, e.g., from GNN-FiLM to GGNN. "
**** "Occasionally, a dead code attack is transferable across example – it has the same effect even in different examples. This is demonstrated in Figure 10: adding the unused variable declaration int introsorter = 0;"
** [[file:ref.bib::ramakrishnan20:_seman_robus_model_sourc_code][Ramakrishnan et al. 2020: Semantic Robustness Models Source Code]]
*** abstract
Deep neural networks are vulnerable to adversarial examples - small input perturbations that result in incorrect predictions.
We study this problem for models of source code, where we want the network to be robust to source-code modifications that preserve code functionality.
(1) We define a powerful adversary that can employ sequences of parametric, semantics-preserving program transformations;
(2) we show how to perform adversarial training to learn models robust to such adversaries;
(3) we conduct an evaluation on different languages and architectures, demonstrating significant quantitative gains in robustness.
*** concept of k-robustness, define program transformations with holes which are then gradient optimized (as in bielik et al)
*** adversarial training improves robustness more for seq2seq than for code2seq => maybe code2seq already includes certain "robustness" by modeling assumption?
*** seq2seq can be better than code2seq in adversarial setting
*** compared models: seq2seq and code2seq
** [[file:ref.bib::DBLP:conf/sigsoft/HenkelLLR18][Henkel et al. 2018: Code vectors]] :dynamic:symbolic:representation:
*** abstract
With the rise of machine learning, there is a great deal of interest in treating programs as data to be fed to learning algorithms. However, programs do not start off in a form that is immediately amenable to most off-the-shelf learning techniques. Instead, it is necessary to transform the program to a suitable representation before a learning technique can be applied. In this paper, we use abstractions of traces obtained from symbolic execution of a program as a representation for learning word embeddings. We trained a variety of word embeddings under hundreds of parameterizations, and evaluated each learned embedding on a suite of different tasks. In our evaluation, we obtain 93% top-1 accuracy on a benchmark consisting of over 19,000 API-usage analogies extracted from the Linux kernel. In addition, we show that embeddings learned from (mainly) semantic abstractions provide nearly triple the accuracy of those learned from (mainly) syntactic abstractions.
*** abstract symbolic traces and learn a word2vec like embedding
*** evaluation focuses on functions that are related in usage patterns (analogies)
*** analogy examples: alloc/free, lock/unlock, store16/store32
*** traces are biased towards start of function (if we think of function as tree)
*** dataflow appears to be quite important
*** study error code prediction, but evaluation poor (only accuracy, top3, no statistical baseline)
*** encodes "behaviour" of function with respect to how they are called, not how they are implemented
** [[file:ref.bib::DBLP:conf/pldi/AllamanisBDG20][Allamanis et al. 2020: Typilus: neural type hints]] :static:graph:representation:
*** transform symbols to "typespace" where similarly-typed symbols have close representation
*** learn using typilus loss, combination of a batched triplet loss (sum similar-dist - sum dissimilar-dist) and classification loss
*** to compute classification loss, project typespace into a parameter-less (no generics) space linearly (combats sparseness in classification without forcing generic types to be reduced to their base type)
*** comparision between sequence based (biLSTM), code2seq (AST path) and graph models (GGNN):
- GGNN outperform AST and biLSTM
- biLSTM better than AST (hypothesis: because biLSTM predicts all types at once (structured) and AST does single prediction)
*** significant improvement in prediction of rare types (meta-learning)
*** new references
- [20] [[*\[\[file:ref.bib::DBLP:conf/icml/CvitkovicSA19\]\[Cvitkovic et al. 2019: Open Vocabulary Learning on Source Code with a Graph-Structured Cache\]\]][Cvitkovic et al. 2019: Open Vocabulary Learning on Source Code with a Graph-Structured Cache]]
- [31] [[*\[\[file:ref.bib::DBLP:conf/sigsoft/HellendoornBBA18\]\[Hellendoorn et al. 2018: Deep learning type inference\]\]][Hellendoorn et al. 2018: Deep learning type inference]]
- [32] [[*\[\[file:ref.bib::DBLP:conf/sigsoft/HellendoornD17\]\[Hellendoorn & Devanbu 2017: Are deep neural networks the best choice for modeling source code?\]\]][Hellendoorn & Devanbu 2017: Are deep neural networks the best choice for modeling source code?]]
- [33] [[*\[\[file:ref.bib::DBLP:conf/pldi/HeoRSN19\]\[Heo et al. 2019: Continuously reasoning about programs using differential Bayesian inference\]\]][Heo et al. 2019: Continuously reasoning about programs using differential Bayesian inference]]
- [55] [[*\[\[file:ref.bib::DBLP:conf/nips/SiDRNS18\]\[Si et al. 2018: Learning Loop Invariants Program Verification\]\]][Si et al. 2018: Learning Loop Invariants Program Verification]]
** [[file:ref.bib::DBLP:conf/nips/Ben-NunJH18][Ben-Nun et al. 2018: Neural Code Comprehension (xfg, inst2vec)]] :static:graphs:representation:
*** abstract
With the recent success of embeddings in natural language processing, research has been conducted into applying similar methods to code analysis. Most works attempt to process the code directly or use a syntactic tree representation, treating it like sentences written in a natural language. However, none of the existing methods are sufficient to comprehend program semantics robustly, due to structural features such as function calls, branching, and interchangeable order of statements. In this paper, we propose a novel processing technique to learn code semantics, and apply it to a variety of program analysis tasks. In particular, we stipulate that a robust distributional hypothesis of code applies to both human- and machine-generated programs. Following this hypothesis, we define an embedding space, inst2vec, based on an Intermediate Representation (IR) of the code that is independent of the source programming language. We provide a novel definition of contextual flow for this IR, leveraging both the underlying data- and control-flow of the program. We then analyze the embeddings qualitatively using analogies and clustering, and evaluate the learned representation on three different high-level tasks. We show that even without fine-tuning, a single RNN architecture and fixed inst2vec embeddings outperform specialized approaches for performance prediction (compute device mapping, optimal thread coarsening); and algorithm classification from raw code (104 classes), where we set a new state-of-the-art.
** [[file:ref.bib::DBLP:journals/corr/abs-2004-00348][Pandi et al. 2020: OptTyper: Probabilistic Type Inference by Optimising Logical and Natural Constraints]] :static:cnn:
*** optimize natural constraints with respect to hard constraints (using continuous relaxation)
*** natural constraints: cnn on symbol name, hard constraints: some simple type constraints (same type, etc)
*** infer types for typescript
*** evaluation only compares accurracy, beats deeptyper, lambdanet and jsnice
** [[file:ref.bib::DBLP:journals/corr/abs-1912-03768][Pradel et al. 2019: TypeWriter]] :static:cnn:
*** use cnn on various information sources (names, comments, usages, available types) and feed forward to combine
*** to get consistent prediction, use greedy search
*** evaluation compares to nl2type and deeptyper
** [[file:ref.bib::DBLP:conf/icse/BrunE04][Brun & Ernst 2004: Finding Latent Code Errors via Machine Learning over Program Executions]]
*** apply machine learning to extracted properties to decide if they are indicative of fault or not
*** features: simple values extracted from properties (such as number of variables, constants etc)
*** use decision trees or svm as classifier
** [[file:ref.bib::DBLP:conf/iclr/WeiGDD20][Wei et al. 2020: LambdaNet]] :graph:
*** manually construct type dependency graph
*** use GNN with MLP message passing function and some attention
*** evaluated against deeptyper and jsnice
** [[file:ref.bib::DBLP:conf/cc/BrauckmannGEC20][Brauckmann et al. 2020: Compiler-based graph representations for deep learning models of code]] :graph:static:
*** argue for using graphs for learning on source code (since compiler representations use them)
*** normalize identifier names and code style
*** GNN+CDFG best if training on all benchmarks, GNN+AST generalizes better if split by benchmark
*** none of the models are good for thread corsening
** [[file:ref.bib::DBLP:journals/corr/abs-1903-06089][Hellendoorn et al. 2019: Are My Invariants Valid?]]
*** detect whether invariant generated by daikon is valid or not
*** to train: use invariants that hold over subset of testsuite -> noisy data
*** find that many daikon-invariants are invalid or irrelevant
*** model: ggnn on code + invariant
*** ablation study comparing to rnn
** [[file:ref.bib::DBLP:conf/nips/SiDRNS18][Si et al. 2018: Learning Loop Invariants Program Verification]] :static:graph:representation:
*** try to mimic human process during loop invariant generation
*** three parts: summarize code to external memory, autoregressive model for inv gen and attention for focus
*** ablation study: gnn beats lstm (106 vs 93 solved instances, less parameter updates for gnn)
** [[file:ref.bib::DBLP:journals/cacm/HindleBGS16][Hindle et al. 2016: On the naturalness of software]]
*** software is far more regular than English
*** compute cross-entropy of ngram model on software (java and c) projects
** [[file:ref.bib::DBLP:conf/icml/GilmerSRVD17][Gilmer et al. 2017: Neural Message Passing Quantum Chemistry]] :graph:
*** three parts: message function M_t(source,dest,edge), vertex update function U_t(state, sum M), readout function R
*** general framework for unifying different models in the literature
*** including source vertex in M has worse perf
*** future challenge: generalize from small to large graphs, better runtime
** [[file:ref.bib::DBLP:conf/icse/MalikPP19][Malik et al. 2019: NL2Type]]
*** architecture: feed word2vec embeddings (separate for comments and identifiers) to LSTM, classify T=1000 different types
*** compare to deeptyper on deeptyper's dataset (mosty difference in precision: 77.5% vs 68.6%)
** [[file:ref.bib::DBLP:conf/popl/RaychevBVK16][Raychev et al. 2016: Learning programs from noisy data]]
*** complicated paper, lots of theorems and definitions
*** learn a program that generates context for code completions using program synthesis
*** dataset extracted from GitHub, removing duplicate files or project forks
** [[file:ref.bib::DBLP:journals/corr/abs-1912-05097][Li et al. 2019: Using GGNN to recommend log statement level]]
*** basically use model from Allamanis 2018
*** output is MLP(hidden state of log statement node)
*** poor paper typesetting quality?
** [[file:ref.bib::DBLP:conf/sigsoft/HellendoornD17][Hellendoorn & Devanbu 2017: Are deep neural networks the best choice for modeling source code?]]
*** identify three challenges for ML on code: open vocabulary, nested/scoped/locality, dynamism
*** adapt model dynamically to current file/package/project context -> hard to do with neural networks
*** design dynamically updatable, nested scope, unlimited vocabulary ngram model -> far faster and better than DL
*** combination of DL + their model has even better perf
** [[file:ref.bib::DBLP:conf/icml/PiechHNPSG15][Piech et al. 2015: Learning Program Embeddings to Propagate Feedback on Student Code]]
*** predict embedding for pre/postcondition f such that program can be represented as f(Post) = M * f(Pre)
*** learn this embedding as an autoencoder
*** collect variable values (pre/post) for whole program as well as subtrees
*** the linear transformation matrix can be used as feature for the program
*** application: force multiplier for teacher feedback, by copying feedback to "similar" student solutions
*** evaluate on feedback generated by manually tuned script
** TODO [[file:ref.bib::DBLP:conf/sigsoft/HellendoornBBA18][Hellendoorn et al. 2018: Deep learning type inference]]
***
** TODO [[file:ref.bib::DBLP:conf/icml/CvitkovicSA19][Cvitkovic et al. 2019: Open Vocabulary Learning on Source Code with a Graph-Structured Cache]]
** TODO [[file:ref.bib::DBLP:conf/pldi/HeoRSN19][Heo et al. 2019: Continuously reasoning about programs using differential Bayesian inference]]
