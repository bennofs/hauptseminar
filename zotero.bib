
@article{yao_learning_2020,
	title = {Learning Nonlinear Loop Invariants With Gated Continuous Logic Networks},
	url = {http://arxiv.org/abs/2003.07959v3},
	abstract = {Verifying real-world programs often requires inferring loop invariants with nonlinear constraints. This is especially true in programs that perform many numerical operations, such as control systems for avionics or industrial plants. Recently, data-driven methods for loop invariant inference have shown promise, especially on linear invariants. However, applying data-driven inference to nonlinear loop invariants is challenging due to the large numbers of and magnitudes of high-order terms, the potential for overfitting on a small number of samples, and the large space of possible inequality bounds. In this paper, we introduce a new neural architecture for general {SMT} learning, the Gated Continuous Logic Network (G-{CLN}), and apply it to nonlinear loop invariant learning. G-{CLNs} extend the Continuous Logic Network ({CLN}) architecture with gating units and dropout, which allow the model to robustly learn general invariants over large numbers of terms. To address overfitting that arises from finite program sampling, we introduce fractional sampling—a sound relaxation of loop semantics to continuous functions that facilitates unbounded sampling on real domain. We additionally design a new {CLN} activation function, the Piecewise Biased Quadratic Unit ({PBQU}), for naturally learning tight inequality bounds. We incorporate these methods into a nonlinear loop invariant inference system that can learn general nonlinear loop invariants. We evaluate our system on a benchmark of nonlinear loop invariants and show it solves 26 out of 27 problems, 3 more than prior work, with an average runtime of 53.3 seconds. We further demonstrate the generic learning ability of G-{CLNs} by solving all 124 problems in the linear Code2Inv benchmark. We also perform a quantitative stability evaluation and show G-{CLNs} have a convergence rate of \$97.5\%\$ on quadratic problems, a \$39.2\%\$ improvement over {CLN} models.},
	journaltitle = {{CoRR}},
	author = {Yao, Jianan and Ryan, Gabriel and Wong, Justin and Jana, Suman and Gu, Ronghui},
	date = {2020},
	note = {\_eprint: 2003.07959v3},
	keywords = {\_tablet, read, ⛔ No {DOI} found},
	file = {Yao et al_2020_Learning Nonlinear Loop Invariants With Gated Continuous Logic Networks.pdf:/data/zotero/storage/P9ET2BJF/Yao et al_2020_Learning Nonlinear Loop Invariants With Gated Continuous Logic Networks.pdf:application/pdf}
}

@inproceedings{menendez_alive-infer_2017,
	title = {Alive-Infer: data-driven precondition inference for peephole optimizations in {LLVM}},
	url = {https://doi.org/10.1145/3062341.3062372},
	doi = {10/gg3j54},
	abstract = {Peephole optimizations are a common source of compiler bugs. Compiler developers typically transform an incorrect peephole optimization into a valid one by strengthening the precondition. This process is challenging and tedious. This paper proposes Alive-Infer, a data-driven approach that infers preconditions for peephole optimizations expressed in Alive. Alive-Infer generates positive and negative examples for an optimization, enumerates predicates on-demand, and learns a set of predicates that separate the positive and negative examples. Alive-Infer repeats this process until it finds a precondition that ensures the validity of the optimization. Alive-Infer reports both a weakest precondition and a set of succinct partial preconditions to the developer. Our prototype generates preconditions that are weaker than {LLVM}’s preconditions for 73 optimizations in the Alive suite. We also demonstrate the applicability of this technique to generalize 54 optimization patterns generated by Souper, an {LLVM} {IR}–based superoptimizer.},
	pages = {nil},
	booktitle = {Proceedings of the 38th {ACM} {SIGPLAN} Conference on Programming Language Design and Implementation - {PLDI} 2017},
	author = {Menendez, David and Nagarakatte, Santosh},
	date = {2017},
	keywords = {\_tablet, read},
	file = {Menendez_Nagarakatte_2017_Alive-Infer.pdf:/data/zotero/storage/TGNNP2VB/Menendez_Nagarakatte_2017_Alive-Infer.pdf:application/pdf}
}

@article{raychev_probabilistic_2016,
	title = {Probabilistic model for code with decision trees},
	url = {http://dx.doi.org/10.1145/2983990.2984041},
	doi = {10/gg3j67},
	abstract = {In this paper we introduce a new approach for learning precise and general probabilistic models of code based on decision tree learning. Our approach directly benefits an emerging class of statistical programming tools which leverage probabilistic models of code learned over large codebases (e.g., {GitHub}) to make predictions about new programs (e.g., code completion, repair, etc).

The key idea is to phrase the problem of learning a probabilistic model of code as learning a decision tree in a domain specific language over abstract syntax trees (called {TGen}). This allows us to condition the prediction of a program element on a dynamically computed context. Further, our problem formulation enables us to easily instantiate known decision tree learning algorithms such as {ID}3, but also to obtain new variants we refer to as {ID}3+ and E13, not previously explored and ones that outperform {ID}3 in prediction accuracy.

Our approach is general and can be used to learn a probabilistic model of any programming language. We implemented our approach in a system called Deep3 and evaluated it for the challenging task of learning probabilistic models of {JavaScript} and Python. Our experimental results indicate that Deep3 predicts elements of {JavaScript} and Python code with precision above 82\% and 69\%, respectively. Further, Deep3 often significantly outperforms state-of-the-art approaches in overall prediction accuracy.},
	journaltitle = {Proceedings of the 2016 {ACM} {SIGPLAN} International Conference on Object-Oriented Programming, Systems, Languages, and Applications - {OOPSLA} 2016},
	author = {Raychev, Veselin and Bielik, Pavol and Vechev, Martin},
	date = {2016},
	note = {{ISBN}: 9781450344449
Publisher: {ACM} Press},
	keywords = {\_tablet, to-read},
	file = {Raychev et al_2016_Probabilistic model for code with decision trees.pdf:/data/zotero/storage/FHCA5GTY/Raychev et al_2016_Probabilistic model for code with decision trees.pdf:application/pdf}
}

@article{raychev_predicting_2019,
	title = {Predicting program properties from “big code”},
	volume = {62},
	issn = {1557-7317},
	url = {http://dx.doi.org/10.1145/3306204},
	doi = {10/gg3j66},
	pages = {99--107},
	number = {3},
	journaltitle = {Communications of the {ACM}},
	author = {Raychev, Veselin and Vechev, Martin and Krause, Andreas},
	date = {2019-02},
	note = {Publisher: Association for Computing Machinery ({ACM})},
	keywords = {\_tablet, read},
	file = {Raychev et al_2019_Predicting program properties from “big code”.pdf:/data/zotero/storage/VQRPALTZ/Raychev et al_2019_Predicting program properties from “big code”.pdf:application/pdf}
}

@article{allamanis_typilus_2020,
	title = {Typilus: Neural Type Hints},
	volume = {abs/2004.10657},
	url = {https://arxiv.org/abs/2004.10657},
	abstract = {Type inference over partial contexts in dynamically typed languages is challenging. In this work, we present a graph neural network model that predicts types by probabilistically reasoning over a program's structure, names, and patterns. The network uses deep similarity learning to learn a {TypeSpace} -- a continuous relaxation of the discrete space of types -- and how to embed the type properties of a symbol (i.e. identifier) into it. Importantly, our model can employ one-shot learning to predict an open vocabulary of types, including rare and user-defined ones. We realise our approach in Typilus for Python that combines the {TypeSpace} with an optional type checker. We show that Typilus accurately predicts types. Typilus confidently predicts types for 70\% of all annotatable symbols; when it predicts a type, that type optionally type checks 95\% of the time. Typilus can also find incorrect type annotations; two important and popular open source libraries, fairseq and allennlp, accepted our pull requests that fixed the annotation errors Typilus discovered.},
	journaltitle = {{CoRR}},
	author = {Allamanis, Miltiadis and Barr, Earl T. and Ducousso, Soline and Gao, Zheng},
	date = {2020},
	note = {\_eprint: 2004.10657},
	keywords = {\_tablet, read, mpnn, ⛔ No {DOI} found},
	file = {Allamanis et al_2020_Typilus.pdf:/data/zotero/storage/HBN6BK8X/Allamanis et al_2020_Typilus.pdf:application/pdf}
}

@article{ernst_daikon_2007,
	title = {The Daikon system for dynamic detection of likely invariants},
	volume = {69},
	doi = {10/drc63v},
	pages = {35--45},
	number = {1},
	journaltitle = {Science of computer programming},
	author = {Ernst, Michael D. and Perkins, Jeff H. and Guo, Philip J. and {McCamant}, Stephen and Pacheco, Carlos and Tschantz, Matthew S. and Xiao, Chen},
	date = {2007},
	note = {Publisher: Elsevier},
	keywords = {\_tablet},
	file = {Ernst et al_2007_The Daikon system for dynamic detection of likely invariants.pdf:/data/zotero/storage/3CP3ANAY/Ernst et al_2007_The Daikon system for dynamic detection of likely invariants.pdf:application/pdf}
}

@inproceedings{ernst_finding_2004,
	title = {Finding latent code errors via machine learning over program executions},
	doi = {10/dndgmg},
	abstract = {This paper proposes a technique for identifying program properties that indicate errors. The technique generates machine learning models of program properties known to result from errors, and applies these models to program properties of user-written code to classify and rank properties that may lead the user to errors. Given a set of properties produced by the program analysis, the technique selects a subset of properties that are most likely to reveal an error. An implementation, the fault invariant classifier, demonstrates the efficacy of the technique. The implementation uses dynamic invariant detection to generate program properties. It uses support vector machine and decision tree learning tools to classify those properties. In our experimental evaluation, the technique increases the relevance (the concentration of fault-revealing properties) by a factor of 50 on average for the C programs, and 4.8 for the Java programs. Preliminary experience suggests that most of the fault-revealing properties do lead a programmer to an error.},
	eventtitle = {Proceedings. 26th International Conference on Software Engineering},
	pages = {480--490},
	booktitle = {Proceedings. 26th International Conference on Software Engineering},
	author = {Ernst, M.D and Brun, Yuriy},
	date = {2004-05},
	note = {{ISSN}: 0270-5257},
	keywords = {\_tablet, skimmed},
	file = {Ernst_Brun_2004_Finding latent code errors via machine learning over program executions.pdf:/data/zotero/storage/8TARWUHP/Ernst_Brun_2004_Finding latent code errors via machine learning over program executions.pdf:application/pdf;IEEE Xplore Abstract Record:/data/zotero/storage/FNM2EDXY/1317470.html:text/html}
}

@article{alon_general_2018,
	title = {A general path-based representation for predicting program properties},
	volume = {53},
	issn = {0362-1340},
	url = {https://doi.org/10.1145/3296979.3192412},
	doi = {10/gg3j5v},
	abstract = {Predicting program properties such as names or expression types has a wide range of applications. It can ease the task of programming, and increase programmer productivity. A major challenge when learning from programs is how to represent programs in a way that facilitates effective learning. We present a general path-based representation for learning from programs. Our representation is purely syntactic and extracted automatically. The main idea is to represent a program using paths in its abstract syntax tree ({AST}). This allows a learning model to leverage the structured nature of code rather than treating it as a flat sequence of tokens. We show that this representation is general and can: (i) cover different prediction tasks, (ii) drive different learning algorithms (for both generative and discriminative models), and (iii) work across different programming languages. We evaluate our approach on the tasks of predicting variable names, method names, and full types. We use our representation to drive both {CRF}-based and word2vec-based learning, for programs of four languages: {JavaScript}, Java, Python and C\#. Our evaluation shows that our approach obtains better results than task-specific handcrafted representations across different tasks and programming languages.},
	pages = {404--419},
	number = {4},
	journaltitle = {{ACM} {SIGPLAN} Notices},
	shortjournal = {{SIGPLAN} Not.},
	author = {Alon, Uri and Zilberstein, Meital and Levy, Omer and Yahav, Eran},
	urldate = {2020-06-01},
	date = {2018-06-11},
	keywords = {read},
	file = {Full Text PDF:/data/zotero/storage/JSCL3N3W/Alon et al. - 2018 - A general path-based representation for predicting.pdf:application/pdf}
}

@inproceedings{yahav_programs_2018,
	location = {Cham},
	title = {From Programs to Interpretable Deep Models and Back},
	isbn = {978-3-319-96145-3},
	doi = {10/gg3j6m},
	series = {Lecture Notes in Computer Science},
	abstract = {We demonstrate how deep learning over programs is used to provide (preliminary) augmented programmer intelligence. In the first part, we show how to tackle tasks like code completion, code summarization, and captioning. We describe a general path-based representation of source code that can be used across programming languages and learning tasks, and discuss how this representation enables different learning algorithms. In the second part, we describe techniques for extracting interpretable representations from deep models, shedding light on what has actually been learned in various tasks.},
	pages = {27--37},
	booktitle = {Computer Aided Verification},
	publisher = {Springer International Publishing},
	author = {Yahav, Eran},
	editor = {Chockler, Hana and Weissenbacher, Georg},
	date = {2018},
	langid = {english},
	keywords = {\_tablet, to-read},
	file = {Yahav_2018_From Programs to Interpretable Deep Models and Back.pdf:/data/zotero/storage/KHN4TCVY/Yahav_2018_From Programs to Interpretable Deep Models and Back.pdf:application/pdf}
}

@article{alon_code2vec_2019,
	title = {code2vec: learning distributed representations of code},
	volume = {3},
	url = {https://doi.org/10.1145/3290353},
	doi = {10/ggssk3},
	shorttitle = {code2vec},
	abstract = {We present a neural model for representing snippets of code as continuous distributed vectors (``code embeddings''). The main idea is to represent a code snippet as a single fixed-length code vector, which can be used to predict semantic properties of the snippet. To this end, code is first decomposed to a collection of paths in its abstract syntax tree. Then, the network learns the atomic representation of each path while simultaneously learning how to aggregate a set of them. We demonstrate the effectiveness of our approach by using it to predict a method's name from the vector representation of its body. We evaluate our approach by training a model on a dataset of 12M methods. We show that code vectors trained on this dataset can predict method names from files that were unobserved during training. Furthermore, we show that our model learns useful method name vectors that capture semantic similarities, combinations, and analogies. A comparison of our approach to previous techniques over the same dataset shows an improvement of more than 75\%, making it the first to successfully predict method names based on a large, cross-project corpus. Our trained model, visualizations and vector similarities are available as an interactive online demo at http://code2vec.org. The code, data and trained models are available at https://github.com/tech-srl/code2vec.},
	pages = {40:1--40:29},
	issue = {{POPL}},
	journaltitle = {Proceedings of the {ACM} on Programming Languages},
	shortjournal = {Proc. {ACM} Program. Lang.},
	author = {Alon, Uri and Zilberstein, Meital and Levy, Omer and Yahav, Eran},
	urldate = {2020-06-01},
	date = {2019-01-02},
	keywords = {\_tablet, read},
	file = {Alon et al_2019_code2vec.pdf:/data/zotero/storage/MAVMSVUJ/Alon et al_2019_code2vec.pdf:application/pdf}
}

@article{pradel_deepbugs_2018,
	title = {{DeepBugs}: a learning approach to name-based bug detection},
	volume = {2},
	url = {https://doi.org/10.1145/3276517},
	doi = {10/ggwxh2},
	shorttitle = {{DeepBugs}},
	abstract = {Natural language elements in source code, e.g., the names of variables and functions, convey useful information. However, most existing bug detection tools ignore this information and therefore miss some classes of bugs. The few existing name-based bug detection approaches reason about names on a syntactic level and rely on manually designed and tuned algorithms to detect bugs. This paper presents {DeepBugs}, a learning approach to name-based bug detection, which reasons about names based on a semantic representation and which automatically learns bug detectors instead of manually writing them. We formulate bug detection as a binary classification problem and train a classifier that distinguishes correct from incorrect code. To address the challenge that effectively learning a bug detector requires examples of both correct and incorrect code, we create likely incorrect code examples from an existing corpus of code through simple code transformations. A novel insight learned from our work is that learning from artificially seeded bugs yields bug detectors that are effective at finding bugs in real-world code. We implement our idea into a framework for learning-based and name-based bug detection. Three bug detectors built on top of the framework detect accidentally swapped function arguments, incorrect binary operators, and incorrect operands in binary operations. Applying the approach to a corpus of 150,000 {JavaScript} files yields bug detectors that have a high accuracy (between 89\% and 95\%), are very efficient (less than 20 milliseconds per analyzed file), and reveal 102 programming mistakes (with 68\% true positive rate) in real-world code.},
	pages = {147:1--147:25},
	issue = {{OOPSLA}},
	journaltitle = {Proceedings of the {ACM} on Programming Languages},
	shortjournal = {Proc. {ACM} Program. Lang.},
	author = {Pradel, Michael and Sen, Koushik},
	urldate = {2020-06-01},
	date = {2018-10-24},
	keywords = {\_tablet, to-read},
	file = {Pradel_Sen_2018_DeepBugs.pdf:/data/zotero/storage/SRKBDBHS/Pradel_Sen_2018_DeepBugs.pdf:application/pdf}
}

@article{pandi_opttyper_2020,
	title = {{OptTyper}: Probabilistic Type Inference by Optimising Logical and Natural Constraints},
	url = {http://arxiv.org/abs/2004.00348},
	shorttitle = {{OptTyper}},
	abstract = {We present a new approach to the type inference problem for dynamic languages. Our goal is to combine logical constraints, that is, deterministic information from a type system, with natural constraints, that is, uncertain statistical information about types learnt from sources like identifier names. To this end, we introduce a framework for probabilistic type inference that combines logic and learning: logical constraints on the types are extracted from the program, and deep learning is applied to predict types from surface-level code properties that are statistically associated, such as variable names. The foremost insight of our method is to constrain the predictions from the learning procedure to respect the logical constraints, which we achieve by relaxing the logical inference problem of type prediction into a continuous optimisation problem. As proof of concept, we build a tool called {OptTyper} to predict missing types for {TypeScript} files. {OptTyper} combines a continuous interpretation of logical constraints derived by a simple program transformation and static analysis of {TypeScript} code, with natural constraints obtained from a deep learning model, which learns naming conventions for types from a large codebase. By evaluating {OptTyper}, we show that the combination of logical and natural constraints yields a large improvement in performance over either kind of information individually and achieves a 3\% improvement over the state-of-the-art.},
	journaltitle = {{arXiv}:2004.00348 [cs]},
	author = {Pandi, Irene Vlassi and Barr, Earl T. and Gordon, Andrew D. and Sutton, Charles},
	urldate = {2020-06-01},
	date = {2020-05-18},
	eprinttype = {arxiv},
	eprint = {2004.00348},
	keywords = {\_tablet, read, ⛔ No {DOI} found},
	file = {Pandi et al_2020_OptTyper.pdf:/data/zotero/storage/B8IUQGKV/Pandi et al_2020_OptTyper.pdf:application/pdf;arXiv.org Snapshot:/data/zotero/storage/RE2WZG54/2004.html:text/html}
}

@article{wei_lambdanet_2020,
	title = {{LambdaNet}: Probabilistic Type Inference using Graph Neural Networks},
	url = {http://arxiv.org/abs/2005.02161},
	shorttitle = {{LambdaNet}},
	abstract = {As gradual typing becomes increasingly popular in languages like Python and {TypeScript}, there is a growing need to infer type annotations automatically. While type annotations help with tasks like code completion and static error catching, these annotations cannot be fully determined by compilers and are tedious to annotate by hand. This paper proposes a probabilistic type inference scheme for {TypeScript} based on a graph neural network. Our approach first uses lightweight source code analysis to generate a program abstraction called a type dependency graph, which links type variables with logical constraints as well as name and usage information. Given this program abstraction, we then use a graph neural network to propagate information between related type variables and eventually make type predictions. Our neural architecture can predict both standard types, like number or string, as well as user-defined types that have not been encountered during training. Our experimental results show that our approach outperforms prior work in this space by \$14{\textbackslash}\%\$ (absolute) on library types, while having the ability to make type predictions that are out of scope for existing techniques.},
	journaltitle = {{arXiv}:2005.02161 [cs, stat]},
	author = {Wei, Jiayi and Goyal, Maruth and Durrett, Greg and Dillig, Isil},
	urldate = {2020-06-01},
	date = {2020-04-29},
	eprinttype = {arxiv},
	eprint = {2005.02161},
	keywords = {\_tablet, read, mpnn, ⛔ No {DOI} found},
	file = {Wei et al_2020_LambdaNet.pdf:/data/zotero/storage/Y8DBPV9C/Wei et al_2020_LambdaNet.pdf:application/pdf;arXiv.org Snapshot:/data/zotero/storage/T687N76K/2005.html:text/html}
}

@article{pradel_typewriter_2020,
	title = {{TypeWriter}: Neural Type Prediction with Search-based Validation},
	url = {http://arxiv.org/abs/1912.03768},
	shorttitle = {{TypeWriter}},
	abstract = {Maintaining large code bases written in dynamically typed languages, such as {JavaScript} or Python, can be challenging due to the absence of type annotations: simple data compatibility errors proliferate, {IDE} support is limited, and {APIs} are hard to comprehend. Recent work attempts to address those issues through either static type inference or probabilistic type prediction. Unfortunately, static type inference for dynamic languages is inherently limited, while probabilistic approaches suffer from imprecision. This paper presents {TypeWriter}, the first combination of probabilistic type prediction with search-based refinement of predicted types. {TypeWriter}'s predictor learns to infer the return and argument types for functions from partially annotated code bases by combining the natural language properties of code with programming language-level information. To validate predicted types, {TypeWriter} invokes a gradual type checker with different combinations of the predicted types, while navigating the space of possible type combinations in a feedback-directed manner. We implement the {TypeWriter} approach for Python and evaluate it on two code corpora: a multi-million line code base at Facebook and a collection of 1,137 popular open-source projects. We show that {TypeWriter}'s type predictor achieves an F1 score of 0.64 (0.79) in the top-1 (top-5) predictions for return types, and 0.57 (0.80) for argument types, which clearly outperforms prior type prediction models. By combining predictions with search-based validation, {TypeWriter} can fully annotate between 14\% to 44\% of the files in a randomly selected corpus, while ensuring type correctness. A comparison with a static type inference tool shows that {TypeWriter} adds many more non-trivial types. {TypeWriter} currently suggests types to developers at Facebook and several thousands of types have already been accepted with minimal changes.},
	journaltitle = {{arXiv}:1912.03768 [cs]},
	author = {Pradel, Michael and Gousios, Georgios and Liu, Jason and Chandra, Satish},
	urldate = {2020-06-01},
	date = {2020-03-06},
	eprinttype = {arxiv},
	eprint = {1912.03768},
	keywords = {\_tablet, read, ⛔ No {DOI} found},
	file = {Pradel et al_2020_TypeWriter.pdf:/data/zotero/storage/4G6NT96X/Pradel et al_2020_TypeWriter.pdf:application/pdf;arXiv.org Snapshot:/data/zotero/storage/LEGIJE7F/1912.html:text/html}
}

@article{allamanis_learning_2018,
	title = {Learning to Represent Programs with Graphs},
	abstract = {Learning tasks on source code (i.e., formal languages) have been considered recently, but most work has tried to transfer natural language methods and does not capitalize on the unique opportunities offered by code's known syntax. For example, long-range dependencies induced by using the same variable or function in distant locations are often not considered. We propose to use graphs to represent both the syntactic and semantic structure of code and use graph-based deep learning methods to learn to reason over program structures. 
In this work, we present how to construct graphs from source code and how to scale Gated Graph Neural Networks training to such large graphs. We evaluate our method on two tasks: {VarNaming}, in which a network attempts to predict the name of a variable given its usage, and {VarMisuse}, in which the network learns to reason about selecting the correct variable that should be used at a given program location. Our comparison to methods that use less structured program representations shows the advantages of modeling known structure, and suggests that our models learn to infer meaningful names and to solve the {VarMisuse} task in many cases. Additionally, our testing showed that {VarMisuse} identifies a number of bugs in mature open-source projects.},
	journaltitle = {{ICLR}},
	author = {Allamanis, Miltiadis and Brockschmidt, Marc and Khademi, Mahmoud},
	date = {2018},
	keywords = {\_tablet, read, mpnn, ⛔ No {DOI} found},
	file = {Allamanis et al_2018_Learning to Represent Programs with Graphs.pdf:/data/zotero/storage/4CMFCKLE/Allamanis et al_2018_Learning to Represent Programs with Graphs.pdf:application/pdf}
}

@article{cummins_programl_2020,
	title = {{ProGraML}: Graph-based Deep Learning for Program Optimization and Analysis},
	shorttitle = {{ProGraML}},
	abstract = {The increasing complexity of computing systems places a tremendous burden on optimizing compilers, requiring ever more accurate and aggressive optimizations. Machine learning offers significant benefits for constructing optimization heuristics but there remains a gap between what state-of-the-art methods achieve and the performance of an optimal heuristic. Closing this gap requires improvements in two key areas: a representation that accurately captures the semantics of programs, and a model architecture with sufficient expressiveness to reason about this representation. 
We introduce {ProGraML} - Program Graphs for Machine Learning - a novel graph-based program representation using a low level, language agnostic, and portable format; and machine learning models capable of performing complex downstream tasks over these graphs. The {ProGraML} representation is a directed attributed multigraph that captures control, data, and call relations, and summarizes instruction and operand types and ordering. Message Passing Neural Networks propagate information through this structured representation, enabling whole-program or per-vertex classification tasks. 
{ProGraML} provides a general-purpose program representation that equips learnable models to perform the types of program analysis that are fundamental to optimization. To this end, we evaluate the performance of our approach first on a suite of traditional compiler analysis tasks: control flow reachability, dominator trees, data dependencies, variable liveness, and common subexpression detection. On a benchmark dataset of 250k {LLVM}-{IR} files covering six source programming languages, {ProGraML} achieves an average 94.0 F1 score, significantly outperforming the state-of-the-art approaches. We then apply our approach to two high-level tasks - heterogeneous device mapping and program classification - setting new state-of-the-art performance in both.},
	journaltitle = {{ArXiv}},
	author = {Cummins, Chris and Fisches, Zacharias V. and Ben-Nun, Tal and Hoefler, Torsten and Leather, Hugh},
	date = {2020},
	keywords = {\_tablet, read, mpnn, ⛔ No {DOI} found},
	file = {Cummins et al_2020_ProGraML.pdf:/data/zotero/storage/M3PXDI69/Cummins et al_2020_ProGraML.pdf:application/pdf}
}

@article{brauckmann_compiler-based_2020,
	title = {Compiler-based graph representations for deep learning models of code},
	doi = {10/gg3j57},
	abstract = {In natural language processing, novel methods in deep learning, like recurrent neural networks ({RNNs}) on sequences of words, have been very successful. In contrast to natural languages, programming languages usually have a well-defined structure. With this structure compilers can reason about programs, using graphs such as abstract syntax trees ({ASTs}) or control-data flow graphs ({CDFGs}). In this paper, we argue that we should use these graph structures instead of sequences for learning compiler optimization tasks. To this end, we use graph neural networks ({GNNs}) for learning predictive compiler tasks on two representations based on {ASTs} and {CDFGs}. Experiments show that this improves upon the state-of-the-art in the task of heterogeneous {OpenCL} mapping, while providing orders of magnitude faster inference times, crucial for compiler optimizations. When testing on benchmark suites not included for training, our {AST}-based model significantly outperforms the state-of-the-art by over 12 percentage points in terms of accuracy. It is the only one to perform clearly better than a random mapping. On the task of predicting thread coarsening factors, we show that all of the methods fail to produce an overall speedup.},
	journaltitle = {{CC}},
	author = {Brauckmann, Alexander and Goens, Andrés and Ertel, Sebastian and Castrillón, Jerónimo},
	date = {2020},
	keywords = {\_tablet, read, mpnn},
	file = {Brauckmann et al_2020_Compiler-based graph representations for deep learning models of code.pdf:/data/zotero/storage/R4DGA26B/Brauckmann et al_2020_Compiler-based graph representations for deep learning models of code.pdf:application/pdf}
}

@article{wang_learning_2019,
	title = {Learning Scalable and Precise Representation of Program Semantics},
	abstract = {Neural program embedding has shown potential in aiding the analysis of large-scale, complicated software. Newly proposed deep neural architectures pride themselves on learning program semantics rather than superficial syntactic features. However, by considering the source code only, the vast majority of neural networks do not capture a deep, precise representation of program semantics. In this paper, we present {\textbackslash}dypro, a novel deep neural network that learns from program execution traces. Compared to the prior dynamic models, not only is {\textbackslash}dypro capable of generalizing across multiple executions for learning a program's dynamic semantics in its entirety, but {\textbackslash}dypro is also more efficient when dealing with programs yielding long execution traces. For evaluation, we task {\textbackslash}dypro with semantic classification (i.e. categorizing programs based on their semantics) and compared it against two prominent static models: Gated Graph Neural Network and {TreeLSTM}. We find that {\textbackslash}dypro achieves the highest prediction accuracy among all models. To further reveal the capacity of all aforementioned deep neural architectures, we examine if the models can learn to detect deeper semantic properties of a program. In particular given a task of recognizing loop invariants, we show {\textbackslash}dypro beats all static models by a wide margin.},
	journaltitle = {{ArXiv}},
	author = {Wang, Ke},
	date = {2019},
	keywords = {\_tablet, read, ⛔ No {DOI} found},
	file = {Wang_2019_Learning Scalable and Precise Representation of Program Semantics.pdf:/data/zotero/storage/JMJKHA7D/Wang_2019_Learning Scalable and Precise Representation of Program Semantics.pdf:application/pdf}
}

@article{hellendoorn_are_2019,
	title = {Are My Invariants Valid? A Learning Approach},
	shorttitle = {Are My Invariants Valid?},
	abstract = {Ensuring that a program operates correctly is a difficult task in large, complex systems. Enshrining invariants -- desired properties of correct execution -- in code or comments can support maintainability and help sustain correctness. Tools that can automatically infer and recommend invariants can thus be very beneficial. However, current invariant-suggesting tools, such as Daikon, suffer from high rates of false positives, in part because they only leverage traced program values from available test cases, rather than directly exploiting knowledge of the source code per se. We propose a machine-learning approach to judging the validity of invariants, specifically of method pre- and post-conditions, based directly on a method's source code. We introduce a new, scalable approach to creating labeled invariants: using programs with large test-suites, we generate Daikon invariants using traces from subsets of these test-suites, and then label these as valid/invalid by cross-validating them with held-out tests. This process induces a large set of labels that provide a form of noisy supervision, which is then used to train a deep neural model, based on gated graph neural networks. Our model learns to map the lexical, syntactic, and semantic structure of a given method's body into a probability that a candidate pre- or post-condition on that method's body is correct and is able to accurately label invariants based on the noisy signal, even in cross-project settings. Most importantly, it performs well on a hand-curated dataset of invariants.},
	journaltitle = {{ArXiv}},
	author = {Hellendoorn, Vincent J. and Devanbu, Premkumar T. and Polozov, Oleksandr and Marron, Mark},
	date = {2019},
	keywords = {\_tablet, read, mpnn, ⛔ No {DOI} found},
	file = {Hellendoorn et al_2019_Are My Invariants Valid.pdf:/data/zotero/storage/AMFS9RJI/Hellendoorn et al_2019_Are My Invariants Valid.pdf:application/pdf}
}

@article{ahmed_learning_2019,
	title = {Learning Lenient Parsing \& Typing via Indirect Supervision},
	abstract = {Both professional coders and teachers frequently deal with imperfect (fragmentary, incomplete, ill-formed) code. Such fragments are common in {StackOverflow}; students also frequently produce ill-formed code, for which instructors, {TAs} (or students themselves) must find repairs. In either case, the developer experience could be greatly improved if such code could somehow be parsed \& typed; this makes them more amenable to use within {IDEs} and allows early detection and repair of potential errors. We introduce a lenient parser, which can parse \& type fragments, even ones with simple errors. Training a machine learner to leniently parse \& type imperfect code requires a large training set of pairs of imperfect code and its repair (and/or type information); such training sets are limited by human effort and curation. In this paper, we present a novel indirectly supervised approach to train a lenient parser, without access to such human-curated training data. We leverage the huge corpus of mostly correct code available on Github, and the massive, efficient learning capacity of Transformer-based {NN} architectures. Using {GitHub} data, we first create a large dataset of fragments of code and corresponding tree fragments and type annotations; we then randomly corrupt the input fragments (while requiring correct output) by seeding errors that mimic corruptions found in {StackOverflow} and student data. Using this data, we train high-capacity transformer models to overcome both fragmentation and corruption. With this novel approach, we can achieve reasonable performance on parsing \& typing {StackOverflow} fragments; we also demonstrate that our approach achieves best-in-class performance on a large dataset of student errors.},
	journaltitle = {{ArXiv}},
	author = {Ahmed, Toufique and Hellendoorn, Vincent J. and Devanbu, Premkumar T.},
	date = {2019},
	keywords = {⛔ No {DOI} found},
	file = {Full Text PDF:/data/zotero/storage/FYQSWIE5/Ahmed et al. - 2019 - Learning Lenient Parsing & Typing via Indirect Sup.pdf:application/pdf}
}

@inproceedings{xu_learning_2017,
	title = {Learning Types for Binaries},
	doi = {10/gg3j62},
	abstract = {Type inference for Binary codes is a challenging problem due partly to the fact that much type-related information has been lost during the compilation from high-level source code. Most of the existing research on binary code type inference tend to resort to program analysis techniques, which can be too conservative to infer types with high accuracy or too heavy-weight to be viable in practice. In this paper, we propose a new approach to learning types for recovered variables from their related representative instructions. Our idea is motivated by “duck typing”, where the type of a variable is determined by its features and properties. Our approach first learns a classifier from existing binaries with debug information and then uses this classifier to predict types for new, unseen binaries. We have implemented our approach in a tool called {BITY} and used it to conduct some experiments on a well-known benchmark coreutils (v8.4). The results show that our tool is more precise than the commercial tool Hey-Rays, both in terms of correct types and compatible types.},
	booktitle = {{ICFEM}},
	author = {Xu, Zhiwu and Wen, Cheng and Qin, Shengchao},
	date = {2017},
	file = {Submitted Version:/data/zotero/storage/FM7URSXZ/Xu et al. - 2017 - Learning Types for Binaries.pdf:application/pdf}
}

@article{xu_type_2019,
	title = {Type Learning for Binaries and Its Applications},
	doi = {10/gg3j7h},
	abstract = {Binary type inference is a challenging problem due partly to the fact that during the compilation much type-related information has been lost. Most existing research work resorts to program analysis techniques, which can be either too heavyweight to be viable in practice or too conservative to be able to infer types with high accuracy. In this paper, we propose a new approach to learning types for binary code. Motivated by “duck typing,” our approach learn types for recovered variables from their features and properties (e.g., related representative instructions). We first use machine learning to train a classifier with basic types as its levels from binaries with debugging information. The classifier is then used to learn types for new and unseen binaries. While for composite types, such as {\textless}inline-formula{\textgreater}{\textless}tex-math notation="{LaTeX}"{\textgreater}\$\{pointer\}\${\textless}/tex-math{\textgreater}{\textless}/inline-formula{\textgreater} and {\textless}inline-formula{\textgreater}{\textless}tex-math notation="{LaTeX}"{\textgreater}\$\{struct\}\${\textless}/tex-math{\textgreater}{\textless}/inline-formula{\textgreater}, a points-to analysis is performed. Finally, several experiments are conducted to evaluate our approach. The results demonstrate that our approach is more precise, both in terms of correct types and compatible types, than the commercial tool Hex-Rays, the open source tool Snowman, and a recent tool {EKLAVYA} using machine learning. We also show that the type information our proposed system learns is capable of helping detect malware.},
	journaltitle = {{IEEE} Transactions on Reliability},
	author = {Xu, Zhiwu and Wen, Cheng and Qin, Shengchao},
	date = {2019},
	file = {Submitted Version:/data/zotero/storage/DGVUPWZ4/Xu et al. - 2019 - Type Learning for Binaries and Its Applications.pdf:application/pdf}
}

@article{venkatakeerthy_ir2vec_2019,
	title = {{IR}2Vec: A Flow Analysis based Scalable Infrastructure for Program Encodings},
	shorttitle = {{IR}2Vec},
	abstract = {We propose {IR}2Vec, a Concise and Scalable encoding infrastructure to represent programs as a distributed embedding in continuous space. This distributed embedding is obtained by combining representation learning methods with data and control flow information to capture the syntax as well as the semantics of the input programs. 
Our embeddings are obtained from the Intermediate Representation ({IR}) of the source code, and are both language as well as machine independent. The entities of the {IR} are modelled as relationships, and their representations are learned to form a seed embedding vocabulary. This vocabulary is used along with the flow analyses information to form a hierarchy of encodings based on various levels of program abstractions. 
We show the effectiveness of our methodology on a software engineering task (program classification) as well as optimization tasks (Heterogeneous device mapping and Thread coarsening). The embeddings generated by {IR}2Vec outperform the existing methods in all the three tasks even when using simple machine learning models. As we follow an agglomerative method of forming encodings at various levels using seed embedding vocabulary, our encoding is naturally more scalable and not data-hungry when compared to the other methods.},
	journaltitle = {{ArXiv}},
	author = {{VenkataKeerthy}, S. and Aggarwal, Rohit and Jain, Shalini and Desarkar, Maunendra Sankar and Upadrasta, Ramakrishna and Srikant, Y. N.},
	date = {2019},
	keywords = {\_tablet, to-read, ⛔ No {DOI} found},
	file = {VenkataKeerthy et al_2019_IR2Vec.pdf:/data/zotero/storage/HJPU75D6/VenkataKeerthy et al_2019_IR2Vec.pdf:application/pdf}
}

@inproceedings{kulkarni_beyond_2018,
	title = {Beyond Deductive Methods in Program Analysis},
	abstract = {Building effective program analysis tools is a challenging endeavor: analysis designers must balance multiple competing objectives, including scalability, fraction of false alarms, and the possibility of missed bugs. Not all of these design decisions are optimal when the analysis is applied to a new program with different coding idioms, environment assumptions, and quality requirements. Furthermore, the alarms produced are typically accompanied by limited information such as their location and abstract counter-examples. We present a framework {DIFFLOG} that fundamentally extends the deductive reasoning rules that underlie program analyses with numerical weights. Each alarm is now naturally accompanied by a score, indicating quantities such as the confidence that the alarm is a real bug, the anticipated severity, or expected relevance of the alarm to the programmer. To the analysis user, these techniques offer a lens by which to focus their attention on the most important alarms and a uniform method for the tool to interactively generalize from human feedback. To the analysis designer, these techniques offer novel ways to automatically synthesize analysis rules in a data-driven style. {DIFFLOG} shows large reductions in false alarm rates and missed bugs in large, complex programs, and it advances the state-of-the-art in synthesizing non-trivial analyses.},
	author = {Kulkarni, Sulekha and Zhang, Rita and Si, Ximing and Heo, Kihong and Lee, Woosuk and Naik, Manali},
	date = {2018},
	keywords = {\_tablet, ⛔ No {DOI} found},
	file = {Kulkarni et al_2018_Beyond Deductive Methods in Program Analysis.pdf:/data/zotero/storage/4DYSY9KE/Kulkarni et al_2018_Beyond Deductive Methods in Program Analysis.pdf:application/pdf}
}

@inproceedings{yahav_typestate_2019,
	location = {Beijing, China},
	title = {From typestate verification to interpretable deep models (invited talk abstract)},
	isbn = {978-1-4503-6224-5},
	url = {https://doi.org/10.1145/3293882.3338992},
	doi = {10/gg3j6k},
	series = {{ISSTA} 2019},
	abstract = {The paper ``Effective Typestate Verification in the Presence of Aliasing'' was published in the International Symposium on Software Testing and Analysis ({ISSTA}) 2006 Proceedings, and has now been selected to receive the {ISSTA} 2019 Retrospective Impact Paper Award. The paper described a scalable framework for verification of typestate properties in real-world Java programs. The paper introduced several techniques that have been used widely in the static analysis of real-world programs. Specifically, it introduced an abstract domain combining access-paths, aliasing information, and typestate that turned out to be simple, powerful, and useful. We review the original paper and show the evolution of the ideas over the years. We show how some of these ideas have evolved into work on machine learning for code completion, and discuss recent general results in machine learning for programming.},
	pages = {4--5},
	booktitle = {Proceedings of the 28th {ACM} {SIGSOFT} International Symposium on Software Testing and Analysis},
	publisher = {Association for Computing Machinery},
	author = {Yahav, Eran and Fink, Stephen J. and Dor, Nurit and Ramalingam, G. and Geay, Emmanuel},
	urldate = {2020-06-01},
	date = {2019-07-10},
	keywords = {\_tablet, to-read},
	file = {Yahav et al_2019_From typestate verification to interpretable deep models (invited talk abstract).pdf:/data/zotero/storage/8JTZQEVG/Yahav et al_2019_From typestate verification to interpretable deep models (invited talk abstract).pdf:application/pdf}
}

@article{bugerya_recovery_2019,
	title = {Recovery of High-Level Intermediate Representations of Algorithms from Binary Code},
	doi = {10/gg3j69},
	abstract = {One of the tasks of binary code security analysis is detection of undocumented features in software. This task is hard to automate, and it requires participation of a cybersecurity expert. The way of representation of the algorithm under analysis strongly determines the analysis effort and quality of its results. Existing intermediate representations and languages are intended for use in software that either carries out optimizing transformations or analyzes binary code. Such representations and intermediate languages are unsuitable for manual data flow analysis. This paper proposes a high-level hierarchical flowchart-based representation of a program algorithm as well as an algorithm for its construction. The proposed representation is based on a hypergraph and it allows both automatic and manual data flow analysis on different detail levels. The hypergraph nodes represent functions. Every node contains a set of other nodes which are fragments. The fragment is a linear sequence of instructions that does not contain call and ret instructions. Edges represent data flows between nodes and correspond to memory buffers and registers. In the future this representation can be used to implement automatic analysis algorithms. An approach is proposed to increasing quality of the developed algorithm representation using grouping of single data flows into one flow connecting logical algorithm modules.},
	journaltitle = {2019 Ivannikov Memorial Workshop ({IVMEM})},
	author = {Bugerya, A. B. and Kulagin, Ivan and Padaryan, Vartan A. and Solov'ev, M. A. and Tikhonov, Andrei Yur'evich},
	date = {2019},
	keywords = {out-of-scope},
	file = {Bugerya et al_2019_Recovery of High-Level Intermediate Representations of Algorithms from Binary.pdf:/data/zotero/storage/T2IV4CVI/Bugerya et al_2019_Recovery of High-Level Intermediate Representations of Algorithms from Binary.pdf:application/pdf}
}

@article{zhao_neural-augmented_2018,
	title = {Neural-augmented static analysis of Android communication},
	doi = {10/gg3j6t},
	abstract = {We address the problem of discovering communication links between applications in the popular Android mobile operating system, an important problem for security and privacy in Android. Any scalable static analysis in this complex setting is bound to produce an excessive amount of false-positives, rendering it impractical. To improve precision, we propose to augment static analysis with a trained neural-network model that estimates the probability that a communication link truly exists. We describe a neural-network architecture that encodes abstractions of communicating objects in two applications and estimates the probability with which a link indeed exists. At the heart of our architecture are type-directed encoders ({TDE}), a general framework for elegantly constructing encoders of a compound data type by recursively composing encoders for its constituent types. We evaluate our approach on a large corpus of Android applications, and demonstrate that it achieves very high accuracy. Further, we conduct thorough interpretability studies to understand the internals of the learned neural networks.},
	journaltitle = {{ESEC}/{FSE} 2018},
	author = {Zhao, Jinman and Albarghouthi, Aws and Rastogi, Vaibhav and Jha, Somesh and Octeau, Damien},
	date = {2018},
	file = {Submitted Version:/data/zotero/storage/B3CBHMPQ/Zhao et al. - 2018 - Neural-augmented static analysis of Android commun.pdf:application/pdf}
}

@online{bielik_learning_2017,
	title = {Learning a Static Analyzer from Data},
	url = {/paper/Learning-a-Static-Analyzer-from-Data-Bielik-Raychev/18fc2aa116bf0d6a54eb658932146857ebe229cb},
	abstract = {To be practically useful, modern static analyzers must precisely model the effect of both, statements in the programming language as well as frameworks used by the program under analysis. While important, manually addressing these challenges is difficult for at least two reasons: (i) the effects on the overall analysis can be non-trivial, and (ii) as the size and complexity of modern libraries increase, so is the number of cases the analysis must handle.},
	titleaddon = {undefined},
	author = {Bielik, Pavol and Raychev, Veselin and Vechev, Martin T.},
	urldate = {2020-06-01},
	date = {2017},
	langid = {english},
	note = {Library Catalog: www.semanticscholar.org},
	keywords = {\_tablet},
	file = {Bielik et al_2017_Learning a Static Analyzer from Data.pdf:/data/zotero/storage/ABMPI7VK/Bielik et al_2017_Learning a Static Analyzer from Data.pdf:application/pdf;Snapshot:/data/zotero/storage/FNAS2LIZ/18fc2aa116bf0d6a54eb658932146857ebe229cb.html:text/html}
}

@inproceedings{ho_data-driven_2017,
	title = {Data-Driven Abstraction},
	abstract = {Given a program analysis problem that consists of a program and a property of interest, we use a data-driven approach to automatically construct a sequence of abstractions that approach an ideal abstraction suitable for solving that problem. This process begins with an infinite concrete domain that maps to a finite abstract domain defined by statistical procedures resulting in a clustering mixture model. Given a set of properties expressed as formulas in a restricted and bounded variant of {CTL}, we can test the success of the abstraction with respect to a predefined performance level. In addition, we can perform iterative abstraction-refinement of the clustering by tuning hyperparameters that determine the accuracy of the cluster representations (abstract states) and determine the number of clusters. Our methodology yields an induced abstraction and refinement procedure for property verification.},
	author = {Ho, Vivian M.},
	date = {2017},
	keywords = {⛔ No {DOI} found},
	file = {Full Text PDF:/data/zotero/storage/38X7GQ8L/Ho - 2017 - Data-Driven Abstraction.pdf:application/pdf}
}

@article{jeong_data-driven_2017,
	title = {Data-driven context-sensitivity for points-to analysis},
	doi = {10/gg3j58},
	abstract = {We present a new data-driven approach to achieve highly cost-effective context-sensitive points-to analysis for Java. While context-sensitivity has greater impact on the analysis precision and performance than any other precision-improving techniques, it is difficult to accurately identify the methods that would benefit the most from context-sensitivity and decide how much context-sensitivity should be used for them. Manually designing such rules is a nontrivial and laborious task that often delivers suboptimal results in practice. To overcome these challenges, we propose an automated and data-driven approach that learns to effectively apply context-sensitivity from codebases. In our approach, points-to analysis is equipped with a parameterized and heuristic rules, in disjunctive form of properties on program elements, that decide when and how much to apply context-sensitivity. We present a greedy algorithm that efficiently learns the parameter of the heuristic rules. We implemented our approach in the Doop framework and evaluated using three types of context-sensitive analyses: conventional object-sensitivity, selective hybrid object-sensitivity, and type-sensitivity. In all cases, experimental results show that our approach significantly outperforms existing techniques.},
	journaltitle = {{PACMPL}},
	author = {Jeong, Sehun and Jeon, Minseok and Cha, Sung Deok and Oh, Hakjoo},
	date = {2017},
	file = {Jeong et al_2017_Data-driven context-sensitivity for points-to analysis.pdf:/data/zotero/storage/FMM2FHLK/Jeong et al_2017_Data-driven context-sensitivity for points-to analysis.pdf:application/pdf}
}

@article{jeon_machine-learning_2019,
	title = {A Machine-Learning Algorithm with Disjunctive Model for Data-Driven Program Analysis},
	doi = {10/gg3j5w},
	abstract = {We present a new machine-learning algorithm with disjunctive model for data-driven program analysis. One major challenge in static program analysis is a substantial amount of manual effort required for tuning the analysis performance. Recently, data-driven program analysis has emerged to address this challenge by automatically adjusting the analysis based on data through a learning algorithm. Although this new approach has proven promising for various program analysis tasks, its effectiveness has been limited due to simple-minded learning models and algorithms that are unable to capture sophisticated, in particular disjunctive, program properties. To overcome this shortcoming, this article presents a new disjunctive model for data-driven program analysis as well as a learning algorithm to find the model parameters. Our model uses Boolean formulas over atomic features and therefore is able to express nonlinear combinations of program properties. A key technical challenge is to efficiently determine a set of good Boolean formulas, as brute-force search would simply be impractical. We present a stepwise and greedy algorithm that efficiently learns Boolean formulas. We show the effectiveness and generality of our algorithm with two static analyzers: context-sensitive points-to analysis for Java and flow-sensitive interval analysis for C. Experimental results show that our automated technique significantly improves the performance of the state-of-the-art techniques including ones hand-crafted by human experts.},
	journaltitle = {{TOPL}},
	author = {Jeon, {MinSeok} and Jeong, Sehun and Cha, S. D. and Oh, Hakjoo},
	date = {2019},
	file = {Jeon et al. - 2019 - A Machine-Learning Algorithm with Disjunctive Mode.pdf:/data/zotero/storage/UNT7ZLYM/Jeon et al. - 2019 - A Machine-Learning Algorithm with Disjunctive Mode.pdf:application/pdf}
}

@article{grech_ptaint_2017,
	title = {P/Taint: unified points-to and taint analysis},
	doi = {10/gg3j64},
	shorttitle = {P/Taint},
	abstract = {Static information-flow analysis (especially taint-analysis) is a key technique in software security, computing where sensitive or untrusted data can propagate in a program. Points-to analysis is a fundamental static program analysis, computing what abstract objects a program expression may point to. In this work, we propose a deep unification of information-flow and points-to analysis. We observe that information-flow analysis is not a mere high-level client of points-to information, but it is indeed identical to points-to analysis on artificial abstract objects that represent different information sources. The very same algorithm can compute, simultaneously, two interlinked but separate results (points-to and information-flow values) with changes only to its initial conditions. 
The benefits of such a unification are manifold. We can use existing points-to analysis implementations, with virtually no modification (only minor additions of extra logic for sanitization) to compute information flow concepts, such as value tainting. The algorithmic enhancements of points-to analysis (e.g., different flavors of context sensitivity) can be applied transparently to information-flow analysis. Heavy engineering work on points-to analysis (e.g., handling of the reflection {API} for Java) applies to information-flow analysis without extra effort. We demonstrate the benefits in a realistic implementation that leverages the Doop points-to analysis framework (including its context-sensitivity and reflection analysis features) to provide an information-flow analysis with excellent precision (over 91\%) and recall (over 99\%) for standard Java information-flow benchmarks. 
The analysis comfortably scales to large, real-world Android applications, analyzing the Facebook Messenger app with more than 55K classes in under 7 hours.},
	journaltitle = {{PACMPL}},
	author = {Grech, Neville and Smaragdakis, Yannis},
	date = {2017},
	file = {Full Text:/data/zotero/storage/ESBXH6BC/Grech and Smaragdakis - 2017 - PTaint unified points-to and taint analysis.pdf:application/pdf}
}

@article{upadhyaya_collective_2018,
	title = {Collective Program Analysis},
	doi = {10/gg3j56},
	abstract = {Popularity of data-driven software engineering has led to an increasing demand on the infrastructures to support efficient execution of tasks that require deeper source code analysis. While task optimization and parallelization are the adopted solutions, other research directions are less explored. We present collective program analysis ({CPA}), a technique for scaling large scale source code analyses, especially those that make use of control and data flow analysis, by leveraging analysis specific similarity. Analysis specific similarity is about, whether two or more programs can be considered similar for a given analysis. The key idea of collective program analysis is to cluster programs based on analysis specific similarity, such that running the analysis on one candidate in each cluster is sufficient to produce the result for others. For determining analysis specific similarity and clustering analysis-equivalent programs, we use a sparse representation and a canonical labeling scheme. Our evaluation shows that for a variety of source code analyses on a large dataset of programs, substantial reduction in the analysis time can be achieved; on average a 69\% reduction when compared to a baseline and on average a 36\% reduction when compared to a prior technique. We also found that a large amount of analysis-equivalent programs exists in large datasets.},
	journaltitle = {2018 {IEEE}/{ACM} 40th International Conference on Software Engineering ({ICSE})},
	author = {Upadhyaya, Ganesha and Rajan, Hridesh},
	date = {2018},
	keywords = {\_tablet},
	file = {Upadhyaya_Rajan_2018_Collective Program Analysis.pdf:/data/zotero/storage/KE8ERF8J/Upadhyaya_Rajan_2018_Collective Program Analysis.pdf:application/pdf}
}

@inproceedings{grigore_abstraction_2016,
	title = {Abstraction refinement guided by a learnt probabilistic model},
	doi = {10/gg3j5z},
	abstract = {The core challenge in designing an effective static program analysis is to find a good program abstraction -- one that retains only details relevant to a given query. In this paper, we present a new approach for automatically finding such an abstraction. Our approach uses a pessimistic strategy, which can optionally use guidance from a probabilistic model. Our approach applies to parametric static analyses implemented in Datalog, and is based on counterexample-guided abstraction refinement. For each untried abstraction, our probabilistic model provides a probability of success, while the size of the abstraction provides an estimate of its cost in terms of analysis time. Combining these two metrics, probability and cost, our refinement algorithm picks an optimal abstraction. Our probabilistic model is a variant of the Erdos--Renyi random graph model, and it is tunable by what we call hyperparameters. We present a method to learn good values for these hyperparameters, by observing past runs of the analysis on an existing codebase. We evaluate our approach on an object sensitive pointer analysis for Java programs, with two client analyses ({PolySite} and Downcast).},
	booktitle = {{POPL} 2016},
	author = {Grigore, Radu and Yang, Hongseok},
	date = {2016},
	file = {Accepted Version:/data/zotero/storage/G5SYDIZU/Grigore and Yang - 2016 - Abstraction refinement guided by a learnt probabil.pdf:application/pdf}
}

@article{seidel_learning_2017,
	title = {Learning to blame: localizing novice type errors with data-driven diagnosis},
	doi = {10/gg3j6z},
	shorttitle = {Learning to blame},
	abstract = {Localizing type errors is challenging in languages with global type inference, as the type checker must make assumptions about what the programmer intended to do. We introduce Nate, a data-driven approach to error localization based on supervised learning. Nate analyzes a large corpus of training data -- pairs of ill-typed programs and their "fixed" versions -- to automatically learn a model of where the error is most likely to be found. Given a new ill-typed program, Nate executes the model to generate a list of potential blame assignments ranked by likelihood. We evaluate Nate by comparing its precision to the state of the art on a set of over 5,000 ill-typed {OCaml} programs drawn from two instances of an introductory programming course. We show that when the top-ranked blame assignment is considered, Nate's data-driven model is able to correctly predict the exact sub-expression that should be changed 72\% of the time, 28 points higher than {OCaml} and 16 points higher than the state-of-the-art {SHErrLoc} tool. Furthermore, Nate's accuracy surpasses 85\% when we consider the top two locations and reaches 91\% if we consider the top three.},
	journaltitle = {{PACMPL}},
	author = {Seidel, Eric L. and Sibghat, Huma and Chaudhuri, Kamalika and Weimer, Westley and Jhala, Ranjit},
	date = {2017},
	file = {Full Text:/data/zotero/storage/CP5JKD9S/Seidel et al. - 2017 - Learning to blame localizing novice type errors w.pdf:application/pdf}
}

@article{wang_learning_2019-1,
	title = {Learning Blended, Precise Semantic Program Embeddings},
	url = {http://arxiv.org/abs/1907.02136},
	abstract = {Learning neural program embeddings is key to utilizing deep neural networks in program languages research --- precise and efficient program representations enable the application of deep models to a wide range of program analysis tasks. Existing approaches predominately learn to embed programs from their source code, and, as a result, they do not capture deep, precise program semantics. On the other hand, models learned from runtime information critically depend on the quality of program executions, thus leading to trained models with highly variant quality. This paper tackles these inherent weaknesses of prior approaches by introducing a new deep neural network, {\textbackslash}liger, which learns program representations from a mixture of symbolic and concrete execution traces. We have evaluated {\textbackslash}liger on {\textbackslash}coset, a recently proposed benchmark suite for evaluating neural program embeddings. Results show {\textbackslash}liger (1) is significantly more accurate than the state-of-the-art syntax-based models Gated Graph Neural Network and code2vec in classifying program semantics, and (2) requires on average 10x fewer executions covering 74{\textbackslash}\% fewer paths than the state-of-the-art dynamic model {\textbackslash}dypro. Furthermore, we extend {\textbackslash}liger to predict the name for a method from its body's vector representation. Learning on the same set of functions (more than 170K in total), {\textbackslash}liger significantly outperforms code2seq, the previous state-of-the-art for method name prediction.},
	journaltitle = {{arXiv}:1907.02136 [cs]},
	author = {Wang, Ke and Su, Zhendong},
	urldate = {2020-06-02},
	date = {2019-07-11},
	eprinttype = {arxiv},
	eprint = {1907.02136},
	keywords = {read, ⛔ No {DOI} found},
	file = {Wang_Su_2019_Learning Blended, Precise Semantic Program Embeddings.pdf:/data/zotero/storage/N9XLAV68/Wang_Su_2019_Learning Blended, Precise Semantic Program Embeddings.pdf:application/pdf;arXiv.org Snapshot:/data/zotero/storage/JUDFCZAW/1907.html:text/html}
}

@inproceedings{cummins_end--end_2017,
	title = {End-to-End Deep Learning of Optimization Heuristics},
	doi = {10/gf8rqb},
	abstract = {Accurate automatic optimization heuristics are necessary for dealing with thecomplexity and diversity of modern hardware and software. Machine learning is aproven technique for learning such heuristics, but its success is bound by thequality of the features used. These features must be hand crafted by developersthrough a combination of expert domain knowledge and trial and error. This makesthe quality of the final model directly dependent on the skill and availabletime of the system architect. Our work introduces a better way for building heuristics. We develop a deepneural network that learns heuristics over raw code, entirely without using codefeatures. The neural network simultaneously constructs appropriaterepresentations of the code and learns how best to optimize, removing the needfor manual feature creation. Further, we show that our neural nets can transferlearning from one optimization problem to another, improving the accuracy of newmodels, without the help of human experts. We compare the effectiveness of our automatically generated heuristics againstones with features hand-picked by experts. We examine two challenging tasks:predicting optimal mapping for heterogeneous parallelism and {GPU} threadcoarsening factors. In 89\% of the cases, the quality of our fully automaticheuristics matches or surpasses that of state-of-the-art predictive models usinghand-crafted features, providing on average 14\% and 12\% more performance withno human effort expended on designing features.},
	eventtitle = {2017 26th International Conference on Parallel Architectures and Compilation Techniques ({PACT})},
	pages = {219--232},
	booktitle = {2017 26th International Conference on Parallel Architectures and Compilation Techniques ({PACT})},
	author = {Cummins, Chris and Petoumenos, Pavlos and Wang, Zheng and Leather, Hugh},
	date = {2017-09},
	file = {IEEE Xplore Abstract Record:/data/zotero/storage/G9QMWSJH/8091247.html:text/html;Cummins et al_2017_End-to-End Deep Learning of Optimization Heuristics.pdf:/data/zotero/storage/VX992JK5/Cummins et al_2017_End-to-End Deep Learning of Optimization Heuristics.pdf:application/pdf}
}

@inproceedings{cheung_using_2012,
	location = {Maui, Hawaii, {USA}},
	title = {Using program synthesis for social recommendations},
	isbn = {978-1-4503-1156-4},
	url = {https://doi.org/10.1145/2396761.2398507},
	doi = {10/gf8nnx},
	series = {{CIKM} '12},
	abstract = {This paper presents a new approach to select events of interest to users in a social media setting where events are generated from mobile devices. We argue that the problem is best solved by inductive learning, where the goal is to first generalize from the users' expressed "likes" and "dislikes" of specific events, then to produce a program that can be used to collect only data of interest. The key contribution of this paper is a new algorithm that combines machine learning techniques with program synthesis technology to learn users' preferences. We show that when compared with the more standard approaches, our new algorithm provides up to order-of-magnitude reductions in model training time, and significantly higher prediction accuracies for our target application.1},
	pages = {1732--1736},
	booktitle = {Proceedings of the 21st {ACM} international conference on Information and knowledge management},
	publisher = {Association for Computing Machinery},
	author = {Cheung, Alvin and Solar-Lezama, Armando and Madden, Samuel},
	urldate = {2020-06-13},
	date = {2012-10-29},
	file = {Cheung et al_2012_Using program synthesis for social recommendations.pdf:/data/zotero/storage/GXELQRPU/Cheung et al_2012_Using program synthesis for social recommendations.pdf:application/pdf}
}

@inproceedings{sankaranarayanan_dynamic_2008,
	location = {Seattle, {WA}, {USA}},
	title = {Dynamic inference of likely data preconditions over predicates by tree learning},
	isbn = {978-1-60558-050-0},
	url = {https://doi.org/10.1145/1390630.1390666},
	doi = {10/fpfzhg},
	series = {{ISSTA} '08},
	abstract = {We present a technique to infer likely data preconditions forprocedures written in an imperative programming language. Given a procedure and a set of predicates over its inputs, our technique enumerates different truth assignments to the predicates, deriving test cases from each feasible truth assignment. The predicates themselves are derived automatically using simple heuristics. The enumeration of truth assignments is performed using a propositional {SAT} solver along with a theory satisfiability checker capable of generating unsatisfiable cores. For each assignment of truth values, a corresponding set of test cases are generated and executed. Based on the result of the execution, the truth assignment is classified as being safe or buggy. Finally, a decision tree classifier is used to generate a Boolean formula over the input predicates that explains the data obtained from the test cases. The resulting Boolean formula is, in effect, a likely data precondition for the procedure under consideration. We apply our techniques on a wide variety of functions from the standard C library. Our experiments show that the proposed technique is quite robust. For most cases, it successfully learns a precondition that captures a safe and permissive calling environment.},
	pages = {295--306},
	booktitle = {Proceedings of the 2008 international symposium on Software testing and analysis},
	publisher = {Association for Computing Machinery},
	author = {Sankaranarayanan, Sriram and Chaudhuri, Swarat and Ivančić, Franjo and Gupta, Aarti},
	urldate = {2020-06-13},
	date = {2008-07-20},
	file = {Sankaranarayanan et al_2008_Dynamic inference of likely data preconditions over predicates by tree learning.pdf:/data/zotero/storage/R4AS3JTG/Sankaranarayanan et al_2008_Dynamic inference of likely data preconditions over predicates by tree learning.pdf:application/pdf}
}

@article{zaremba_learning_2015,
	title = {Learning to Execute},
	url = {http://arxiv.org/abs/1410.4615},
	abstract = {Recurrent Neural Networks ({RNNs}) with Long Short-Term Memory units ({LSTM}) are widely used because they are expressive and are easy to train. Our interest lies in empirically evaluating the expressiveness and the learnability of {LSTMs} in the sequence-to-sequence regime by training them to evaluate short computer programs, a domain that has traditionally been seen as too complex for neural networks. We consider a simple class of programs that can be evaluated with a single left-to-right pass using constant memory. Our main result is that {LSTMs} can learn to map the character-level representations of such programs to their correct outputs. Notably, it was necessary to use curriculum learning, and while conventional curriculum learning proved ineffective, we developed a new variant of curriculum learning that improved our networks' performance in all experimental conditions. The improved curriculum had a dramatic impact on an addition problem, making it possible to train an {LSTM} to add two 9-digit numbers with 99\% accuracy.},
	journaltitle = {{arXiv}:1410.4615 [cs]},
	author = {Zaremba, Wojciech and Sutskever, Ilya},
	urldate = {2020-06-13},
	date = {2015-02-19},
	eprinttype = {arxiv},
	eprint = {1410.4615},
	keywords = {⛔ No {DOI} found},
	file = {Zaremba_Sutskever_2015_Learning to Execute.pdf:/data/zotero/storage/A7M54DL5/Zaremba_Sutskever_2015_Learning to Execute.pdf:application/pdf;arXiv.org Snapshot:/data/zotero/storage/ISI6SM53/1410.html:text/html}
}

@article{keller_what_2020,
	title = {What You See is What it Means! Semantic Representation Learning of Code based on Visualization and Transfer Learning},
	abstract = {Recent successes in training word embeddings for {NLP} tasks have encouraged a wave of research on representation learning for source code, which builds on similar {NLP} methods. The overall objective is then to produce code embeddings that capture the maximum of program semantics. State-of-the-art approaches invariably rely on a syntactic representation (i.e., raw lexical tokens, abstract syntax trees, or intermediate representation tokens) to generate embeddings, which are criticized in the literature as non-robust or non-generalizable. In this work, we investigate a novel embedding approach based on the intuition that source code has visual patterns of semantics. We further use these patterns to address the outstanding challenge of identifying semantic code clones. We propose the {WYSIWIM} ("What You See Is What It Means") approach where visual representations of source code are fed into powerful pre-trained image classification neural networks from the field of computer vision to benefit from the practical advantages of transfer learning. We evaluate the proposed embedding approach on two variations of the task of semantic code clone identification: code clone detection (a binary classification problem), and code classification (a multi-classification problem). We show with experiments on the {BigCloneBench} (Java) and Open Judge (C) datasets that although simple, our {WYSIWIM} approach performs as effectively as state of the art approaches such as {ASTNN} or {TBCNN}. We further explore the influence of different steps in our approach, such as the choice of visual representations or the classification algorithm, to eventually discuss the promises and limitations of this research direction.},
	journaltitle = {{ArXiv}},
	author = {Keller, Patrick and Plein, L. N. and Bissyand'e, Tegawend'e F. and Klein, Jacques and Traon, Yves Le},
	date = {2020},
	keywords = {\_tablet, to-read, ⛔ No {DOI} found},
	file = {Keller et al_2020_What You See is What it Means.pdf:/data/zotero/storage/PG3MKWGL/Keller et al_2020_What You See is What it Means.pdf:application/pdf}
}

@article{chae_automatically_2017,
	title = {Automatically generating features for learning program analysis heuristics for C-like languages},
	volume = {1},
	url = {https://doi.org/10.1145/3133925},
	doi = {10/gg3j55},
	abstract = {We present a technique for automatically generating features for data-driven program analyses. Recently data-driven approaches for building a program analysis have been developed, which mine existing codebases and automatically learn heuristics for finding a cost-effective abstraction for a given analysis task. Such approaches reduce the burden of the analysis designers, but they do not remove it completely; they still leave the nontrivial task of designing so called features to the hands of the designers. Our technique aims at automating this feature design process. The idea is to use programs as features after reducing and abstracting them. Our technique goes through selected program-query pairs in codebases, and it reduces and abstracts the program in each pair to a few lines of code, while ensuring that the analysis behaves similarly for the original and the new programs with respect to the query. Each reduced program serves as a boolean feature for program-query pairs. This feature evaluates to true for a given program-query pair when (as a program) it is included in the program part of the pair. We have implemented our approach for three real-world static analyses. The experimental results show that these analyses with automatically-generated features are cost-effective and consistently perform well on a wide range of programs.},
	pages = {101:1--101:25},
	issue = {{OOPSLA}},
	journaltitle = {Proceedings of the {ACM} on Programming Languages},
	shortjournal = {Proc. {ACM} Program. Lang.},
	author = {Chae, Kwonsoo and Oh, Hakjoo and Heo, Kihong and Yang, Hongseok},
	urldate = {2020-06-13},
	date = {2017-10-12},
	file = {Chae et al_2017_Automatically generating features for learning program analysis heuristics for.pdf:/data/zotero/storage/AITBKF9B/Chae et al_2017_Automatically generating features for learning program analysis heuristics for.pdf:application/pdf}
}

@article{wang_search_2018,
	title = {Search, align, and repair: data-driven feedback generation for introductory programming exercises},
	volume = {53},
	issn = {0362-1340},
	url = {https://doi.org/10.1145/3296979.3192384},
	doi = {10/gg3j7c},
	shorttitle = {Search, align, and repair},
	abstract = {This paper introduces the “Search, Align, and Repair” data-driven program repair framework to automate feedback generation for introductory programming exercises. Distinct from existing techniques, our goal is to develop an efficient, fully automated, and problem-agnostic technique for large or {MOOC}-scale introductory programming courses. We leverage the large amount of available student submissions in such settings and develop new algorithms for identifying similar programs, aligning correct and incorrect programs, and repairing incorrect programs by finding minimal fixes. We have implemented our technique in the Sarfgen system and evaluated it on thousands of real student attempts from the Microsoft-{DEV}204.1x {edX} course and the Microsoft {CodeHunt} platform. Our results show that Sarfgen can, within two seconds on average, generate concise, useful feedback for 89.7\% of the incorrect student submissions. It has been integrated with the Microsoft-{DEV}204.1X {edX} class and deployed for production use.},
	pages = {481--495},
	number = {4},
	journaltitle = {{ACM} {SIGPLAN} Notices},
	shortjournal = {{SIGPLAN} Not.},
	author = {Wang, Ke and Singh, Rishabh and Su, Zhendong},
	urldate = {2020-06-13},
	date = {2018-06-11},
	file = {Wang et al_2018_Search, align, and repair.pdf:/data/zotero/storage/XV57H4BF/Wang et al_2018_Search, align, and repair.pdf:application/pdf}
}

@article{wang_coset_2019,
	title = {{COSET}: A Benchmark for Evaluating Neural Program Embeddings},
	url = {http://arxiv.org/abs/1905.11445},
	shorttitle = {{COSET}},
	abstract = {Neural program embedding can be helpful in analyzing large software, a task that is challenging for traditional logic-based program analyses due to their limited scalability. A key focus of recent machine-learning advances in this area is on modeling program semantics instead of just syntax. Unfortunately evaluating such advances is not obvious, as program semantics does not lend itself to straightforward metrics. In this paper, we introduce a benchmarking framework called {COSET} for standardizing the evaluation of neural program embeddings. {COSET} consists of a diverse dataset of programs in source-code format, labeled by human experts according to a number of program properties of interest. A point of novelty is a suite of program transformations included in {COSET}. These transformations when applied to the base dataset can simulate natural changes to program code due to optimization and refactoring and can serve as a "debugging" tool for classification mistakes. We conducted a pilot study on four prominent models: {TreeLSTM}, gated graph neural network ({GGNN}), {AST}-Path neural network ({APNN}), and {DYPRO}. We found that {COSET} is useful in identifying the strengths and limitations of each model and in pinpointing specific syntactic and semantic characteristics of programs that pose challenges.},
	journaltitle = {{arXiv}:1905.11445 [cs, stat]},
	author = {Wang, Ke and Christodorescu, Mihai},
	urldate = {2020-06-13},
	date = {2019-05-27},
	eprinttype = {arxiv},
	eprint = {1905.11445},
	keywords = {\_tablet, read, ⛔ No {DOI} found},
	file = {Wang_Christodorescu_2019_COSET.pdf:/data/zotero/storage/YKCUVZ6L/Wang_Christodorescu_2019_COSET.pdf:application/pdf;arXiv.org Snapshot:/data/zotero/storage/6VVKFAKX/1905.html:text/html}
}

@inproceedings{henkel_code_2018,
	location = {Lake Buena Vista, {FL}, {USA}},
	title = {Code vectors: understanding programs through embedded abstracted symbolic traces},
	isbn = {978-1-4503-5573-5},
	url = {https://doi.org/10.1145/3236024.3236085},
	doi = {10/gf6gbq},
	series = {{ESEC}/{FSE} 2018},
	shorttitle = {Code vectors},
	abstract = {With the rise of machine learning, there is a great deal of interest in treating programs as data to be fed to learning algorithms. However, programs do not start off in a form that is immediately amenable to most off-the-shelf learning techniques. Instead, it is necessary to transform the program to a suitable representation before a learning technique can be applied. In this paper, we use abstractions of traces obtained from symbolic execution of a program as a representation for learning word embeddings. We trained a variety of word embeddings under hundreds of parameterizations, and evaluated each learned embedding on a suite of different tasks. In our evaluation, we obtain 93\% top-1 accuracy on a benchmark consisting of over 19,000 {API}-usage analogies extracted from the Linux kernel. In addition, we show that embeddings learned from (mainly) semantic abstractions provide nearly triple the accuracy of those learned from (mainly) syntactic abstractions.},
	pages = {163--174},
	booktitle = {Proceedings of the 2018 26th {ACM} Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
	publisher = {Association for Computing Machinery},
	author = {Henkel, Jordan and Lahiri, Shuvendu K. and Liblit, Ben and Reps, Thomas},
	urldate = {2020-06-13},
	date = {2018-10-26},
	keywords = {\_tablet, read},
	file = {Henkel et al_2018_Code vectors.pdf:/data/zotero/storage/G7EN5D36/Henkel et al_2018_Code vectors.pdf:application/pdf}
}

@article{proksch_intelligent_2015,
	title = {Intelligent Code Completion with Bayesian Networks},
	volume = {25},
	issn = {1049-331X},
	url = {https://doi.org/10.1145/2744200},
	doi = {10/f73z6p},
	abstract = {Code completion is an integral part of modern Integrated Development Environments ({IDEs}). Developers often use it to explore Application Programming Interfaces ({APIs}). It is also useful to reduce the required amount of typing and to help avoid typos. Traditional code completion systems propose all type-correct methods to the developer. Such a list is often very long with many irrelevant items. More intelligent code completion systems have been proposed in prior work to reduce the list of proposed methods to relevant items. This work extends one of these existing approaches, the Best Matching Neighbor ({BMN}) algorithm. We introduce Bayesian networks as an alternative underlying model, use additional context information for more precise recommendations, and apply clustering techniques to improve model sizes. We compare our new approach, Pattern-based Bayesian Networks ({PBN}), to the existing {BMN} algorithm. We extend previously used evaluation methodologies and, in addition to prediction quality, we also evaluate model size and inference speed. Our results show that the additional context information we collect improves prediction quality, especially for queries that do not contain method calls. We also show that {PBN} can obtain comparable prediction quality to {BMN}, while model size and inference speed scale better with large input sizes.},
	pages = {3:1--3:31},
	number = {1},
	journaltitle = {{ACM} Transactions on Software Engineering and Methodology},
	shortjournal = {{ACM} Trans. Softw. Eng. Methodol.},
	author = {Proksch, Sebastian and Lerch, Johannes and Mezini, Mira},
	urldate = {2020-06-13},
	date = {2015-12-02},
	keywords = {\_tablet, to-read},
	file = {Proksch et al_2015_Intelligent Code Completion with Bayesian Networks.pdf:/data/zotero/storage/FMMJL9VF/Proksch et al_2015_Intelligent Code Completion with Bayesian Networks.pdf:application/pdf}
}

@inproceedings{piech_learning_2015,
	title = {Learning Program Embeddings to Propagate Feedback on Student Code},
	url = {http://proceedings.mlr.press/v37/piech15.html},
	abstract = {Providing feedback, both assessing final work
and giving hints to stuck students, is difficult
for open-ended assignments in massive online
classes which can range from thousands to mil-
lions of students. We introduce a neural network
method to encode programs as a linear mapping
from an embedded precondition space to an em-
bedded postcondition space and propose an al-
gorithm for feedback at scale using these lin-
ear maps as features. We apply our algorithm
to assessments from the Code.org Hour of Code
and Stanford University’s {CS}1 course, where we
propagate human comments on student assign-
ments to orders of magnitude more submissions.},
	eventtitle = {International Conference on Machine Learning},
	pages = {1093--1102},
	booktitle = {International Conference on Machine Learning},
	author = {Piech, Chris and Huang, Jonathan and Nguyen, Andy and Phulsuksombati, Mike and Sahami, Mehran and Guibas, Leonidas},
	urldate = {2020-06-15},
	date = {2015-06-01},
	langid = {english},
	note = {{ISSN}: 1938-7228
Section: Machine Learning},
	keywords = {read},
	file = {Full Text PDF:/data/zotero/storage/VBHQUR9H/Piech et al. - 2015 - Learning Program Embeddings to Propagate Feedback .pdf:application/pdf;Snapshot:/data/zotero/storage/KVWFVIGK/piech15.html:text/html}
}

@article{reed_neural_2016,
	title = {Neural Programmer-Interpreters},
	url = {http://arxiv.org/abs/1511.06279},
	abstract = {We propose the neural programmer-interpreter ({NPI}): a recurrent and compositional neural network that learns to represent and execute programs. {NPI} has three learnable components: a task-agnostic recurrent core, a persistent key-value program memory, and domain-specific encoders that enable a single {NPI} to operate in multiple perceptually diverse environments with distinct affordances. By learning to compose lower-level programs to express higher-level programs, {NPI} reduces sample complexity and increases generalization ability compared to sequence-to-sequence {LSTMs}. The program memory allows efficient learning of additional tasks by building on existing programs. {NPI} can also harness the environment (e.g. a scratch pad with read-write pointers) to cache intermediate results of computation, lessening the long-term memory burden on recurrent hidden units. In this work we train the {NPI} with fully-supervised execution traces; each program has example sequences of calls to the immediate subprograms conditioned on the input. Rather than training on a huge number of relatively weak labels, {NPI} learns from a small number of rich examples. We demonstrate the capability of our model to learn several types of compositional programs: addition, sorting, and canonicalizing 3D models. Furthermore, a single {NPI} learns to execute these programs and all 21 associated subprograms.},
	journaltitle = {{arXiv}:1511.06279 [cs]},
	author = {Reed, Scott and de Freitas, Nando},
	urldate = {2020-06-15},
	date = {2016-02-29},
	eprinttype = {arxiv},
	eprint = {1511.06279},
	keywords = {\_tablet, to-read, ⛔ No {DOI} found},
	file = {Reed_de Freitas_2016_Neural Programmer-Interpreters.pdf:/data/zotero/storage/ZLEZYAPP/Reed_de Freitas_2016_Neural Programmer-Interpreters.pdf:application/pdf;arXiv.org Snapshot:/data/zotero/storage/7HXHHA8H/1511.html:text/html}
}

@inproceedings{chibotaru_scalable_2019,
	location = {Phoenix, {AZ}, {USA}},
	title = {Scalable taint specification inference with big code},
	isbn = {978-1-4503-6712-7},
	url = {https://doi.org/10.1145/3314221.3314648},
	doi = {10/gg3j7b},
	series = {{PLDI} 2019},
	abstract = {We present a new scalable, semi-supervised method for inferring taint analysis specifications by learning from a large dataset of programs. Taint specifications capture the role of library {APIs} (source, sink, sanitizer) and are a critical ingredient of any taint analyzer that aims to detect security violations based on information flow. The core idea of our method is to formulate the taint specification learning problem as a linear optimization task over a large set of information flow constraints. The resulting constraint system can then be efficiently solved with state-of-the-art solvers. Thanks to its scalability, our method can infer many new and interesting taint specifications by simultaneously learning from a large dataset of programs (e.g., as found on {GitHub}), while requiring few manual annotations. We implemented our method in an end-to-end system, called Seldon, targeting Python, a language where static specification inference is particularly hard due to lack of typing information. We show that Seldon is practically effective: it learned almost 7,000 {API} roles from over 210,000 candidate {APIs} with very little supervision (less than 300 annotations) and with high estimated precision (67\%). Further, using the learned specifications, our taint analyzer flagged more than 20,000 violations in open source projects, 97\% of which were undetectable without the inferred specifications.},
	pages = {760--774},
	booktitle = {Proceedings of the 40th {ACM} {SIGPLAN} Conference on Programming Language Design and Implementation},
	publisher = {Association for Computing Machinery},
	author = {Chibotaru, Victor and Bichsel, Benjamin and Raychev, Veselin and Vechev, Martin},
	urldate = {2020-06-15},
	date = {2019-06-08},
	keywords = {low-prio},
	file = {Chibotaru et al_2019_Scalable taint specification inference with big code.pdf:/data/zotero/storage/3KY9YA6B/Chibotaru et al_2019_Scalable taint specification inference with big code.pdf:application/pdf}
}

@inproceedings{padhi_data-driven_2016,
	location = {Santa Barbara, {CA}, {USA}},
	title = {Data-driven precondition inference with learned features},
	isbn = {978-1-4503-4261-2},
	url = {https://doi.org/10.1145/2908080.2908099},
	doi = {10/gg3j59},
	series = {{PLDI} '16},
	abstract = {We extend the data-driven approach to inferring preconditions for code from a set of test executions. Prior work requires a fixed set of features, atomic predicates that define the search space of possible preconditions, to be specified in advance. In contrast, we introduce a technique for on-demand feature learning, which automatically expands the search space of candidate preconditions in a targeted manner as necessary. We have instantiated our approach in a tool called {PIE}. In addition to making precondition inference more expressive, we show how to apply our feature-learning technique to the setting of data-driven loop invariant inference. We evaluate our approach by using {PIE} to infer rich preconditions for black-box {OCaml} library functions and using our loop-invariant inference algorithm as part of an automatic program verifier for C++ programs.},
	pages = {42--56},
	booktitle = {Proceedings of the 37th {ACM} {SIGPLAN} Conference on Programming Language Design and Implementation},
	publisher = {Association for Computing Machinery},
	author = {Padhi, Saswat and Sharma, Rahul and Millstein, Todd},
	urldate = {2020-06-15},
	date = {2016-06-02},
	keywords = {read},
	file = {Padhi et al_2016_Data-driven precondition inference with learned features.pdf:/data/zotero/storage/ZJUPJZ4X/Padhi et al_2016_Data-driven precondition inference with learned features.pdf:application/pdf}
}

@inproceedings{churchill_semantic_2019,
	location = {Phoenix, {AZ}, {USA}},
	title = {Semantic program alignment for equivalence checking},
	isbn = {978-1-4503-6712-7},
	url = {https://doi.org/10.1145/3314221.3314596},
	doi = {10/gg3j7d},
	series = {{PLDI} 2019},
	abstract = {We introduce a robust semantics-driven technique for program equivalence checking. Given two functions we find a trace alignment over a set of concrete executions of both programs and construct a product program particularly amenable to checking equivalence. We demonstrate that our algorithm is applicable to challenging equivalence problems beyond the scope of existing techniques. For example, we verify the correctness of the hand-optimized vector implementation of strlen that ships as part of the {GNU} C Library, as well as the correctness of vectorization optimizations for 56 benchmarks derived from the Test Suite for Vectorizing Compilers.},
	pages = {1027--1040},
	booktitle = {Proceedings of the 40th {ACM} {SIGPLAN} Conference on Programming Language Design and Implementation},
	publisher = {Association for Computing Machinery},
	author = {Churchill, Berkeley and Padon, Oded and Sharma, Rahul and Aiken, Alex},
	urldate = {2020-06-15},
	date = {2019-06-08},
	keywords = {out-of-scope},
	file = {Churchill et al_2019_Semantic program alignment for equivalence checking.pdf:/data/zotero/storage/GKP8IQ7X/Churchill et al_2019_Semantic program alignment for equivalence checking.pdf:application/pdf}
}

@inproceedings{bastani_active_2018,
	location = {Philadelphia, {PA}, {USA}},
	title = {Active learning of points-to specifications},
	isbn = {978-1-4503-5698-5},
	url = {https://doi.org/10.1145/3192366.3192383},
	doi = {10/gg3j52},
	series = {{PLDI} 2018},
	abstract = {When analyzing programs, large libraries pose significant challenges to static points-to analysis. A popular solution is to have a human analyst provide points-to specifications that summarize relevant behaviors of library code, which can substantially improve precision and handle missing code such as native code. We propose Atlas, a tool that automatically infers points-to specifications. Atlas synthesizes unit tests that exercise the library code, and then infers points-to specifications based on observations from these executions. Atlas automatically infers specifications for the Java standard library, and produces better results for a client static information flow analysis on a benchmark of 46 Android apps compared to using existing handwritten specifications.},
	pages = {678--692},
	booktitle = {Proceedings of the 39th {ACM} {SIGPLAN} Conference on Programming Language Design and Implementation},
	publisher = {Association for Computing Machinery},
	author = {Bastani, Osbert and Sharma, Rahul and Aiken, Alex and Liang, Percy},
	urldate = {2020-06-15},
	date = {2018-06-11},
	keywords = {\_tablet, skimmed},
	file = {Bastani et al_2018_Active learning of points-to specifications.pdf:/data/zotero/storage/TZWYUBLY/Bastani et al_2018_Active learning of points-to specifications.pdf:application/pdf}
}

@inproceedings{sharma_data_2013,
	location = {Berlin, Heidelberg},
	title = {A Data Driven Approach for Algebraic Loop Invariants},
	isbn = {978-3-642-37036-6},
	doi = {10/gg3j5t},
	series = {Lecture Notes in Computer Science},
	abstract = {We describe a Guess-and-Check algorithm for computing algebraic equation invariants of the form ∧ i f i (x 1,…,x n ) = 0, where each f i is a polynomial over the variables x 1,…,x n of the program. The “guess” phase is data driven and derives a candidate invariant from data generated from concrete executions of the program. This candidate invariant is subsequently validated in a “check” phase by an off-the-shelf {SMT} solver. Iterating between the two phases leads to a sound algorithm. Moreover, we are able to prove a bound on the number of decision procedure queries which Guess-and-Check requires to obtain a sound invariant. We show how Guess-and-Check can be extended to generate arbitrary boolean combinations of linear equalities as invariants, which enables us to generate expressive invariants to be consumed by tools that cannot handle non-linear arithmetic. We have evaluated our technique on a number of benchmark programs from recent papers on invariant generation. Our results are encouraging – we are able to efficiently compute algebraic invariants in all cases, with only a few tests.},
	pages = {574--592},
	booktitle = {Programming Languages and Systems},
	publisher = {Springer},
	author = {Sharma, Rahul and Gupta, Saurabh and Hariharan, Bharath and Aiken, Alex and Liang, Percy and Nori, Aditya V.},
	editor = {Felleisen, Matthias and Gardner, Philippa},
	date = {2013},
	langid = {english},
	file = {Sharma et al_2013_A Data Driven Approach for Algebraic Loop Invariants.pdf:/data/zotero/storage/G3X9KVWZ/Sharma et al_2013_A Data Driven Approach for Algebraic Loop Invariants.pdf:application/pdf}
}

@article{wang_learning_2020,
	title = {Learning Semantic Program Embeddings with Graph Interval Neural Network},
	url = {http://arxiv.org/abs/2005.09997},
	abstract = {Learning distributed representations of source code has been a challenging task for machine learning models. Earlier works treated programs as text so that natural language methods can be readily applied. Unfortunately, such approaches do not capitalize on the rich structural information possessed by source code. Of late, Graph Neural Network ({GNN}) was proposed to learn embeddings of programs from their graph representations. Due to the homogeneous and expensive message-passing procedure, {GNN} can suffer from precision issues, especially when dealing with programs rendered into large graphs. In this paper, we present a new graph neural architecture, called Graph Interval Neural Network ({GINN}), to tackle the weaknesses of the existing {GNN}. Unlike the standard {GNN}, {GINN} generalizes from a curated graph representation obtained through an abstraction method designed to aid models to learn. In particular, {GINN} focuses exclusively on intervals for mining the feature representation of a program, furthermore, {GINN} operates on a hierarchy of intervals for scaling the learning to large graphs. We evaluate {GINN} for two popular downstream applications: variable misuse prediction and method name prediction. Results show in both cases {GINN} outperforms the state-of-the-art models by a comfortable margin. We have also created a neural bug detector based on {GINN} to catch null pointer deference bugs in Java code. While learning from the same 9,000 methods extracted from 64 projects, {GINN}-based bug detector significantly outperforms {GNN}-based bug detector on 13 unseen test projects. Next, we deploy our trained {GINN}-based bug detector and Facebook Infer to scan the codebase of 20 highly starred projects on {GitHub}. Through our manual inspection, we confirm 38 bugs out of 102 warnings raised by {GINN}-based bug detector compared to 34 bugs out of 129 warnings for Facebook Infer.},
	journaltitle = {{arXiv}:2005.09997 [cs]},
	author = {Wang, Yu and Gao, Fengjuan and Wang, Linzhang and Wang, Ke},
	urldate = {2020-06-15},
	date = {2020-05-26},
	eprinttype = {arxiv},
	eprint = {2005.09997},
	keywords = {\_tablet, read, ⛔ No {DOI} found},
	file = {Wang et al_2020_Learning Semantic Program Embeddings with Graph Interval Neural Network.pdf:/data/zotero/storage/IMPECCRJ/Wang et al_2020_Learning Semantic Program Embeddings with Graph Interval Neural Network.pdf:application/pdf;arXiv.org Snapshot:/data/zotero/storage/3KRZ67XA/2005.html:text/html}
}

@inproceedings{clapp_modelgen_2015,
	location = {Baltimore, {MD}, {USA}},
	title = {Modelgen: mining explicit information flow specifications from concrete executions},
	isbn = {978-1-4503-3620-8},
	url = {https://doi.org/10.1145/2771783.2771810},
	doi = {10/gg3j63},
	series = {{ISSTA} 2015},
	shorttitle = {Modelgen},
	abstract = {We present a technique to mine explicit information flow specifications from concrete executions. These specifications can be consumed by a static taint analysis, enabling static analysis to work even when method definitions are missing or portions of the program are too difficult to analyze statically (e.g., due to dynamic features such as reflection). We present an implementation of our technique for the Android platform. When compared to a set of manually written specifications for 309 methods across 51 classes, our technique is able to recover 96.36\% of these manual specifications and produces many more correct annotations that our manual models missed. We incorporate the generated specifications into an existing static taint analysis system, and show that they enable it to find additional true flows. Although our implementation is Android-specific, our approach is applicable to other application frameworks.},
	pages = {129--140},
	booktitle = {Proceedings of the 2015 International Symposium on Software Testing and Analysis},
	publisher = {Association for Computing Machinery},
	author = {Clapp, Lazaro and Anand, Saswat and Aiken, Alex},
	urldate = {2020-06-15},
	date = {2015-07-13},
	keywords = {out-of-scope},
	file = {Clapp et al_2015_Modelgen.pdf:/data/zotero/storage/6J7IZCHH/Clapp et al_2015_Modelgen.pdf:application/pdf}
}

@article{bader_getafix_2019,
	title = {Getafix: Learning to Fix Bugs Automatically},
	volume = {1902},
	url = {http://adsabs.harvard.edu/abs/2019arXiv190206111B},
	shorttitle = {Getafix},
	abstract = {Static analyzers help find bugs early by warning about recurring bug categories. While fixing these bugs still remains a mostly manual task in practice, we observe that fixes for a specific bug category often are repetitive. This paper addresses the problem of automatically fixing instances of common bugs by learning from past fixes. We present Getafix, an approach that produces human-like fixes while being fast enough to suggest fixes in time proportional to the amount of time needed to obtain static analysis results in the first place. Getafix is based on a novel hierarchical clustering algorithm that summarizes fix patterns into a hierarchy ranging from general to specific patterns. Instead of a computationally expensive exploration of a potentially large space of candidate fixes, Getafix uses a simple yet effective ranking technique that uses the context of a code change to select the most appropriate fix for a given bug. Our evaluation applies Getafix to 1,268 bug fixes for six bug categories reported by popular static analyzers for Java, including null dereferences, incorrect {API} calls, and misuses of particular language constructs. The approach predicts exactly the human-written fix as the top-most suggestion between 12\% and 91\% of the time, depending on the bug category. The top-5 suggestions contain fixes for 526 of the 1,268 bugs. Moreover, we report on
deploying the approach within Facebook, where it contributes to the reliability of software used by billions of people. To the best of our knowledge, Getafix is the first industrially-deployed automated
bug-fixing tool that learns fix patterns from past, human-written fixes to produce human-like fixes.},
	pages = {arXiv:1902.06111},
	journaltitle = {{arXiv} e-prints},
	shortjournal = {{arXiv} e-prints},
	author = {Bader, Johannes and Scott, Andrew and Pradel, Michael and Chandra, Satish},
	urldate = {2020-06-18},
	date = {2019-02-01},
	keywords = {⛔ No {DOI} found},
	file = {Bader et al_2019_Getafix.pdf:/data/zotero/storage/GR59MM6T/Bader et al_2019_Getafix.pdf:application/pdf}
}

@article{ramakrishnan_semantic_2020,
	title = {Semantic Robustness of Models of Source Code},
	url = {http://arxiv.org/abs/2002.03043},
	abstract = {Deep neural networks are vulnerable to adversarial examples - small input perturbations that result in incorrect predictions. We study this problem for models of source code, where we want the network to be robust to source-code modifications that preserve code functionality. (1) We define a powerful adversary that can employ sequences of parametric, semantics-preserving program transformations; (2) we show how to perform adversarial training to learn models robust to such adversaries; (3) we conduct an evaluation on different languages and architectures, demonstrating significant quantitative gains in robustness.},
	journaltitle = {{arXiv}:2002.03043 [cs, stat]},
	author = {Ramakrishnan, Goutham and Henkel, Jordan and Wang, Zi and Albarghouthi, Aws and Jha, Somesh and Reps, Thomas},
	urldate = {2020-06-18},
	date = {2020-06-11},
	eprinttype = {arxiv},
	eprint = {2002.03043},
	keywords = {read, ⛔ No {DOI} found},
	file = {Ramakrishnan et al_2020_Semantic Robustness of Models of Source Code.pdf:/data/zotero/storage/XJQTVDH4/Ramakrishnan et al_2020_Semantic Robustness of Models of Source Code.pdf:application/pdf;arXiv.org Snapshot:/data/zotero/storage/GWWE3QTQ/2002.html:text/html}
}

@inproceedings{hindle_naturalness_2012,
	title = {On the naturalness of software},
	doi = {10/gg3j6w},
	abstract = {Natural languages like English are rich, complex, and powerful. The highly creative and graceful use of languages like English and Tamil, by masters like Shakespeare and Avvaiyar, can certainly delight and inspire. But in practice, given cognitive constraints and the exigencies of daily life, most human utterances are far simpler and much more repetitive and predictable. In fact, these utterances can be very usefully modeled using modern statistical methods. This fact has led to the phenomenal success of statistical approaches to speech recognition, natural language translation, question-answering, and text mining and comprehension. We begin with the conjecture that most software is also natural, in the sense that it is created by humans at work, with all the attendant constraints and limitations - and thus, like natural language, it is also likely to be repetitive and predictable. We then proceed to ask whether a) code can be usefully modeled by statistical language models and b) such models can be leveraged to support software engineers. Using the widely adopted n-gram model, we provide empirical evidence supportive of a positive answer to both these questions. We show that code is also very repetitive, and in fact even more so than natural languages. As an example use of the model, we have developed a simple code completion engine for Java that, despite its simplicity, already improves Eclipse's built-in completion capability. We conclude the paper by laying out a vision for future research in this area.},
	eventtitle = {2012 34th International Conference on Software Engineering ({ICSE})},
	pages = {837--847},
	booktitle = {2012 34th International Conference on Software Engineering ({ICSE})},
	author = {Hindle, Abram and Barr, Earl T. and Su, Zhendong and Gabel, Mark and Devanbu, Premkumar},
	date = {2012-06},
	note = {{ISSN}: 1558-1225},
	keywords = {\_tablet, skimmed},
	file = {Hindle et al_2012_On the naturalness of software.pdf:/data/zotero/storage/6NBC3TI8/Hindle et al_2012_On the naturalness of software.pdf:application/pdf;IEEE Xplore Abstract Record:/data/zotero/storage/K47AQZL2/6227135.html:text/html}
}

@article{hu_codesum_2018,
	title = {{CodeSum}: Translate Program Language to Natural Language},
	url = {http://arxiv.org/abs/1708.01837},
	shorttitle = {{CodeSum}},
	abstract = {During software maintenance, programmers spend a lot of time on code comprehension. Reading comments is an effective way for programmers to reduce the reading and navigating time when comprehending source code. Therefore, as a critical task in software engineering, code summarization aims to generate brief natural language descriptions for source code. In this paper, we propose a new code summarization model named {CodeSum}. {CodeSum} exploits the attention-based sequence-to-sequence (Seq2Seq) neural network with Structure-based Traversal ({SBT}) of Abstract Syntax Trees ({AST}). The {AST} sequences generated by {SBT} can better present the structure of {ASTs} and keep unambiguous. We conduct experiments on three large-scale corpora in different program languages, i.e., Java, C\#, and {SQL}, in which Java corpus is our new proposed industry code extracted from Github. Experimental results show that our method {CodeSum} outperforms the state-of-the-art significantly.},
	journaltitle = {{arXiv}:1708.01837 [cs]},
	author = {Hu, Xing and Wei, Yuhan and Li, Ge and Jin, Zhi},
	urldate = {2020-06-18},
	date = {2018-01-31},
	eprinttype = {arxiv},
	eprint = {1708.01837},
	keywords = {to-read, ⛔ No {DOI} found},
	file = {arXiv.org Snapshot:/data/zotero/storage/UBNA8UZ4/1708.html:text/html}
}

@article{brockschmidt_gnn-film_2019,
	title = {{GNN}-{FiLM}: Graph Neural Networks with Feature-wise Linear Modulation},
	url = {http://arxiv.org/abs/1906.12192},
	shorttitle = {{GNN}-{FiLM}},
	abstract = {This paper presents a new Graph Neural Network ({GNN}) type using feature-wise linear modulation ({FiLM}). Many standard {GNN} variants propagate information along the edges of a graph by computing "messages" based only on the representation of the source of each edge. In {GNN}-{FiLM}, the representation of the target node of an edge is additionally used to compute a transformation that can be applied to all incoming messages, allowing feature-wise modulation of the passed information. Results of experiments comparing different {GNN} architectures on three tasks from the literature are presented, based on re-implementations of baseline methods. Hyperparameters for all methods were found using extensive search, yielding somewhat surprising results: differences between baseline models are smaller than reported in the literature. Nonetheless, {GNN}-{FiLM} outperforms baseline methods on a regression task on molecular graphs and performs competitively on other tasks.},
	journaltitle = {{arXiv}:1906.12192 [cs, stat]},
	author = {Brockschmidt, Marc},
	urldate = {2020-06-18},
	date = {2019-11-03},
	eprinttype = {arxiv},
	eprint = {1906.12192},
	keywords = {⛔ No {DOI} found},
	file = {Brockschmidt_2019_GNN-FiLM.pdf:/data/zotero/storage/8VX8AAR8/Brockschmidt_2019_GNN-FiLM.pdf:application/pdf;arXiv.org Snapshot:/data/zotero/storage/DND3S9GN/1906.html:text/html}
}

@article{bielik_adversarial_2020,
	title = {Adversarial Robustness for Code},
	url = {http://arxiv.org/abs/2002.04694},
	abstract = {We propose a novel technique which addresses the challenge of learning accurate and robust models of code in a principled way. Our method consists of three key components: (i) learning to abstain from making a prediction if uncertain, (ii) adversarial training, and (iii) representation refinement which learns the program parts relevant for the prediction and abstracts the rest. These components are used to iteratively train multiple models, each of which learns a suitable program representation necessary to make robust predictions on a different subset of the dataset. We instantiated our approach to the task of type inference for dynamically typed languages and demonstrate its effectiveness by learning a model that achieves 88\% accuracy and 84\% robustness. Further, our evaluation shows that using the combination of all three components is key to obtaining accurate and robust models.},
	journaltitle = {{arXiv}:2002.04694 [cs, stat]},
	author = {Bielik, Pavol and Vechev, Martin},
	urldate = {2020-06-18},
	date = {2020-02-11},
	eprinttype = {arxiv},
	eprint = {2002.04694},
	keywords = {read, mpnn, ⛔ No {DOI} found},
	file = {Bielik_Vechev_2020_Adversarial Robustness for Code.pdf:/data/zotero/storage/F6JXFY3U/Bielik_Vechev_2020_Adversarial Robustness for Code.pdf:application/pdf;arXiv.org Snapshot:/data/zotero/storage/J6U3W82U/2002.html:text/html}
}

@inproceedings{brockschmidt_generative_2018,
	title = {Generative Code Modeling with Graphs},
	url = {https://openreview.net/forum?id=Bke4KsA5FX},
	abstract = {Generative models forsource code are an interesting structured prediction problem, requiring to reason about both hard syntactic and semantic constraints as well as about natural, likely programs....},
	eventtitle = {International Conference on Learning Representations},
	author = {Brockschmidt, Marc and Allamanis, Miltiadis and Gaunt, Alexander L. and Polozov, Oleksandr},
	urldate = {2020-06-18},
	date = {2018-09-27},
	file = {Brockschmidt et al_2018_Generative Code Modeling with Graphs.pdf:/data/zotero/storage/LTBVTBKW/Brockschmidt et al_2018_Generative Code Modeling with Graphs.pdf:application/pdf;Snapshot:/data/zotero/storage/VZHNHCD2/forum.html:text/html}
}

@article{rabin_testing_2019,
	title = {Testing Neural Program Analyzers},
	url = {http://arxiv.org/abs/1908.10711},
	abstract = {Deep neural networks have been increasingly used in software engineering and program analysis tasks. They usually take a program and make some predictions about it, e.g., bug prediction. We call these models neural program analyzers. The reliability of neural programs can impact the reliability of the encompassing analyses. In this paper, we describe our ongoing efforts to develop effective techniques for testing neural programs. We discuss the challenges involved in developing such tools and our future plans. In our preliminary experiment on a neural model recently proposed in the literature, we found that the model is very brittle, and simple perturbations in the input can cause the model to make mistakes in its prediction.},
	journaltitle = {{arXiv}:1908.10711 [cs, stat]},
	author = {Rabin, Md Rafiqul Islam and Wang, Ke and Alipour, Mohammad Amin},
	urldate = {2020-06-18},
	date = {2019-09-25},
	eprinttype = {arxiv},
	eprint = {1908.10711},
	keywords = {\_tablet, read, ⛔ No {DOI} found},
	file = {Rabin et al_2019_Testing Neural Program Analyzers.pdf:/data/zotero/storage/HCEQLPSV/Rabin et al_2019_Testing Neural Program Analyzers.pdf:application/pdf;arXiv.org Snapshot:/data/zotero/storage/7AYW2MB7/1908.html:text/html}
}

@inproceedings{tai_improved_2015,
	location = {Beijing, China},
	title = {Improved Semantic Representations From Tree-Structured Long Short-Term Memory Networks},
	url = {https://www.aclweb.org/anthology/P15-1150},
	doi = {10/gfshwj},
	abstract = {Because of their superior ability to preserve sequence information over time, Long Short-Term Memory ({LSTM}) networks, a type of recurrent neural network with a more complex computational unit, have obtained strong results on a variety of sequence modeling tasks. The only underlying {LSTM} structure that has been explored so far is a linear chain. However, natural language exhibits syntactic properties that would naturally combine words to phrases. We introduce the Tree-{LSTM}, a generalization of {LSTMs} to tree-structured network topologies. Tree-{LSTMs} outperform all existing systems and strong {LSTM} baselines on two tasks: predicting the semantic relatedness of two sentences ({SemEval} 2014, Task 1) and sentiment classification (Stanford Sentiment Treebank).},
	eventtitle = {{ACL}-{IJCNLP} 2015},
	pages = {1556--1566},
	booktitle = {Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)},
	publisher = {Association for Computational Linguistics},
	author = {Tai, Kai Sheng and Socher, Richard and Manning, Christopher D.},
	urldate = {2020-06-18},
	date = {2015-07},
	file = {Tai et al_2015_Improved Semantic Representations From Tree-Structured Long Short-Term Memory.pdf:/data/zotero/storage/VGXNLNEU/Tai et al_2015_Improved Semantic Representations From Tree-Structured Long Short-Term Memory.pdf:application/pdf}
}

@article{shido_automatic_2019,
	title = {Automatic Source Code Summarization with Extended Tree-{LSTM}},
	url = {http://arxiv.org/abs/1906.08094},
	abstract = {Neural machine translation models are used to automatically generate a document from given source code since this can be regarded as a machine translation task. Source code summarization is one of the components for automatic document generation, which generates a summary in natural language from given source code. This suggests that techniques used in neural machine translation, such as Long Short-Term Memory ({LSTM}), can be used for source code summarization. However, there is a considerable difference between source code and natural language: Source code is essentially \{{\textbackslash}em structured\}, having loops and conditional branching, etc. Therefore, there is some obstacle to apply known machine translation models to source code. Abstract syntax trees ({ASTs}) capture these structural properties and play an important role in recent machine learning studies on source code. Tree-{LSTM} is proposed as a generalization of {LSTMs} for tree-structured data. However, there is a critical issue when applying it to {ASTs}: It cannot handle a tree that contains nodes having an arbitrary number of children and their order simultaneously, which {ASTs} generally have such nodes. To address this issue, we propose an extension of Tree-{LSTM}, which we call {\textbackslash}emph\{Multi-way Tree-{LSTM}\} and apply it for source code summarization. As a result of computational experiments, our proposal achieved better results when compared with several state-of-the-art techniques.},
	journaltitle = {{arXiv}:1906.08094 [cs, stat]},
	author = {Shido, Yusuke and Kobayashi, Yasuaki and Yamamoto, Akihiro and Miyamoto, Atsushi and Matsumura, Tadayuki},
	urldate = {2020-06-18},
	date = {2019-06-20},
	eprinttype = {arxiv},
	eprint = {1906.08094},
	keywords = {\_tablet, to-read, ⛔ No {DOI} found},
	file = {Shido et al_2019_Automatic Source Code Summarization with Extended Tree-LSTM.pdf:/data/zotero/storage/VSU7M6L2/Shido et al_2019_Automatic Source Code Summarization with Extended Tree-LSTM.pdf:application/pdf;arXiv.org Snapshot:/data/zotero/storage/8DH6IZZN/1906.html:text/html}
}

@article{ahmad_transformer-based_2020,
	title = {A Transformer-based Approach for Source Code Summarization},
	url = {http://arxiv.org/abs/2005.00653},
	abstract = {Generating a readable summary that describes the functionality of a program is known as source code summarization. In this task, learning code representation by modeling the pairwise relationship between code tokens to capture their long-range dependencies is crucial. To learn code representation for summarization, we explore the Transformer model that uses a self-attention mechanism and has shown to be effective in capturing long-range dependencies. In this work, we show that despite the approach is simple, it outperforms the state-of-the-art techniques by a significant margin. We perform extensive analysis and ablation studies that reveal several important findings, e.g., the absolute encoding of source code tokens' position hinders, while relative encoding significantly improves the summarization performance. We have made our code publicly available to facilitate future research.},
	journaltitle = {{arXiv}:2005.00653 [cs, stat]},
	author = {Ahmad, Wasi Uddin and Chakraborty, Saikat and Ray, Baishakhi and Chang, Kai-Wei},
	urldate = {2020-06-18},
	date = {2020-05-01},
	eprinttype = {arxiv},
	eprint = {2005.00653},
	keywords = {⛔ No {DOI} found},
	file = {Ahmad et al_2020_A Transformer-based Approach for Source Code Summarization.pdf:/data/zotero/storage/TWA6SBGL/Ahmad et al_2020_A Transformer-based Approach for Source Code Summarization.pdf:application/pdf;arXiv.org Snapshot:/data/zotero/storage/NQD6SVWB/2005.html:text/html}
}

@incollection{ben-nun_neural_2018,
	title = {Neural Code Comprehension: A Learnable Representation of Code Semantics},
	url = {http://papers.nips.cc/paper/7617-neural-code-comprehension-a-learnable-representation-of-code-semantics.pdf},
	shorttitle = {Neural Code Comprehension},
	abstract = {With the recent success of embeddings in natural language processing, research has been conducted into applying similar methods to code analysis. Most works attempt to process the code directly or use a syntactic tree representation, treating it like sentences written in a natural language. However, none of the existing methods are sufficient to comprehend program semantics robustly, due to structural features such as function calls, branching, and interchangeable order of statements. In this paper, we propose a novel processing technique to learn code semantics, and apply it to a variety of program analysis tasks. In particular, we stipulate that a robust distributional hypothesis of code applies to both human- and machine-generated programs. Following this hypothesis, we define an embedding space, inst2vec, based on an Intermediate Representation ({IR}) of the code that is independent of the source programming language. We provide a novel definition of contextual flow for this {IR}, leveraging both the underlying data- and control-flow of the program. We then analyze the embeddings qualitatively using analogies and clustering, and evaluate the learned representation on three different high-level tasks. We show that even without fine-tuning, a single {RNN} architecture and fixed inst2vec embeddings outperform specialized approaches for performance prediction (compute device mapping, optimal thread coarsening); and algorithm classification from raw code (104 classes), where we set a new state-of-the-art.},
	pages = {3585--3597},
	booktitle = {Advances in Neural Information Processing Systems 31},
	publisher = {Curran Associates, Inc.},
	author = {Ben-Nun, Tal and Jakobovits, Alice Shoshana and Hoefler, Torsten},
	editor = {Bengio, S. and Wallach, H. and Larochelle, H. and Grauman, K. and Cesa-Bianchi, N. and Garnett, R.},
	urldate = {2020-06-19},
	date = {2018},
	keywords = {\_tablet, read},
	file = {Ben-Nun et al_2018_Neural Code Comprehension.pdf:/data/zotero/storage/LLSXY9JL/Ben-Nun et al_2018_Neural Code Comprehension.pdf:application/pdf;NIPS Snapshot:/data/zotero/storage/KK7AGQ6Y/7617-neural-code-comprehension-a-learnable-representation-of-code-semantics.html:text/html}
}

@inproceedings{mou_convolutional_2016,
	title = {Convolutional Neural Networks over Tree Structures for Programming Language Processing},
	rights = {Authors who publish a paper in this conference agree to the following terms:   Author(s) agree to transfer their copyrights in their article/paper to the Association for the Advancement of Artificial Intelligence ({AAAI}), in order to deal with future requests for reprints, translations, anthologies, reproductions, excerpts, and other publications. This grant will include, without limitation, the entire copyright in the article/paper in all countries of the world, including all renewals, extensions, and reversions thereof, whether such rights current exist or hereafter come into effect, and also the exclusive right to create electronic versions of the article/paper, to the extent that such right is not subsumed under copyright.  The author(s) warrants that they are the sole author and owner of the copyright in the above article/paper, except for those portions shown to be in quotations; that the article/paper is original throughout; and that the undersigned right to make the grants set forth above is complete and unencumbered.  The author(s) agree that if anyone brings any claim or action alleging facts that, if true, constitute a breach of any of the foregoing warranties, the author(s) will hold harmless and indemnify {AAAI}, their grantees, their licensees, and their distributors against any liability, whether under judgment, decree, or compromise, and any legal fees and expenses arising out of that claim or actions, and the undersigned will cooperate fully in any defense {AAAI} may make to such claim or action. Moreover, the undersigned agrees to cooperate in any claim or other action seeking to protect or enforce any right the undersigned has granted to {AAAI} in the article/paper. If any such claim or action fails because of facts that constitute a breach of any of the foregoing warranties, the undersigned agrees to reimburse whomever brings such claim or action for expenses and attorneys’ fees incurred therein.  Author(s) retain all proprietary rights other than copyright (such as patent rights).  Author(s) may make personal reuse of all or portions of the above article/paper in other works of their own authorship.  Author(s) may reproduce, or have reproduced, their article/paper for the author’s personal use, or for company use provided that {AAAI} copyright and the source are indicated, and that the copies are not used in a way that implies {AAAI} endorsement of a product or service of an employer, and that the copies per se are not offered for sale. The foregoing right shall not permit the posting of the article/paper in electronic or digital form on any computer network, except by the author or the author’s employer, and then only on the author’s or the employer’s own web page or ftp site. Such web page or ftp site, in addition to the aforementioned requirements of this Paragraph, must provide an electronic reference or link back to the {AAAI} electronic server, and shall not post other {AAAI} copyrighted materials not of the author’s or the employer’s creation (including tables of contents with links to other papers) without {AAAI}’s written permission.  Author(s) may make limited distribution of all or portions of their article/paper prior to publication.  In the case of work performed under U.S. Government contract, {AAAI} grants the U.S. Government royalty-free permission to reproduce all or portions of the above article/paper, and to authorize others to do so, for U.S. Government purposes.  In the event the above article/paper is not accepted and published by {AAAI}, or is withdrawn by the author(s) before acceptance by {AAAI}, this agreement becomes null and void.},
	url = {https://www.aaai.org/ocs/index.php/AAAI/AAAI16/paper/view/11775},
	abstract = {Programming language processing (similar to natural language processing) is a hot research topic in the field of software engineering; it has also aroused growing interest in the artificial intelligence community. However, different from a natural language sentence, a program contains rich, explicit, and complicated structural information. Hence, traditional {NLP} models may be inappropriate for programs. In this paper, we propose a novel tree-based convolutional neural network ({TBCNN}) for programming language processing, in which a convolution kernel is designed over programs' abstract syntax trees to capture structural information. {TBCNN} is a generic architecture for programming language processing; our experiments show its effectiveness in two different program analysis tasks: classifying programs according to functionality, and detecting code snippets of certain patterns. {TBCNN} outperforms baseline methods, including several neural models for {NLP}.},
	eventtitle = {Thirtieth {AAAI} Conference on Artificial Intelligence},
	booktitle = {Thirtieth {AAAI} Conference on Artificial Intelligence},
	author = {Mou, Lili and Li, Ge and Zhang, Lu and Wang, Tao and Jin, Zhi},
	urldate = {2020-06-19},
	date = {2016-02-21},
	langid = {english},
	keywords = {dataset, skimmed},
	file = {Mou et al_2016_Convolutional Neural Networks over Tree Structures for Programming Language.pdf:/data/zotero/storage/EJ8HYDXP/Mou et al_2016_Convolutional Neural Networks over Tree Structures for Programming Language.pdf:application/pdf;Snapshot:/data/zotero/storage/9UQ37QMH/11775.html:text/html}
}

@inproceedings{xu_neural_2017,
	location = {Dallas, Texas, {USA}},
	title = {Neural Network-based Graph Embedding for Cross-Platform Binary Code Similarity Detection},
	isbn = {978-1-4503-4946-8},
	url = {https://doi.org/10.1145/3133956.3134018},
	doi = {10/gg3j6s},
	series = {{CCS} '17},
	abstract = {The problem of cross-platform binary code similarity detection aims at detecting whether two binary functions coming from different platforms are similar or not. It has many security applications, including plagiarism detection, malware detection, vulnerability search, etc. Existing approaches rely on approximate graph-matching algorithms, which are inevitably slow and sometimes inaccurate, and hard to adapt to a new task. To address these issues, in this work, we propose a novel neural network-based approach to compute the embedding, i.e., a numeric vector, based on the control flow graph of each binary function, then the similarity detection can be done efficiently by measuring the distance between the embeddings for two functions. We implement a prototype called Gemini. Our extensive evaluation shows that Gemini outperforms the state-of-the-art approaches by large margins with respect to similarity detection accuracy. Further, Gemini can speed up prior art's embedding generation time by 3 to 4 orders of magnitude and reduce the required training time from more than 1 week down to 30 minutes to 10 hours. Our real world case studies demonstrate that Gemini can identify significantly more vulnerable firmware images than the state-of-the-art, i.e., Genius. Our research showcases a successful application of deep learning on computer security problems.},
	pages = {363--376},
	booktitle = {Proceedings of the 2017 {ACM} {SIGSAC} Conference on Computer and Communications Security},
	publisher = {Association for Computing Machinery},
	author = {Xu, Xiaojun and Liu, Chang and Feng, Qian and Yin, Heng and Song, Le and Song, Dawn},
	urldate = {2020-06-19},
	date = {2017-10-30},
	keywords = {out-of-scope},
	file = {Xu et al_2017_Neural Network-based Graph Embedding for Cross-Platform Binary Code Similarity.pdf:/data/zotero/storage/LBX7M6Z2/Xu et al_2017_Neural Network-based Graph Embedding for Cross-Platform Binary Code Similarity.pdf:application/pdf}
}

@inproceedings{xu_python_2016,
	location = {Seattle, {WA}, {USA}},
	title = {Python probabilistic type inference with natural language support},
	isbn = {978-1-4503-4218-6},
	url = {https://doi.org/10.1145/2950290.2950343},
	doi = {10/gg3j68},
	series = {{FSE} 2016},
	abstract = {We propose a novel type inference technique for Python programs. Type inference is difficult for Python programs due to their heavy dependence on external {APIs} and the dynamic language features. We observe that Python source code often contains a lot of type hints such as attribute accesses and variable names. However, such type hints are not reliable. We hence propose to use probabilistic inference to allow the beliefs of individual type hints to be propagated, aggregated, and eventually converge on probabilities of variable types. Our results show that our technique substantially outperforms a state-of-the-art Python type inference engine based on abstract interpretation.},
	pages = {607--618},
	booktitle = {Proceedings of the 2016 24th {ACM} {SIGSOFT} International Symposium on Foundations of Software Engineering},
	publisher = {Association for Computing Machinery},
	author = {Xu, Zhaogui and Zhang, Xiangyu and Chen, Lin and Pei, Kexin and Xu, Baowen},
	urldate = {2020-06-23},
	date = {2016-11-01},
	keywords = {to-read},
	file = {Xu et al_2016_Python probabilistic type inference with natural language support.pdf:/data/zotero/storage/PRSWI5Y6/Xu et al_2016_Python probabilistic type inference with natural language support.pdf:application/pdf}
}

@inproceedings{defreez_path-based_2018,
	location = {Lake Buena Vista, {FL}, {USA}},
	title = {Path-based function embedding and its application to error-handling specification mining},
	isbn = {978-1-4503-5573-5},
	url = {https://doi.org/10.1145/3236024.3236059},
	doi = {10/gg3j65},
	series = {{ESEC}/{FSE} 2018},
	abstract = {Identifying relationships among program elements is useful for program understanding, debugging, and analysis. One such kind of relationship is synonymy. Function synonyms are functions that play a similar role in code; examples include functions that perform initialization for different device drivers, and functions that implement different symmetric-key encryption schemes. Function synonyms are not necessarily semantically equivalent and can be syntactically dissimilar; consequently, approaches for identifying code clones or functional equivalence cannot be used to identify them. This paper presents Func2{\textless}pre{\textgreater}vec{\textless}/pre{\textgreater}, a technique that learns an embedding mapping each function to a vector in a continuous vector space such that vectors for function synonyms are in close proximity. We compute the function embedding by training a neural network on sentences generated using random walks over the interprocedural control-flow graph. We show the effectiveness of Func2{\textless}pre{\textgreater}vec{\textless}/pre{\textgreater} at identifying function synonyms in the Linux kernel. Finally, we apply Func2{\textless}pre{\textgreater}vec{\textless}/pre{\textgreater} to the problem of mining error-handling specifications in Linux file systems and drivers. We show that the function synonyms identified by Func2{\textless}pre{\textgreater}vec{\textless}/pre{\textgreater} result in error-handling specifications with high support.},
	pages = {423--433},
	booktitle = {Proceedings of the 2018 26th {ACM} Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
	publisher = {Association for Computing Machinery},
	author = {{DeFreez}, Daniel and Thakur, Aditya V. and Rubio-González, Cindy},
	urldate = {2020-06-23},
	date = {2018-10-26},
	keywords = {\_tablet, to-read},
	file = {DeFreez et al_2018_Path-based function embedding and its application to error-handling.pdf:/data/zotero/storage/C9RG9W4J/DeFreez et al_2018_Path-based function embedding and its application to error-handling.pdf:application/pdf}
}

@article{schrouff_inferring_2019,
	title = {Inferring Javascript types using Graph Neural Networks},
	url = {http://arxiv.org/abs/1905.06707},
	abstract = {The recent use of `Big Code' with state-of-the-art deep learning methods offers promising avenues to ease program source code writing and correction. As a first step towards automatic code repair, we implemented a graph neural network model that predicts token types for Javascript programs. The predictions achieve an accuracy above \$90{\textbackslash}\%\$, which improves on previous similar work.},
	journaltitle = {{arXiv}:1905.06707 [cs, stat]},
	author = {Schrouff, Jessica and Wohlfahrt, Kai and Marnette, Bruno and Atkinson, Liam},
	urldate = {2020-06-24},
	date = {2019-05-16},
	eprinttype = {arxiv},
	eprint = {1905.06707},
	keywords = {read, mpnn, ⛔ No {DOI} found},
	file = {Schrouff et al_2019_Inferring Javascript types using Graph Neural Networks.pdf:/data/zotero/storage/XUGRZKWV/Schrouff et al_2019_Inferring Javascript types using Graph Neural Networks.pdf:application/pdf;arXiv.org Snapshot:/data/zotero/storage/TZMFR92B/1905.html:text/html}
}

@article{chen_literature_2019,
	title = {A Literature Study of Embeddings on Source Code},
	url = {http://arxiv.org/abs/1904.03061},
	abstract = {Natural language processing has improved tremendously after the success of word embedding techniques such as word2vec. Recently, the same idea has been applied on source code with encouraging results. In this survey, we aim to collect and discuss the usage of word embedding techniques on programs and source code. The articles in this survey have been collected by asking authors of related work and with an extensive search on Google Scholar. Each article is categorized into five categories: 1. embedding of tokens 2. embedding of functions or methods 3. embedding of sequences or sets of method calls 4. embedding of binary code 5. other embeddings. We also provide links to experimental data and show some remarkable visualization of code embeddings. In summary, word embedding has been successfully applied on different granularities of source code. With access to countless open-source repositories, we see a great potential of applying other data-driven natural language processing techniques on source code in the future.},
	journaltitle = {{arXiv}:1904.03061 [cs, stat]},
	author = {Chen, Zimin and Monperrus, Martin},
	urldate = {2020-06-24},
	date = {2019-04-05},
	eprinttype = {arxiv},
	eprint = {1904.03061},
	keywords = {read, ⛔ No {DOI} found},
	file = {Chen_Monperrus_2019_A Literature Study of Embeddings on Source Code.pdf:/data/zotero/storage/YB7DAWYF/Chen_Monperrus_2019_A Literature Study of Embeddings on Source Code.pdf:application/pdf;arXiv.org Snapshot:/data/zotero/storage/IM87K93M/1904.html:text/html}
}

@article{alon_code2seq_2019,
	title = {code2seq: Generating Sequences from Structured Representations of Code},
	url = {http://arxiv.org/abs/1808.01400},
	shorttitle = {code2seq},
	abstract = {The ability to generate natural language sequences from source code snippets has a variety of applications such as code summarization, documentation, and retrieval. Sequence-to-sequence (seq2seq) models, adopted from neural machine translation ({NMT}), have achieved state-of-the-art performance on these tasks by treating source code as a sequence of tokens. We present \$\{{\textbackslash}rm \{{\textbackslash}scriptsize {CODE}2SEQ\}\}\$: an alternative approach that leverages the syntactic structure of programming languages to better encode source code. Our model represents a code snippet as the set of compositional paths in its abstract syntax tree ({AST}) and uses attention to select the relevant paths while decoding. We demonstrate the effectiveness of our approach for two tasks, two programming languages, and four datasets of up to \$16\$M examples. Our model significantly outperforms previous models that were specifically designed for programming languages, as well as state-of-the-art {NMT} models. An interactive online demo of our model is available at http://code2seq.org. Our code, data and trained models are available at http://github.com/tech-srl/code2seq.},
	journaltitle = {{arXiv}:1808.01400 [cs, stat]},
	author = {Alon, Uri and Brody, Shaked and Levy, Omer and Yahav, Eran},
	urldate = {2020-06-24},
	date = {2019-02-21},
	eprinttype = {arxiv},
	eprint = {1808.01400},
	keywords = {⛔ No {DOI} found},
	file = {Alon et al_2019_code2seq.pdf:/data/zotero/storage/G8HURE7G/Alon et al_2019_code2seq.pdf:application/pdf;arXiv.org Snapshot:/data/zotero/storage/MLX724AD/1808.html:text/html}
}

@article{zhou_graph_2019,
	title = {Graph Neural Networks: A Review of Methods and Applications},
	url = {http://arxiv.org/abs/1812.08434},
	shorttitle = {Graph Neural Networks},
	abstract = {Lots of learning tasks require dealing with graph data which contains rich relation information among elements. Modeling physics system, learning molecular fingerprints, predicting protein interface, and classifying diseases require a model to learn from graph inputs. In other domains such as learning from non-structural data like texts and images, reasoning on extracted structures, like the dependency tree of sentences and the scene graph of images, is an important research topic which also needs graph reasoning models. Graph neural networks ({GNNs}) are connectionist models that capture the dependence of graphs via message passing between the nodes of graphs. Unlike standard neural networks, graph neural networks retain a state that can represent information from its neighborhood with arbitrary depth. Although the primitive {GNNs} have been found difficult to train for a fixed point, recent advances in network architectures, optimization techniques, and parallel computation have enabled successful learning with them. In recent years, systems based on variants of graph neural networks such as graph convolutional network ({GCN}), graph attention network ({GAT}), gated graph neural network ({GGNN}) have demonstrated ground-breaking performance on many tasks mentioned above. In this survey, we provide a detailed review over existing graph neural network models, systematically categorize the applications, and propose four open problems for future research.},
	journaltitle = {{arXiv}:1812.08434 [cs, stat]},
	author = {Zhou, Jie and Cui, Ganqu and Zhang, Zhengyan and Yang, Cheng and Liu, Zhiyuan and Wang, Lifeng and Li, Changcheng and Sun, Maosong},
	urldate = {2020-06-24},
	date = {2019-07-10},
	eprinttype = {arxiv},
	eprint = {1812.08434},
	keywords = {⛔ No {DOI} found},
	file = {Zhou et al_2019_Graph Neural Networks.pdf:/data/zotero/storage/W7G7IGR7/Zhou et al_2019_Graph Neural Networks.pdf:application/pdf;arXiv.org Snapshot:/data/zotero/storage/ZLSV22XP/1812.html:text/html}
}

@article{le_deep_2020,
	title = {Deep Learning for Source Code Modeling and Generation: Models, Applications, and Challenges},
	volume = {53},
	issn = {0360-0300},
	url = {https://doi.org/10.1145/3383458},
	doi = {10/gg3j6b},
	shorttitle = {Deep Learning for Source Code Modeling and Generation},
	abstract = {Deep Learning ({DL}) techniques for Natural Language Processing have been evolving remarkably fast. Recently, the {DL} advances in language modeling, machine translation, and paragraph understanding are so prominent that the potential of {DL} in Software Engineering cannot be overlooked, especially in the field of program learning. To facilitate further research and applications of {DL} in this field, we provide a comprehensive review to categorize and investigate existing {DL} methods for source code modeling and generation. To address the limitations of the traditional source code models, we formulate common program learning tasks under an encoder-decoder framework. After that, we introduce recent {DL} mechanisms suitable to solve such problems. Then, we present the state-of-the-art practices and discuss their challenges with some recommendations for practitioners and researchers as well.},
	pages = {62:1--62:38},
	number = {3},
	journaltitle = {{ACM} Computing Surveys},
	shortjournal = {{ACM} Comput. Surv.},
	author = {Le, Triet H. M. and Chen, Hao and Babar, Muhammad Ali},
	urldate = {2020-06-24},
	date = {2020-06-12},
	file = {Le et al_2020_Deep Learning for Source Code Modeling and Generation.pdf:/data/zotero/storage/ZAXLNWEW/Le et al_2020_Deep Learning for Source Code Modeling and Generation.pdf:application/pdf}
}

@inproceedings{kang_assessing_2019,
	title = {Assessing the Generalizability of Code2vec Token Embeddings},
	doi = {10/ggsskz},
	abstract = {Many Natural Language Processing ({NLP}) tasks, such as sentiment analysis or syntactic parsing, have benefited from the development of word embedding models. In particular, regardless of the training algorithms, the learned embeddings have often been shown to be generalizable to different {NLP} tasks. In contrast, despite recent momentum on word embeddings for source code, the literature lacks evidence of their generalizability beyond the example task they have been trained for. In this experience paper, we identify 3 potential downstream tasks, namely code comments generation, code authorship identification, and code clones detection, that source code token embedding models can be applied to. We empirically assess a recently proposed code token embedding model, namely code2vec's token embeddings. Code2vec was trained on the task of predicting method names, and while there is potential for using the vectors it learns on other tasks, it has not been explored in literature. Therefore, we fill this gap by focusing on its generalizability for the tasks we have identified. Eventually, we show that source code token embeddings cannot be readily leveraged for the downstream tasks. Our experiments even show that our attempts to use them do not result in any improvements over less sophisticated methods. We call for more research into effective and general use of code embeddings.},
	eventtitle = {2019 34th {IEEE}/{ACM} International Conference on Automated Software Engineering ({ASE})},
	pages = {1--12},
	booktitle = {2019 34th {IEEE}/{ACM} International Conference on Automated Software Engineering ({ASE})},
	author = {Kang, Hong Jin and Bissyandé, Tegawendé F. and Lo, David},
	date = {2019-11},
	note = {{ISSN}: 2643-1572},
	file = {Kang et al_2019_Assessing the Generalizability of Code2vec Token Embeddings.pdf:/data/zotero/storage/H9NA7GKU/Kang et al_2019_Assessing the Generalizability of Code2vec Token Embeddings.pdf:application/pdf;IEEE Xplore Abstract Record:/data/zotero/storage/HD77Q9XY/8952475.html:text/html}
}

@article{kanade_pre-trained_2019,
	title = {Pre-trained Contextual Embedding of Source Code},
	url = {http://arxiv.org/abs/2001.00059},
	abstract = {The source code of a program not only serves as a formal description of an executable task, but it also serves to communicate developer intent in a human-readable form. To facilitate this, developers use meaningful identifier names and natural-language documentation. This makes it possible to successfully apply sequence-modeling approaches, shown to be effective in natural-language processing, to source code. A major advancement in natural-language understanding has been the use of pre-trained token embeddings; {BERT} and other works have further shown that pre-trained contextual embeddings can be extremely powerful and can be fine-tuned effectively for a variety of downstream supervised tasks. Inspired by these developments, we present the first attempt to replicate this success on source code. We curate a massive corpus of Python programs from {GitHub} to pre-train a {BERT} model, which we call Code Understanding {BERT} ({CuBERT}). We also pre-train Word2Vec embeddings on the same dataset. We create a benchmark of five classification tasks and compare fine-tuned {CuBERT} against sequence models trained with and without the Word2Vec embeddings. Our results show that {CuBERT} outperforms the baseline methods by a margin of 2.9-22\%. We also show its superiority when fine-tuned with smaller datasets, and over fewer epochs. We further evaluate {CuBERT}'s effectiveness on a joint classification, localization and repair task involving prediction of two pointers.},
	journaltitle = {{arXiv}:2001.00059 [cs]},
	author = {Kanade, Aditya and Maniatis, Petros and Balakrishnan, Gogul and Shi, Kensen},
	urldate = {2020-06-24},
	date = {2019-12-21},
	eprinttype = {arxiv},
	eprint = {2001.00059},
	keywords = {⛔ No {DOI} found},
	file = {Kanade et al_2019_Pre-trained Contextual Embedding of Source Code.pdf:/data/zotero/storage/ZI9MPRSM/Kanade et al_2019_Pre-trained Contextual Embedding of Source Code.pdf:application/pdf;arXiv.org Snapshot:/data/zotero/storage/56K4UP4G/2001.html:text/html}
}

@article{shi_learning_2020,
	title = {Learning Execution through Neural Code Fusion},
	url = {http://arxiv.org/abs/1906.07181},
	abstract = {As the performance of computer systems stagnates due to the end of Moore's Law, there is a need for new models that can understand and optimize the execution of general purpose code. While there is a growing body of work on using Graph Neural Networks ({GNNs}) to learn representations of source code, these representations do not understand how code dynamically executes. In this work, we propose a new approach to use {GNNs} to learn fused representations of general source code and its execution. Our approach defines a multi-task {GNN} over low-level representations of source code and program state (i.e., assembly code and dynamic memory states), converting complex source code constructs and complex data structures into a simpler, more uniform format. We show that this leads to improved performance over similar methods that do not use execution and it opens the door to applying {GNN} models to new tasks that would not be feasible from static code alone. As an illustration of this, we apply the new model to challenging dynamic tasks (branch prediction and prefetching) from the {SPEC} {CPU} benchmark suite, outperforming the state-of-the-art by 26\% and 45\% respectively. Moreover, we use the learned fused graph embeddings to demonstrate transfer learning with high performance on an indirectly related task (algorithm classification).},
	journaltitle = {{arXiv}:1906.07181 [cs, stat]},
	author = {Shi, Zhan and Swersky, Kevin and Tarlow, Daniel and Ranganathan, Parthasarathy and Hashemi, Milad},
	urldate = {2020-06-24},
	date = {2020-03-10},
	eprinttype = {arxiv},
	eprint = {1906.07181},
	keywords = {⛔ No {DOI} found},
	file = {Shi et al_2020_Learning Execution through Neural Code Fusion.pdf:/data/zotero/storage/TF6M96DW/Shi et al_2020_Learning Execution through Neural Code Fusion.pdf:application/pdf;arXiv.org Snapshot:/data/zotero/storage/DD2DEP45/1906.html:text/html}
}

@book{paasen_execution_2016,
	title = {Execution Traces as a Powerful Data Representation for Intelligent Tutoring Systems for Programming},
	url = {https://eric.ed.gov/?id=ED592662},
	abstract = {The first intelligent tutoring systems for computer programming have been proposed more than 30 years ago, mostly focusing on well defined programming tasks e.g. in the context of logic programming. Recent systems also teach complex programs, where explicit modelling of every possible program and mistake is no longer possible. Such systems are based on data-driven approaches, which focus on the syntax of a program or consider the output for example cases. However, the system's understanding of student programs could be enriched by a deeper focus on the actual execution of a program. This requires a suitable data representation which encodes information of programming style as well as its functionality in a suitable way, thus offering entry points for automated feedback generation. In this contribution we propose a representation of computer programs via execution traces for example input and demonstrate the power of this representation in three key challenges for intelligent tutoring systems: identifying the underlying solution strategy, identifying erroneous solutions and locating the errors in erroneous programs for feedback display. [For the full proceedings, see {ED}592609.]},
	publisher = {International Educational Data Mining Society},
	author = {Paaßen, Benjamin and Jensen, Joris and Hammer, Barbara},
	urldate = {2020-06-24},
	date = {2016},
	langid = {english},
	note = {Publication Title: International Educational Data Mining Society},
	file = {Paaßen et al_2016_Execution Traces as a Powerful Data Representation for Intelligent Tutoring.pdf:/data/zotero/storage/TYRCVD6R/Paaßen et al_2016_Execution Traces as a Powerful Data Representation for Intelligent Tutoring.pdf:application/pdf;Snapshot:/data/zotero/storage/E7H4DSZH/eric.ed.gov.html:text/html}
}

@article{shepperd_researcher_2014,
	title = {Researcher Bias: The Use of Machine Learning in Software Defect Prediction},
	volume = {40},
	issn = {1939-3520},
	doi = {10/gfsckz},
	shorttitle = {Researcher Bias},
	abstract = {Background. The ability to predict defect-prone software components would be valuable. Consequently, there have been many empirical studies to evaluate the performance of different techniques endeavouring to accomplish this effectively. However no one technique dominates and so designing a reliable defect prediction model remains problematic. Objective. We seek to make sense of the many conflicting experimental results and understand which factors have the largest effect on predictive performance. Method. We conduct a meta-analysis of all relevant, high quality primary studies of defect prediction to determine what factors influence predictive performance. This is based on 42 primary studies that satisfy our inclusion criteria that collectively report 600 sets of empirical prediction results. By reverse engineering a common response variable we build a random effects {ANOVA} model to examine the relative contribution of four model building factors (classifier, data set, input metrics and researcher group) to model prediction performance. Results. Surprisingly we find that the choice of classifier has little impact upon performance (1.3 percent) and in contrast the major (31 percent) explanatory factor is the researcher group. It matters more who does the work than what is done. Conclusion. To overcome this high level of researcher bias, defect prediction researchers should (i) conduct blind analysis, (ii) improve reporting protocols and (iii) conduct more intergroup studies in order to alleviate expertise issues. Lastly, research is required to determine whether this bias is prevalent in other applications domains.},
	pages = {603--616},
	number = {6},
	journaltitle = {{IEEE} Transactions on Software Engineering},
	author = {Shepperd, Martin and Bowes, David and Hall, Tracy},
	date = {2014-06},
	note = {Conference Name: {IEEE} Transactions on Software Engineering},
	file = {IEEE Xplore Abstract Record:/data/zotero/storage/FU34QWEI/6824804.html:text/html;Shepperd et al_2014_Researcher Bias.pdf:/data/zotero/storage/UXASBBBZ/Shepperd et al_2014_Researcher Bias.pdf:application/pdf}
}

@article{rabin_evaluation_2020,
	title = {Evaluation of Generalizability of Neural Program Analyzers under Semantic-Preserving Transformations},
	url = {http://arxiv.org/abs/2004.07313},
	abstract = {The abundance of publicly available source code repositories, in conjunction with the advances in neural networks, has enabled data-driven approaches to program analysis. These approaches, called neural program analyzers, use neural networks to extract patterns in the programs for tasks ranging from development productivity to program reasoning. Despite the growing popularity of neural program analyzers, the extent to which their results are generalizable is unknown. In this paper, we perform a large-scale evaluation of the generalizability of two popular neural program analyzers using seven semantically-equivalent transformations of programs. Our results caution that in many cases the neural program analyzers fail to generalize well, sometimes to programs with negligible textual differences. The results provide the initial stepping stones for quantifying robustness in neural program analyzers.},
	journaltitle = {{arXiv}:2004.07313 [cs]},
	author = {Rabin, Md Rafiqul Islam and Alipour, Mohammad Amin},
	urldate = {2020-06-24},
	date = {2020-04-15},
	eprinttype = {arxiv},
	eprint = {2004.07313},
	keywords = {⛔ No {DOI} found},
	file = {Rabin_Alipour_2020_Evaluation of Generalizability of Neural Program Analyzers under.pdf:/data/zotero/storage/Y6JLVEJD/Rabin_Alipour_2020_Evaluation of Generalizability of Neural Program Analyzers under.pdf:application/pdf;arXiv.org Snapshot:/data/zotero/storage/YZY2EBGN/2004.html:text/html}
}

@inproceedings{khaled_saifullah_exploring_2020,
	title = {Exploring Type Inference Techniques of Dynamically Typed Languages},
	doi = {10/gg3j6h},
	abstract = {Developers often prefer dynamically typed programming languages, such as {JavaScript}, because such languages do not require explicit type declarations. However, such a feature hinders software engineering tasks, such as code completion, type related bug fixes and so on. Deep learning-based techniques are proposed in the literature to infer the types of code elements in {JavaScript} snippets. These techniques are computationally expensive. While several type inference techniques have been developed to detect types in code snippets written in statically typed languages, it is not clear how effective those techniques are for inferring types in dynamically typed languages, such as {JavaScript}. In this paper, we investigate the type inference techniques of {JavaScript} to understand the above two issues further. While doing that we propose a new technique that considers the locally specific code tokens as the context to infer the types of code elements. The evaluation result shows that the proposed technique is 20-47\% more accurate than the statically typed language-based techniques and 5-14 times faster than the deep learning techniques without sacrificing accuracy. Our analysis of sensitivity, overlapping of predicted types and the number of training examples justify the importance of our technique.},
	eventtitle = {2020 {IEEE} 27th International Conference on Software Analysis, Evolution and Reengineering ({SANER})},
	pages = {70--80},
	booktitle = {2020 {IEEE} 27th International Conference on Software Analysis, Evolution and Reengineering ({SANER})},
	author = {Khaled Saifullah, C. M. and Asaduzzaman, Muhammad and Roy, Chanchal K.},
	date = {2020-02},
	note = {{ISSN}: 1534-5351},
	file = {Khaled Saifullah et al_2020_Exploring Type Inference Techniques of Dynamically Typed Languages.pdf:/data/zotero/storage/4UET3L4Z/Khaled Saifullah et al_2020_Exploring Type Inference Techniques of Dynamically Typed Languages.pdf:application/pdf;IEEE Xplore Abstract Record:/data/zotero/storage/IHJX7B23/9054814.html:text/html}
}

@article{wehr_learning_2019,
	title = {Learning Semantic Vector Representations of Source Code via a Siamese Neural Network},
	url = {http://arxiv.org/abs/1904.11968},
	abstract = {The abundance of open-source code, coupled with the success of recent advances in deep learning for natural language processing, has given rise to a promising new application of machine learning to source code. In this work, we explore the use of a Siamese recurrent neural network model on Python source code to create vectors which capture the semantics of code. We evaluate the quality of embeddings by identifying which problem from a programming competition the code solves. Our model significantly outperforms a bag-of-tokens embedding, providing promising results for improving code embeddings that can be used in future software engineering tasks.},
	journaltitle = {{arXiv}:1904.11968 [cs, stat]},
	author = {Wehr, David and Fede, Halley and Pence, Eleanor and Zhang, Bo and Ferreira, Guilherme and Walczyk, John and Hughes, Joseph},
	urldate = {2020-06-24},
	date = {2019-04-26},
	eprinttype = {arxiv},
	eprint = {1904.11968},
	keywords = {low-prio, ⛔ No {DOI} found},
	file = {Wehr et al_2019_Learning Semantic Vector Representations of Source Code via a Siamese Neural.pdf:/data/zotero/storage/DWU4E26P/Wehr et al_2019_Learning Semantic Vector Representations of Source Code via a Siamese Neural.pdf:application/pdf;arXiv.org Snapshot:/data/zotero/storage/84X6LJ3M/1904.html:text/html}
}

@inproceedings{compton_embedding_2020,
	title = {Embedding Java classes with code2vec: improvements from variable obfuscation [Accepted]},
	url = {https://researchcommons.waikato.ac.nz/handle/10289/13618},
	doi = {10.1145/3379597.3387445},
	shorttitle = {Embedding Java classes with code2vec},
	abstract = {Automatic source code analysis in key areas of software engineering, such as code security, can benefit from Machine Learning ({ML}). However, many standard {ML} approaches require a numeric representation of data and cannot be applied directly to source code. Thus, to enable {ML}, we need to embed source code into numeric feature vectors while maintaining the semantics of the code as much as possible. code2vec is a recently released embedding approach that uses the proxy task of method name prediction to map Java methods to feature vectors. However, experimentation with code2vec shows that it learns to rely on variable names for prediction, causing it to be easily fooled by typos or adversarial attacks. Moreover, it is only able to embed individual Java methods and cannot embed an entire collection of methods such as those present in a typical Java class, making it difficult to perform predictions at the class level (e.g., for the identification of malicious Java classes). Both shortcomings are addressed in the research presented in this paper. We investigate the effect of obfuscating variable names during training of a code2vec model to force it to rely on the structure of the code rather than specific names and consider a simple approach to creating class-level embeddings by aggregating sets of method embeddings. Our results, obtained on a challenging new collection of source-code classification problems, indicate that obfuscating variable names produces an embedding model that is both impervious to variable naming and more accurately reflects code semantics. The datasets, models, and code are shared1 for further {ML} research on source code.},
	eventtitle = {{MSR} 2020},
	booktitle = {{MSR} 2020},
	publisher = {{ACM}},
	author = {Compton, Rhys and Frank, Eibe and Patros, Panagiotis and Koay, Abigail},
	urldate = {2020-06-24},
	date = {2020},
	langid = {english},
	note = {Accepted: 2020-06-11T02:18:19Z},
	keywords = {⚠️ Invalid {DOI}},
	file = {Compton et al_2020_Embedding Java classes with code2vec.pdf:/data/zotero/storage/2M8AM37D/Compton et al_2020_Embedding Java classes with code2vec.pdf:application/pdf;Snapshot:/data/zotero/storage/P3K6QHAM/13618.html:text/html}
}

@incollection{si_learning_2018,
	title = {Learning Loop Invariants for Program Verification},
	url = {http://papers.nips.cc/paper/8001-learning-loop-invariants-for-program-verification.pdf},
	pages = {7751--7762},
	booktitle = {Advances in Neural Information Processing Systems 31},
	publisher = {Curran Associates, Inc.},
	author = {Si, Xujie and Dai, Hanjun and Raghothaman, Mukund and Naik, Mayur and Song, Le},
	editor = {Bengio, S. and Wallach, H. and Larochelle, H. and Grauman, K. and Cesa-Bianchi, N. and Garnett, R.},
	urldate = {2020-06-25},
	date = {2018},
	keywords = {mpnn},
	file = {NIPS Snapshot:/data/zotero/storage/ABZQEEHM/8001-learning-loop-invariants-forprogram-verification.html:text/html;Si et al_2018_Learning Loop Invariants for Program Verification.pdf:/data/zotero/storage/8H9SZW4P/Si et al_2018_Learning Loop Invariants for Program Verification.pdf:application/pdf}
}

@inproceedings{lacomis_dire_2019,
	title = {{DIRE}: A Neural Approach to Decompiled Identifier Naming},
	doi = {10/gg3j6f},
	shorttitle = {{DIRE}},
	abstract = {The decompiler is one of the most common tools for examining binaries without corresponding source code. It transforms binaries into high-level code, reversing the compilation process. Decompilers can reconstruct much of the information that is lost during the compilation process (e.g., structure and type information). Unfortunately, they do not reconstruct semantically meaningful variable names, which are known to increase code understandability. We propose the Decompiled Identifier Renaming Engine ({DIRE}), a novel probabilistic technique for variable name recovery that uses both lexical and structural information recovered by the decompiler. We also present a technique for generating corpora suitable for training and evaluating models of decompiled code renaming, which we use to create a corpus of 164,632 unique x86-64 binaries generated from C projects mined from {GitHub}. Our results show that on this corpus {DIRE} can predict variable names identical to the names in the original source code up to 74.3\% of the time.},
	eventtitle = {2019 34th {IEEE}/{ACM} International Conference on Automated Software Engineering ({ASE})},
	pages = {628--639},
	booktitle = {2019 34th {IEEE}/{ACM} International Conference on Automated Software Engineering ({ASE})},
	author = {Lacomis, Jeremy and Yin, Pengcheng and Schwartz, Edward and Allamanis, Miltiadis and Le Goues, Claire and Neubig, Graham and Vasilescu, Bogdan},
	date = {2019-11},
	note = {{ISSN}: 2643-1572},
	file = {Lacomis et al_2019_DIRE.pdf:/data/zotero/storage/II5VUXSN/Lacomis et al_2019_DIRE.pdf:application/pdf;IEEE Xplore Abstract Record:/data/zotero/storage/7JE8R8G4/8952404.html:text/html}
}

@incollection{shin_program_2019,
	title = {Program Synthesis and Semantic Parsing with Learned Code Idioms},
	url = {http://papers.nips.cc/paper/9265-program-synthesis-and-semantic-parsing-with-learned-code-idioms.pdf},
	pages = {10825--10835},
	booktitle = {Advances in Neural Information Processing Systems 32},
	publisher = {Curran Associates, Inc.},
	author = {Shin, Eui Chul and Allamanis, Miltiadis and Brockschmidt, Marc and Polozov, Alex},
	editor = {Wallach, H. and Larochelle, H. and Beygelzimer, A. and Alché-Buc, F. d\{{\textbackslash}textbackslash\}textquotesingle and Fox, E. and Garnett, R.},
	urldate = {2020-06-25},
	date = {2019},
	file = {Shin et al_2019_Program Synthesis and Semantic Parsing with Learned Code Idioms.pdf:/data/zotero/storage/GPCPYWY6/Shin et al_2019_Program Synthesis and Semantic Parsing with Learned Code Idioms.pdf:application/pdf;NIPS Snapshot:/data/zotero/storage/75228INK/9265-program-synthesis-and-semantic-parsing-with-learned-code-idioms.html:text/html}
}

@article{lipton_critical_2015,
	title = {A Critical Review of Recurrent Neural Networks for Sequence Learning},
	url = {http://arxiv.org/abs/1506.00019},
	abstract = {Countless learning tasks require dealing with sequential data. Image captioning, speech synthesis, and music generation all require that a model produce outputs that are sequences. In other domains, such as time series prediction, video analysis, and musical information retrieval, a model must learn from inputs that are sequences. Interactive tasks, such as translating natural language, engaging in dialogue, and controlling a robot, often demand both capabilities. Recurrent neural networks ({RNNs}) are connectionist models that capture the dynamics of sequences via cycles in the network of nodes. Unlike standard feedforward neural networks, recurrent networks retain a state that can represent information from an arbitrarily long context window. Although recurrent neural networks have traditionally been difficult to train, and often contain millions of parameters, recent advances in network architectures, optimization techniques, and parallel computation have enabled successful large-scale learning with them. In recent years, systems based on long short-term memory ({LSTM}) and bidirectional ({BRNN}) architectures have demonstrated ground-breaking performance on tasks as varied as image captioning, language translation, and handwriting recognition. In this survey, we review and synthesize the research that over the past three decades first yielded and then made practical these powerful learning models. When appropriate, we reconcile conflicting notation and nomenclature. Our goal is to provide a self-contained explication of the state of the art together with a historical perspective and references to primary research.},
	journaltitle = {{arXiv}:1506.00019 [cs]},
	author = {Lipton, Zachary C. and Berkowitz, John and Elkan, Charles},
	urldate = {2020-06-25},
	date = {2015-10-17},
	eprinttype = {arxiv},
	eprint = {1506.00019},
	keywords = {⛔ No {DOI} found},
	file = {Lipton et al_2015_A Critical Review of Recurrent Neural Networks for Sequence Learning.pdf:/data/zotero/storage/89YIZ97S/Lipton et al_2015_A Critical Review of Recurrent Neural Networks for Sequence Learning.pdf:application/pdf;arXiv.org Snapshot:/data/zotero/storage/SZDVJMWQ/1506.html:text/html}
}

@article{gilmer_neural_2017,
	title = {Neural Message Passing for Quantum Chemistry},
	url = {http://arxiv.org/abs/1704.01212},
	abstract = {Supervised learning on molecules has incredible potential to be useful in chemistry, drug discovery, and materials science. Luckily, several promising and closely related neural network models invariant to molecular symmetries have already been described in the literature. These models learn a message passing algorithm and aggregation procedure to compute a function of their entire input graph. At this point, the next step is to find a particularly effective variant of this general approach and apply it to chemical prediction benchmarks until we either solve them or reach the limits of the approach. In this paper, we reformulate existing models into a single common framework we call Message Passing Neural Networks ({MPNNs}) and explore additional novel variations within this framework. Using {MPNNs} we demonstrate state of the art results on an important molecular property prediction benchmark; these results are strong enough that we believe future work should focus on datasets with larger molecules or more accurate ground truth labels.},
	journaltitle = {{arXiv}:1704.01212 [cs]},
	author = {Gilmer, Justin and Schoenholz, Samuel S. and Riley, Patrick F. and Vinyals, Oriol and Dahl, George E.},
	urldate = {2020-06-27},
	date = {2017-06-12},
	eprinttype = {arxiv},
	eprint = {1704.01212},
	keywords = {read, ⛔ No {DOI} found},
	file = {Gilmer et al_2017_Neural Message Passing for Quantum Chemistry.pdf:/data/zotero/storage/I7RMPB2I/Gilmer et al_2017_Neural Message Passing for Quantum Chemistry.pdf:application/pdf;arXiv.org Snapshot:/data/zotero/storage/78R9WQEE/1704.html:text/html}
}

@article{wang_detecting_2020,
	title = {Detecting Code Clones with Graph Neural Networkand Flow-Augmented Abstract Syntax Tree},
	url = {http://arxiv.org/abs/2002.08653},
	abstract = {Code clones are semantically similar code fragments pairs that are syntactically similar or different. Detection of code clones can help to reduce the cost of software maintenance and prevent bugs. Numerous approaches of detecting code clones have been proposed previously, but most of them focus on detecting syntactic clones and do not work well on semantic clones with different syntactic features. To detect semantic clones, researchers have tried to adopt deep learning for code clone detection to automatically learn latent semantic features from data. Especially, to leverage grammar information, several approaches used abstract syntax trees ({AST}) as input and achieved significant progress on code clone benchmarks in various programming languages. However, these {AST}-based approaches still can not fully leverage the structural information of code fragments, especially semantic information such as control flow and data flow. To leverage control and data flow information, in this paper, we build a graph representation of programs called flow-augmented abstract syntax tree ({FA}-{AST}). We construct {FA}-{AST} by augmenting original {ASTs} with explicit control and data flow edges. Then we apply two different types of graph neural networks ({GNN}) on {FA}-{AST} to measure the similarity of code pairs. As far as we have concerned, we are the first to apply graph neural networks on the domain of code clone detection. We apply our {FA}-{AST} and graph neural networks on two Java datasets: Google Code Jam and {BigCloneBench}. Our approach outperforms the state-of-the-art approaches on both Google Code Jam and {BigCloneBench} tasks.},
	journaltitle = {{arXiv}:2002.08653 [cs]},
	author = {Wang, Wenhan and Li, Ge and Ma, Bo and Xia, Xin and Jin, Zhi},
	urldate = {2020-06-27},
	date = {2020-02-20},
	eprinttype = {arxiv},
	eprint = {2002.08653},
	keywords = {⛔ No {DOI} found},
	file = {Wang et al_2020_Detecting Code Clones with Graph Neural Networkand Flow-Augmented Abstract.pdf:/data/zotero/storage/ISPWS4AA/Wang et al_2020_Detecting Code Clones with Graph Neural Networkand Flow-Augmented Abstract.pdf:application/pdf;arXiv.org Snapshot:/data/zotero/storage/WWIH8WJG/2002.html:text/html}
}

@article{vasic_neural_2019,
	title = {Neural Program Repair by Jointly Learning to Localize and Repair},
	url = {http://arxiv.org/abs/1904.01720},
	abstract = {Due to its potential to improve programmer productivity and software quality, automated program repair has been an active topic of research. Newer techniques harness neural networks to learn directly from examples of buggy programs and their fixes. In this work, we consider a recently identified class of bugs called variable-misuse bugs. The state-of-the-art solution for variable misuse enumerates potential fixes for all possible bug locations in a program, before selecting the best prediction. We show that it is beneficial to train a model that jointly and directly localizes and repairs variable-misuse bugs. We present multi-headed pointer networks for this purpose, with one head each for localization and repair. The experimental results show that the joint model significantly outperforms an enumerative solution that uses a pointer based model for repair alone.},
	journaltitle = {{arXiv}:1904.01720 [cs, stat]},
	author = {Vasic, Marko and Kanade, Aditya and Maniatis, Petros and Bieber, David and Singh, Rishabh},
	urldate = {2020-06-27},
	date = {2019-04-02},
	eprinttype = {arxiv},
	eprint = {1904.01720},
	keywords = {⛔ No {DOI} found},
	file = {Vasic et al_2019_Neural Program Repair by Jointly Learning to Localize and Repair.pdf:/data/zotero/storage/V9I3CVIG/Vasic et al_2019_Neural Program Repair by Jointly Learning to Localize and Repair.pdf:application/pdf;arXiv.org Snapshot:/data/zotero/storage/X36VZ3I8/1904.html:text/html}
}

@article{hamilton_representation_2018,
	title = {Representation Learning on Graphs: Methods and Applications},
	url = {http://arxiv.org/abs/1709.05584},
	shorttitle = {Representation Learning on Graphs},
	abstract = {Machine learning on graphs is an important and ubiquitous task with applications ranging from drug design to friendship recommendation in social networks. The primary challenge in this domain is finding a way to represent, or encode, graph structure so that it can be easily exploited by machine learning models. Traditionally, machine learning approaches relied on user-defined heuristics to extract features encoding structural information about a graph (e.g., degree statistics or kernel functions). However, recent years have seen a surge in approaches that automatically learn to encode graph structure into low-dimensional embeddings, using techniques based on deep learning and nonlinear dimensionality reduction. Here we provide a conceptual review of key advancements in this area of representation learning on graphs, including matrix factorization-based methods, random-walk based algorithms, and graph neural networks. We review methods to embed individual nodes as well as approaches to embed entire (sub)graphs. In doing so, we develop a unified framework to describe these recent approaches, and we highlight a number of important applications and directions for future work.},
	journaltitle = {{arXiv}:1709.05584 [cs]},
	author = {Hamilton, William L. and Ying, Rex and Leskovec, Jure},
	urldate = {2020-06-27},
	date = {2018-04-10},
	eprinttype = {arxiv},
	eprint = {1709.05584},
	keywords = {⛔ No {DOI} found},
	file = {Hamilton et al_2018_Representation Learning on Graphs.pdf:/data/zotero/storage/2B6RR9LL/Hamilton et al_2018_Representation Learning on Graphs.pdf:application/pdf;arXiv.org Snapshot:/data/zotero/storage/8M54ITCS/1709.html:text/html}
}

@inproceedings{devlin_robustfill_2017,
	location = {Sydney, {NSW}, Australia},
	title = {{RobustFill}: neural program learning under noisy I/O},
	series = {{ICML}'17},
	shorttitle = {{RobustFill}},
	abstract = {The problem of automatically generating a computer program from some specification has been studied since the early days of {AI}. Recently, two competing approaches for automatic program learning have received significant attention: (1) neural program synthesis, where a neural network is conditioned on input/output (I/O) examples and learns to generate a program, and (2) neural program induction, where a neural network generates new outputs directly using a latent program representation. Here, for the first time, we directly compare both approaches on a large-scale, real-world learning task and we additionally contrast to rule-based program synthesis, which uses hand-crafted semantics to guide the program generation. Our neural models use a modified attention {RNN} to allow encoding of variable-sized sets of I/O pairs, which achieve 92\% accuracy on a real-world test set, compared to the 34\% accuracy of the previous best neural synthesis approach. The synthesis model also outperforms a comparable induction model on this task, but we more importantly demonstrate that the strength of each approach is highly dependent on the evaluation metric and end-user application. Finally, we show that we can train our neural models to remain very robust to the type of noise expected in real-world data (e.g., typos), while a highly-engineered rule-based system fails entirely.},
	pages = {990--998},
	booktitle = {Proceedings of the 34th International Conference on Machine Learning - Volume 70},
	publisher = {{JMLR}.org},
	author = {Devlin, Jacob and Uesato, Jonathan and Bhupatiraju, Surya and Singh, Rishabh and Mohamed, Abdel-rahman and Kohli, Pushmeet},
	urldate = {2020-06-27},
	date = {2017-08-06},
	keywords = {to-read},
	file = {Devlin et al_2017_RobustFill.pdf:/data/zotero/storage/IHA7L8EE/Devlin et al_2017_RobustFill.pdf:application/pdf}
}

@inproceedings{kalyan_neural-guided_2018,
	title = {Neural-Guided Deductive Search for Real-Time Program Synthesis from Examples},
	url = {https://openreview.net/forum?id=rywDjg-RW},
	abstract = {Synthesizing user-intended programs from a small number of input-output exam-
  ples is a challenging problem with several important applications like spreadsheet
  manipulation, data wrangling and...},
	eventtitle = {International Conference on Learning Representations},
	author = {Kalyan, Ashwin and Mohta, Abhishek and Polozov, Oleksandr and Batra, Dhruv and Jain, Prateek and Gulwani, Sumit},
	urldate = {2020-06-27},
	date = {2018-02-15},
	file = {Kalyan et al_2018_Neural-Guided Deductive Search for Real-Time Program Synthesis from Examples.pdf:/data/zotero/storage/C6PKJE8F/Kalyan et al_2018_Neural-Guided Deductive Search for Real-Time Program Synthesis from Examples.pdf:application/pdf;Snapshot:/data/zotero/storage/HIBB8E4A/forum.html:text/html}
}

@article{velickovic_neural_2020,
	title = {Neural Execution of Graph Algorithms},
	url = {http://arxiv.org/abs/1910.10593},
	abstract = {Graph Neural Networks ({GNNs}) are a powerful representational tool for solving problems on graph-structured inputs. In almost all cases so far, however, they have been applied to directly recovering a final solution from raw inputs, without explicit guidance on how to structure their problem-solving. Here, instead, we focus on learning in the space of algorithms: we train several state-of-the-art {GNN} architectures to imitate individual steps of classical graph algorithms, parallel (breadth-first search, Bellman-Ford) as well as sequential (Prim's algorithm). As graph algorithms usually rely on making discrete decisions within neighbourhoods, we hypothesise that maximisation-based message passing neural networks are best-suited for such objectives, and validate this claim empirically. We also demonstrate how learning in the space of algorithms can yield new opportunities for positive transfer between tasks---showing how learning a shortest-path algorithm can be substantially improved when simultaneously learning a reachability algorithm.},
	journaltitle = {{arXiv}:1910.10593 [cs, stat]},
	author = {Veličković, Petar and Ying, Rex and Padovano, Matilde and Hadsell, Raia and Blundell, Charles},
	urldate = {2020-06-27},
	date = {2020-01-15},
	eprinttype = {arxiv},
	eprint = {1910.10593},
	keywords = {⛔ No {DOI} found},
	file = {Veličković et al_2020_Neural Execution of Graph Algorithms.pdf:/data/zotero/storage/IVD5DIBJ/Veličković et al_2020_Neural Execution of Graph Algorithms.pdf:application/pdf;arXiv.org Snapshot:/data/zotero/storage/ZWIZPWB8/1910.html:text/html}
}

@article{yan_neural_2020,
	title = {Neural Execution Engines: Learning to Execute Subroutines},
	url = {http://arxiv.org/abs/2006.08084},
	shorttitle = {Neural Execution Engines},
	abstract = {A significant effort has been made to train neural networks that replicate algorithmic reasoning, but they often fail to learn the abstract concepts underlying these algorithms. This is evidenced by their inability to generalize to data distributions that are outside of their restricted training sets, namely larger inputs and unseen data. We study these generalization issues at the level of numerical subroutines that comprise common algorithms like sorting, shortest paths, and minimum spanning trees. First, we observe that transformer-based sequence-to-sequence models can learn subroutines like sorting a list of numbers, but their performance rapidly degrades as the length of lists grows beyond those found in the training set. We demonstrate that this is due to attention weights that lose fidelity with longer sequences, particularly when the input numbers are numerically similar. To address the issue, we propose a learned conditional masking mechanism, which enables the model to strongly generalize far outside of its training range with near-perfect accuracy on a variety of algorithms. Second, to generalize to unseen data, we show that encoding numbers with a binary representation leads to embeddings with rich structure once trained on downstream tasks like addition or multiplication. This allows the embedding to handle missing data by faithfully interpolating numbers not seen during training.},
	journaltitle = {{arXiv}:2006.08084 [cs, stat]},
	author = {Yan, Yujun and Swersky, Kevin and Koutra, Danai and Ranganathan, Parthasarathy and Hashemi, Milad},
	urldate = {2020-06-27},
	date = {2020-06-22},
	eprinttype = {arxiv},
	eprint = {2006.08084},
	keywords = {⛔ No {DOI} found},
	file = {Yan et al_2020_Neural Execution Engines.pdf:/data/zotero/storage/R5Z9WGZL/Yan et al_2020_Neural Execution Engines.pdf:application/pdf;arXiv.org Snapshot:/data/zotero/storage/X5CRABTM/2006.html:text/html}
}

@article{parisotto_neuro-symbolic_2016,
	title = {Neuro-Symbolic Program Synthesis},
	url = {http://arxiv.org/abs/1611.01855},
	abstract = {Recent years have seen the proposal of a number of neural architectures for the problem of Program Induction. Given a set of input-output examples, these architectures are able to learn mappings that generalize to new test inputs. While achieving impressive results, these approaches have a number of important limitations: (a) they are computationally expensive and hard to train, (b) a model has to be trained for each task (program) separately, and (c) it is hard to interpret or verify the correctness of the learnt mapping (as it is defined by a neural network). In this paper, we propose a novel technique, Neuro-Symbolic Program Synthesis, to overcome the above-mentioned problems. Once trained, our approach can automatically construct computer programs in a domain-specific language that are consistent with a set of input-output examples provided at test time. Our method is based on two novel neural modules. The first module, called the cross correlation I/O network, given a set of input-output examples, produces a continuous representation of the set of I/O examples. The second module, the Recursive-Reverse-Recursive Neural Network (R3NN), given the continuous representation of the examples, synthesizes a program by incrementally expanding partial programs. We demonstrate the effectiveness of our approach by applying it to the rich and complex domain of regular expression based string transformations. Experiments show that the R3NN model is not only able to construct programs from new input-output examples, but it is also able to construct new programs for tasks that it had never observed before during training.},
	journaltitle = {{arXiv}:1611.01855 [cs]},
	author = {Parisotto, Emilio and Mohamed, Abdel-rahman and Singh, Rishabh and Li, Lihong and Zhou, Dengyong and Kohli, Pushmeet},
	urldate = {2020-06-28},
	date = {2016-11-06},
	eprinttype = {arxiv},
	eprint = {1611.01855},
	keywords = {⛔ No {DOI} found},
	file = {Parisotto et al_2016_Neuro-Symbolic Program Synthesis.pdf:/data/zotero/storage/Y6AZQGZA/Parisotto et al_2016_Neuro-Symbolic Program Synthesis.pdf:application/pdf;arXiv.org Snapshot:/data/zotero/storage/PBTVXW7A/1611.html:text/html}
}

@article{nguyen_suggesting_nodate,
	title = {Suggesting Natural Method Names to Check Name Consistencies},
	author = {Nguyen, Son and Phan, Hung and Le, Trinh and Nguyen, Tien N.},
	keywords = {⛔ No {DOI} found},
	file = {Nguyen et al_Suggesting Natural Method Names to Check Name Consistencies.pdf:/data/zotero/storage/47VQSDNE/Nguyen et al_Suggesting Natural Method Names to Check Name Consistencies.pdf:application/pdf}
}

@article{leclair_improved_2020,
	title = {Improved Code Summarization via a Graph Neural Network},
	url = {http://arxiv.org/abs/2004.02843},
	abstract = {Automatic source code summarization is the task of generating natural language descriptions for source code. Automatic code summarization is a rapidly expanding research area, especially as the community has taken greater advantage of advances in neural network and {AI} technologies. In general, source code summarization techniques use the source code as input and outputs a natural language description. Yet a strong consensus is developing that using structural information as input leads to improved performance. The first approaches to use structural information flattened the {AST} into a sequence. Recently, more complex approaches based on random {AST} paths or graph neural networks have improved on the models using flattened {ASTs}. However, the literature still does not describe the using a graph neural network together with source code sequence as separate inputs to a model. Therefore, in this paper, we present an approach that uses a graph-based neural architecture that better matches the default structure of the {AST} to generate these summaries. We evaluate our technique using a data set of 2.1 million Java method-comment pairs and show improvement over four baseline techniques, two from the software engineering literature, and two from machine learning literature.},
	journaltitle = {{arXiv}:2004.02843 [cs]},
	author = {{LeClair}, Alexander and Haque, Sakib and Wu, Lingfei and {McMillan}, Collin},
	urldate = {2020-06-28},
	date = {2020-04-07},
	eprinttype = {arxiv},
	eprint = {2004.02843},
	keywords = {⛔ No {DOI} found},
	file = {LeClair et al_2020_Improved Code Summarization via a Graph Neural Network.pdf:/data/zotero/storage/VFX8K9G9/LeClair et al_2020_Improved Code Summarization via a Graph Neural Network.pdf:application/pdf;arXiv.org Snapshot:/data/zotero/storage/2MHRCUU3/2004.html:text/html}
}

@article{yefet_adversarial_2020,
	title = {Adversarial Examples for Models of Code},
	url = {http://arxiv.org/abs/1910.07517},
	abstract = {Neural models of code have shown impressive performance for tasks such as predicting method names and identifying certain kinds of bugs. In this paper, we show that these models are vulnerable to adversarial examples, and introduce a novel approach for attacking trained models of code with adversarial examples. The main idea is to force a given trained model to make an incorrect prediction as specified by the adversary by introducing small perturbations that do not change the program's semantics. To find such perturbations, we present a new technique for Discrete Adversarial Manipulation of Programs ({DAMP}). {DAMP} works by deriving the desired prediction with respect to the model's inputs while holding the model weights constant and following the gradients to slightly modify the input code. We show that our {DAMP} attack is effective across three neural architectures: code2vec, {GGNN}, and {GNN}-{FiLM}, in both Java and C\#. We show that {DAMP} has up to 89\% success rate in changing a prediction to the adversary's choice ("targeted attack"), and a success rate of up to 94\% in changing a given prediction to any incorrect prediction ("non-targeted attack"). To defend a model against such attacks, we examine a variety of possible defenses empirically and discuss their trade-offs. We show that some of these defenses drop the success rate of the attacker drastically, with a minor penalty of 2\% relative degradation in accuracy while not performing under attack.},
	journaltitle = {{arXiv}:1910.07517 [cs]},
	author = {Yefet, Noam and Alon, Uri and Yahav, Eran},
	urldate = {2020-06-28},
	date = {2020-05-27},
	eprinttype = {arxiv},
	eprint = {1910.07517},
	keywords = {⛔ No {DOI} found},
	file = {Yefet et al_2020_Adversarial Examples for Models of Code.pdf:/data/zotero/storage/NAW8Y4SP/Yefet et al_2020_Adversarial Examples for Models of Code.pdf:application/pdf;arXiv.org Snapshot:/data/zotero/storage/FFFHWVRB/1910.html:text/html}
}

@article{zhang_generating_2020,
	title = {Generating Adversarial Examples for Holding Robustness of Source Code Processing Models},
	volume = {34},
	rights = {Copyright (c) 2020 Association for the Advancement of Artificial Intelligence},
	issn = {2374-3468},
	url = {https://www.aaai.org/ojs/index.php/AAAI/article/view/5469},
	doi = {10/gg3j6p},
	abstract = {Automated processing, analysis, and generation of source code are among the key activities in software and system lifecycle. To this end, while deep learning ({DL}) exhibits a certain level of capability in handling these tasks, the current state-of-the-art {DL} models still suffer from non-robust issues and can be easily fooled by adversarial attacks.Different from adversarial attacks for image, audio, and natural languages, the structured nature of programming languages brings new challenges. In this paper, we propose a Metropolis-Hastings sampling-based identifier renaming technique, named {\textbackslash}fullmethod ({\textbackslash}method), which generates adversarial examples for {DL} models specialized for source code processing. Our in-depth evaluation on a functionality classification benchmark demonstrates the effectiveness of {\textbackslash}method in generating adversarial examples of source code. The higher robustness and performance enhanced through our adversarial training with {\textbackslash}method further confirms the usefulness of {DL} models-based method for future fully automated source code processing.},
	pages = {1169--1176},
	number = {1},
	journaltitle = {Proceedings of the {AAAI} Conference on Artificial Intelligence},
	author = {Zhang, Huangzhao and Li, Zhuo and Li, Ge and Ma, Lei and Liu, Yang and Jin, Zhi},
	urldate = {2020-06-28},
	date = {2020-04-03},
	langid = {english},
	note = {Number: 01},
	file = {Zhang et al_2020_Generating Adversarial Examples for Holding Robustness of Source Code.pdf:/data/zotero/storage/V76LMS33/Zhang et al_2020_Generating Adversarial Examples for Holding Robustness of Source Code.pdf:application/pdf;Snapshot:/data/zotero/storage/V5HMPJH7/5469.html:text/html}
}

@article{fernandes_structured_2020,
	title = {Structured Neural Summarization},
	url = {http://arxiv.org/abs/1811.01824},
	abstract = {Summarization of long sequences into a concise statement is a core problem in natural language processing, requiring non-trivial understanding of the input. Based on the promising results of graph neural networks on highly structured data, we develop a framework to extend existing sequence encoders with a graph component that can reason about long-distance relationships in weakly structured data such as text. In an extensive evaluation, we show that the resulting hybrid sequence-graph models outperform both pure sequence models as well as pure graph models on a range of summarization tasks.},
	journaltitle = {{arXiv}:1811.01824 [cs, stat]},
	author = {Fernandes, Patrick and Allamanis, Miltiadis and Brockschmidt, Marc},
	urldate = {2020-06-28},
	date = {2020-05-04},
	eprinttype = {arxiv},
	eprint = {1811.01824},
	keywords = {⛔ No {DOI} found},
	file = {Fernandes et al_2020_Structured Neural Summarization.pdf:/data/zotero/storage/6G8WT7JE/Fernandes et al_2020_Structured Neural Summarization.pdf:application/pdf;arXiv.org Snapshot:/data/zotero/storage/678H7KKD/1811.html:text/html}
}

@article{yasunaga_graph-based_2020,
	title = {Graph-based, Self-Supervised Program Repair from Diagnostic Feedback},
	url = {http://arxiv.org/abs/2005.10636},
	abstract = {We consider the problem of learning to repair programs from diagnostic feedback (e.g., compiler error messages). Program repair is challenging for two reasons: First, it requires reasoning and tracking symbols across source code and diagnostic feedback. Second, labeled datasets available for program repair are relatively small. In this work, we propose novel solutions to these two challenges. First, we introduce a program-feedback graph, which connects symbols relevant to program repair in source code and diagnostic feedback, and then apply a graph neural network on top to model the reasoning process. Second, we present a self-supervised learning paradigm for program repair that leverages unlabeled programs available online to create a large amount of extra program repair examples, which we use to pre-train our models. We evaluate our proposed approach on two applications: correcting introductory programming assignments ({DeepFix} dataset) and correcting the outputs of program synthesis ({SPoC} dataset). Our final system, {DrRepair}, significantly outperforms prior work, achieving 66.1\% full repair rate on {DeepFix} (+20.8\% over the prior best), and 48.0\% synthesis success rate on {SPoC} (+3.3\% over the prior best).},
	journaltitle = {{arXiv}:2005.10636 [cs, stat]},
	author = {Yasunaga, Michihiro and Liang, Percy},
	urldate = {2020-06-28},
	date = {2020-05-20},
	eprinttype = {arxiv},
	eprint = {2005.10636},
	keywords = {⛔ No {DOI} found},
	file = {Yasunaga_Liang_2020_Graph-based, Self-Supervised Program Repair from Diagnostic Feedback.pdf:/data/zotero/storage/52KW3PYF/Yasunaga_Liang_2020_Graph-based, Self-Supervised Program Repair from Diagnostic Feedback.pdf:application/pdf;arXiv.org Snapshot:/data/zotero/storage/96YZY5KD/2005.html:text/html}
}

@article{li_using_2019,
	title = {Using {GGNN} to recommend log statement level},
	url = {http://arxiv.org/abs/1912.05097},
	abstract = {In software engineering, log statement is an important part because programmers can't access to users' program and they can only rely on log message to find the root of bugs. The mechanism of "log level" allows developers and users to specify the appropriate amount of logs to print during the execution of the software. And 26{\textbackslash}\% of the log statement modification is to modify the level. We tried to use {ML} method to predict the suitable level of log statement. The specific model is {GGNN}(gated graph neural network) and we have drawn lessons from Microsoft's research. In this work, we apply Graph Neural Networks to predict the usage of log statement level of some open source java projects from github. Given the good performance of {GGNN} in this task, we are confident that {GGNN} is an excellent choice for processing source code. We envision this model can play an important role in applying {AI}/{ML} technique for Software Development Life Cycle more broadly.},
	journaltitle = {{arXiv}:1912.05097 [cs]},
	author = {Li, Mingzhe and Pei, Jianrui and He, Jin and Song, Kevin and Che, Frank and Huang, Yongfeng and Wang, Chitai},
	urldate = {2020-06-28},
	date = {2019-12-10},
	eprinttype = {arxiv},
	eprint = {1912.05097},
	keywords = {mpnn, ⛔ No {DOI} found},
	file = {Li et al_2019_Using GGNN to recommend log statement level.pdf:/data/zotero/storage/FKDDA4D5/Li et al_2019_Using GGNN to recommend log statement level.pdf:application/pdf;arXiv.org Snapshot:/data/zotero/storage/IW7Q8GDP/1912.html:text/html}
}

@inproceedings{tufano_deep_2018,
	title = {Deep Learning Similarities from Different Representations of Source Code},
	abstract = {Assessing the similarity between code components plays a pivotal role in a number of Software Engineering ({SE}) tasks, such as clone detection, impact analysis, refactoring, etc. Code similarity is generally measured by relying on manually defined or hand-crafted features, e.g., by analyzing the overlap among identifiers or comparing the Abstract Syntax Trees of two code components. These features represent a best guess at what {SE} researchers can utilize to exploit and reliably assess code similarity for a given task. Recent work has shown, when using a stream of identifiers to represent the code, that Deep Learning ({DL}) can effectively replace manual feature engineering for the task of clone detection. However, source code can be represented at different levels of abstraction: identifiers, Abstract Syntax Trees, Control Flow Graphs, and Bytecode. We conjecture that each code representation can provide a different, yet orthogonal view of the same code fragment, thus, enabling a more reliable detection of similarities in code. In this paper, we demonstrate how {SE} tasks can benefit from a {DL}-based approach, which can automatically learn code similarities from different representations.},
	eventtitle = {2018 {IEEE}/{ACM} 15th International Conference on Mining Software Repositories ({MSR})},
	pages = {542--553},
	booktitle = {2018 {IEEE}/{ACM} 15th International Conference on Mining Software Repositories ({MSR})},
	author = {Tufano, Michele and Watson, Cody and Bavota, Gabriele and Di Penta, Massimiliano and White, Martin and Poshyvanyk, Denys},
	date = {2018-05},
	note = {{ISSN}: 2574-3864},
	keywords = {to-read},
	file = {Tufano et al_2018_Deep Learning Similarities from Different Representations of Source Code.pdf:/data/zotero/storage/2SWKYEPB/Tufano et al_2018_Deep Learning Similarities from Different Representations of Source Code.pdf:application/pdf;IEEE Xplore Abstract Record:/data/zotero/storage/DXPNHVSS/8595238.html:text/html}
}

@inproceedings{hellendoorn_global_2019,
	title = {Global Relational Models of Source Code},
	url = {https://openreview.net/forum?id=B1lnbRNtwr&noteId=B1lnbRNtwr},
	abstract = {Models of code can learn distributed representations of a program's syntax and semantics to predict many non-trivial properties of a program. Recent state-of-the-art models leverage highly...},
	eventtitle = {International Conference on Learning Representations},
	author = {Hellendoorn, Vincent J. and Sutton, Charles and Singh, Rishabh and Maniatis, Petros and Bieber, David},
	urldate = {2020-06-28},
	date = {2019-09-25},
	keywords = {mpnn},
	file = {Hellendoorn et al_2019_Global Relational Models of Source Code.pdf:/data/zotero/storage/SGSSMPK5/Hellendoorn et al_2019_Global Relational Models of Source Code.pdf:application/pdf;Snapshot:/data/zotero/storage/KDBYDTCY/forum.html:text/html}
}

@inproceedings{hellendoorn_deep_2018,
	location = {Lake Buena Vista, {FL}, {USA}},
	title = {Deep learning type inference},
	isbn = {978-1-4503-5573-5},
	url = {http://dl.acm.org/citation.cfm?doid=3236024.3236051},
	doi = {10/gf8npn},
	abstract = {Dynamically typed languages such as {JavaScript} and Python are increasingly popular, yet static typing has not been totally eclipsed: Python now supports type annotations and languages like {TypeScript} offer a middle-ground for {JavaScript}: a strict superset of {JavaScript}, to which it transpiles, coupled with a type system that permits partially typed programs. However, static typing has a cost: adding annotations, reading the added syntax, and wrestling with the type system to fix type errors. Type inference can ease the transition to more statically typed code and unlock the benefits of richer compile-time information, but is limited in languages like {JavaScript} as it cannot soundly handle duck-typing or runtime evaluation via eval. We propose {DeepTyper}, a deep learning model that understands which types naturally occur in certain contexts and relations and can provide type suggestions, which can often be verified by the type checker, even if it could not infer the type initially. {DeepTyper}, leverages an automatically aligned corpus of tokens and types to accurately predict thousands of variable and function type annotations. Furthermore, we demonstrate that context is key in accurately assigning these types and introduce a technique to reduce overfitting on local cues while highlighting the need for further improvements. Finally, we show that our model can interact with a compiler to provide more than 4,000 additional type annotations with over 95\% precision that could not be inferred without the aid of {DeepTyper}.},
	eventtitle = {the 2018 26th {ACM} Joint Meeting},
	pages = {152--162},
	booktitle = {Proceedings of the 2018 26th {ACM} Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering - {ESEC}/{FSE} 2018},
	publisher = {{ACM} Press},
	author = {Hellendoorn, Vincent J. and Bird, Christian and Barr, Earl T. and Allamanis, Miltiadis},
	urldate = {2020-06-28},
	date = {2018},
	langid = {english},
	keywords = {to-read},
	file = {Hellendoorn et al_2018_Deep learning type inference.pdf:/data/zotero/storage/XGRWB53Y/Hellendoorn et al_2018_Deep learning type inference.pdf:application/pdf}
}

@article{arakelyan_towards_2020,
	title = {Towards Learning Representations of Binary Executable Files for Security Tasks},
	url = {http://arxiv.org/abs/2002.03388},
	abstract = {Tackling binary analysis problems has traditionally implied manually defining rules and heuristics. As an alternative, we are suggesting using machine learning models for learning distributed representations of binaries that can be applicable for a number of downstream tasks. We construct a computational graph from the binary executable and use it with a graph convolutional neural network to learn a high dimensional representation of the program. We show the versatility of this approach by using our representations to solve two semantically different binary analysis tasks -- algorithm classification and vulnerability discovery. We compare the proposed approach to our own strong baseline as well as published results and demonstrate improvement on the state of the art methods for both tasks.},
	journaltitle = {{arXiv}:2002.03388 [cs, stat]},
	author = {Arakelyan, Shushan and Hauser, Christophe and Kline, Erik and Galstyan, Aram},
	urldate = {2020-06-28},
	date = {2020-02-09},
	eprinttype = {arxiv},
	eprint = {2002.03388},
	keywords = {⛔ No {DOI} found},
	file = {Arakelyan et al_2020_Towards Learning Representations of Binary Executable Files for Security Tasks.pdf:/data/zotero/storage/5PMH833B/Arakelyan et al_2020_Towards Learning Representations of Binary Executable Files for Security Tasks.pdf:application/pdf;arXiv.org Snapshot:/data/zotero/storage/WESJKMNE/2002.html:text/html}
}

@article{wang_cocogum_2020,
	title = {{CoCoGUM}: Contextual Code Summarization with Multi-Relational {GNN} on {UMLs}},
	url = {https://www.microsoft.com/en-us/research/publication/cocogum-contextual-code-summarization-with-multi-relational-gnn-on-umls/},
	shorttitle = {{CoCoGUM}},
	abstract = {Code summaries are short natural language ({NL}) descriptions of code snippets that help developers better understand and maintain source code. Due to the pivotal role of code summaries in software development and maintenance, there is a surge of works on automatic code summarization to reduce the heavy burdens of developers. However, contemporary approaches only leverage […]},
	author = {Wang, Yanlin and Du, Lun and Shi, Ensheng and Hu, Yuxuan and Han, Shi and Zhang, Dongmei},
	urldate = {2020-06-28},
	date = {2020-05-20},
	langid = {american},
	keywords = {⛔ No {DOI} found},
	file = {Wang et al_2020_CoCoGUM.pdf:/data/zotero/storage/U3D25DPJ/Wang et al_2020_CoCoGUM.pdf:application/pdf;Snapshot:/data/zotero/storage/JGX2M2M9/cocogum-contextual-code-summarization-with-multi-relational-gnn-on-umls.html:text/html}
}

@inproceedings{dinella_hoppity_2019,
	title = {{HOPPITY}: {LEARNING} {GRAPH} {TRANSFORMATIONS} {TO} {DETECT} {AND} {FIX} {BUGS} {IN} {PROGRAMS}},
	url = {https://openreview.net/forum?id=SJeqs6EFvB&noteId=SJeqs6EFvB},
	shorttitle = {{HOPPITY}},
	abstract = {We present a learning-based approach to detect and fix a broad range of bugs in Javascript programs. We frame the problem in terms of learning a sequence of graph transformations: given a buggy...},
	eventtitle = {International Conference on Learning Representations},
	author = {Dinella, Elizabeth and Dai, Hanjun and Li, Ziyang and Naik, Mayur and Song, Le and Wang, Ke},
	urldate = {2020-06-28},
	date = {2019-09-25},
	file = {Dinella et al_2019_HOPPITY.pdf:/data/zotero/storage/UM2L3P5T/Dinella et al_2019_HOPPITY.pdf:application/pdf;Snapshot:/data/zotero/storage/HCR34JYR/forum.html:text/html}
}

@inproceedings{oono_graph_2020,
	title = {Graph Neural Networks Exponentially Lose Expressive Power for Node Classification},
	url = {https://iclr.cc/virtual_2020/poster_S1ldO2EFPr.html#details},
	abstract = {Graph Neural Networks (graph {NNs}) are a promising deep learning approach for analyzing graph-structured data. However, it is known that they do not improve (or sometimes worsen) their predictive performance as we pile up many layers and add non-lineality. To tackle this problem, we investigate the expressive power of graph {NNs} via their asymptotic behaviors as the layer size tends to infinity. Our strategy is to generalize the forward propagation of a Graph Convolutional Network ({GCN}), which is a popular graph {NN} variant, as a specific dynamical system. In the case of a {GCN}, we show that when its weights satisfy the conditions determined by the spectra of the (augmented) normalized Laplacian, its output exponentially approaches the set of signals that carry information of the connected components and node degrees only for distinguishing nodes. Our theory enables us to relate the expressive power of {GCNs} with the topological information of the underlying graphs inherent in the graph spectra. To demonstrate this, we characterize the asymptotic behavior of {GCNs} on the Erd{\textbackslash}H\{o\}s -- R{\textbackslash}'\{e\}nyi graph. We show that when the Erd{\textbackslash}H\{o\}s -- R{\textbackslash}'\{e\}nyi graph is sufficiently dense and large, a broad range of {GCNs} on it suffers from the ``information loss" in the limit of infinite layers with high probability. Based on the theory, we provide a principled guideline for weight normalization of graph {NNs}. We experimentally confirm that the proposed weight scaling enhances the predictive performance of {GCNs} in real data. Code is available at https://github.com/delta2323/gnn-asymptotics.},
	eventtitle = {Eighth International Conference on Learning Representations},
	author = {Oono, Kenta and Suzuki, Taiji},
	urldate = {2020-06-28},
	date = {2020-04},
	langid = {english},
	file = {Oono_Suzuki_2020_Graph Neural Networks Exponentially Lose Expressive Power for Node.pdf:/data/zotero/storage/A428VZVG/Oono_Suzuki_2020_Graph Neural Networks Exponentially Lose Expressive Power for Node.pdf:application/pdf;Snapshot:/data/zotero/storage/2MCA2D72/poster_S1ldO2EFPr.html:text/html}
}

@inproceedings{deng_graphzoom_2020,
	title = {{GraphZoom}: A Multi-level Spectral Approach for Accurate and Scalable Graph Embedding},
	url = {https://iclr.cc/virtual_2020/poster_r1lGO0EKDH.html},
	shorttitle = {{GraphZoom}},
	abstract = {Graph embedding techniques have been increasingly deployed in a multitude of different applications that involve learning on non-Euclidean data. However, existing graph embedding models either fail to incorporate node attribute information during training or suffer from node attribute noise, which compromises the accuracy. Moreover, very few of them scale to large graphs due to their high computational complexity and memory usage. In this paper we propose {GraphZoom}, a multi-level framework for improving both accuracy and scalability of unsupervised graph embedding algorithms. {GraphZoom} first performs graph fusion to generate a new graph that effectively encodes the topology of the original graph and the node attribute information. This fused graph is then repeatedly coarsened into much smaller graphs by merging nodes with high spectral similarities. {GraphZoom} allows any existing embedding methods to be applied to the coarsened graph, before it progressively refine the embeddings obtained at the coarsest level to increasingly finer graphs. We have evaluated our approach on a number of popular graph datasets for both transductive and inductive tasks. Our experiments show that {GraphZoom} can substantially increase the classification accuracy and significantly accelerate the entire graph embedding process by up to \$40.8 {\textbackslash}times\$, when compared to the state-of-the-art unsupervised embedding methods.},
	eventtitle = {Eighth International Conference on Learning Representations},
	author = {Deng, Chenhui and Zhao, Zhiqiang and Wang, Yongyu and Zhang, Zhiru and Feng, Zhuo},
	urldate = {2020-06-28},
	date = {2020-04},
	langid = {english},
	file = {Deng et al_2020_GraphZoom.pdf:/data/zotero/storage/U3HYIYCK/Deng et al_2020_GraphZoom.pdf:application/pdf;Snapshot:/data/zotero/storage/39IM3AJI/poster_r1lGO0EKDH.html:text/html}
}

@article{li_improving_2019,
	title = {Improving bug detection via context-based code representation learning and attention-based neural networks},
	volume = {3},
	url = {https://doi.org/10.1145/3360588},
	doi = {10/gg3j6n},
	abstract = {Bug detection has been shown to be an effective way to help developers in detecting bugs early, thus, saving much effort and time in software development process. Recently, deep learning-based bug detection approaches have gained successes over the traditional machine learning-based approaches, the rule-based program analysis approaches, and mining-based approaches. However, they are still limited in detecting bugs that involve multiple methods and suffer high rate of false positives. In this paper, we propose a combination approach with the use of contexts and attention neural network to overcome those limitations. We propose to use as the global context the Program Dependence Graph ({PDG}) and Data Flow Graph ({DFG}) to connect the method under investigation with the other relevant methods that might contribute to the buggy code. The global context is complemented by the local context extracted from the path on the {AST} built from the method’s body. The use of {PDG} and {DFG} enables our model to reduce the false positive rate, while to complement for the potential reduction in recall, we make use of the attention neural network mechanism to put more weights on the buggy paths in the source code. That is, the paths that are similar to the buggy paths will be ranked higher, thus, improving the recall of our model. We have conducted several experiments to evaluate our approach on a very large dataset with +4.973M methods in 92 different project versions. The results show that our tool can have a relative improvement up to 160\% on F-score when comparing with the state-of-the-art bug detection approaches. Our tool can detect 48 true bugs in the list of top 100 reported bugs, which is 24 more true bugs when comparing with the baseline approaches. We also reported that our representation is better suitable for bug detection and relatively improves over the other representations up to 206\% in accuracy.},
	pages = {162:1--162:30},
	issue = {{OOPSLA}},
	journaltitle = {Proceedings of the {ACM} on Programming Languages},
	shortjournal = {Proc. {ACM} Program. Lang.},
	author = {Li, Yi and Wang, Shaohua and Nguyen, Tien N. and Van Nguyen, Son},
	urldate = {2020-06-28},
	date = {2019-10-10},
	file = {Li et al_2019_Improving bug detection via context-based code representation learning and.pdf:/data/zotero/storage/QYGVK3TG/Li et al_2019_Improving bug detection via context-based code representation learning and.pdf:application/pdf}
}

@article{tarlow_learning_2019,
	title = {Learning to Fix Build Errors with Graph2Diff Neural Networks},
	url = {http://arxiv.org/abs/1911.01205},
	abstract = {Professional software developers spend a significant amount of time fixing builds, but this has received little attention as a problem in automatic program repair. We present a new deep learning architecture, called Graph2Diff, for automatically localizing and fixing build errors. We represent source code, build configuration files, and compiler diagnostic messages as a graph, and then use a Graph Neural Network model to predict a diff. A diff specifies how to modify the code's abstract syntax tree, represented in the neural network as a sequence of tokens and of pointers to code locations. Our network is an instance of a more general abstraction that we call Graph2Tocopo, which is potentially useful in any development tool for predicting source code changes. We evaluate the model on a dataset of over 500k real build errors and their resolutions from professional developers. Compared to the approach of {DeepDelta} (Mesbah et al., 2019), our approach tackles the harder task of predicting a more precise diff but still achieves over double the accuracy.},
	journaltitle = {{arXiv}:1911.01205 [cs, stat]},
	author = {Tarlow, Daniel and Moitra, Subhodeep and Rice, Andrew and Chen, Zimin and Manzagol, Pierre-Antoine and Sutton, Charles and Aftandilian, Edward},
	urldate = {2020-06-28},
	date = {2019-11-04},
	eprinttype = {arxiv},
	eprint = {1911.01205},
	keywords = {⛔ No {DOI} found},
	file = {Tarlow et al_2019_Learning to Fix Build Errors with Graph2Diff Neural Networks.pdf:/data/zotero/storage/ET9WHQFK/Tarlow et al_2019_Learning to Fix Build Errors with Graph2Diff Neural Networks.pdf:application/pdf;arXiv.org Snapshot:/data/zotero/storage/SSCLASPE/1911.html:text/html}
}

@article{tomczak_simulating_2019,
	title = {Simulating Execution Time of Tensor Programs using Graph Neural Networks},
	url = {http://arxiv.org/abs/1904.11876},
	abstract = {Optimizing the execution time of tensor program, e.g., a convolution, involves finding its optimal configuration. Searching the configuration space exhaustively is typically infeasible in practice. In line with recent research using {TVM}, we propose to learn a surrogate model to overcome this issue. The model is trained on an acyclic graph called an abstract syntax tree, and utilizes a graph convolutional network to exploit structure in the graph. We claim that a learnable graph-based data processing is a strong competitor to heuristic-based feature extraction. We present a new dataset of graphs corresponding to configurations and their execution time for various tensor programs. We provide baselines for a runtime prediction task.},
	journaltitle = {{arXiv}:1904.11876 [cs, stat]},
	author = {Tomczak, Jakub M. and Lepert, Romain and Wiggers, Auke},
	urldate = {2020-06-28},
	date = {2019-11-27},
	eprinttype = {arxiv},
	eprint = {1904.11876},
	keywords = {⛔ No {DOI} found},
	file = {Tomczak et al_2019_Simulating Execution Time of Tensor Programs using Graph Neural Networks.pdf:/data/zotero/storage/2CJPK6XZ/Tomczak et al_2019_Simulating Execution Time of Tensor Programs using Graph Neural Networks.pdf:application/pdf;arXiv.org Snapshot:/data/zotero/storage/YZ5VBGKK/1904.html:text/html}
}

@article{allamanis_survey_2018,
	title = {A Survey of Machine Learning for Big Code and Naturalness},
	url = {http://arxiv.org/abs/1709.06182},
	abstract = {Research at the intersection of machine learning, programming languages, and software engineering has recently taken important steps in proposing learnable probabilistic models of source code that exploit code's abundance of patterns. In this article, we survey this work. We contrast programming languages against natural languages and discuss how these similarities and differences drive the design of probabilistic models. We present a taxonomy based on the underlying design principles of each model and use it to navigate the literature. Then, we review how researchers have adapted these models to application areas and discuss cross-cutting and application-specific challenges and opportunities.},
	journaltitle = {{arXiv}:1709.06182 [cs]},
	author = {Allamanis, Miltiadis and Barr, Earl T. and Devanbu, Premkumar and Sutton, Charles},
	urldate = {2020-06-28},
	date = {2018-05-04},
	eprinttype = {arxiv},
	eprint = {1709.06182},
	keywords = {⛔ No {DOI} found},
	file = {Allamanis et al_2018_A Survey of Machine Learning for Big Code and Naturalness.pdf:/data/zotero/storage/YCJQEGXB/Allamanis et al_2018_A Survey of Machine Learning for Big Code and Naturalness.pdf:application/pdf;arXiv.org Snapshot:/data/zotero/storage/3QBG83QT/1709.html:text/html}
}

@article{cho_properties_2014,
	title = {On the Properties of Neural Machine Translation: Encoder-Decoder Approaches},
	url = {http://arxiv.org/abs/1409.1259},
	shorttitle = {On the Properties of Neural Machine Translation},
	abstract = {Neural machine translation is a relatively new approach to statistical machine translation based purely on neural networks. The neural machine translation models often consist of an encoder and a decoder. The encoder extracts a fixed-length representation from a variable-length input sentence, and the decoder generates a correct translation from this representation. In this paper, we focus on analyzing the properties of the neural machine translation using two models; {RNN} Encoder--Decoder and a newly proposed gated recursive convolutional neural network. We show that the neural machine translation performs relatively well on short sentences without unknown words, but its performance degrades rapidly as the length of the sentence and the number of unknown words increase. Furthermore, we find that the proposed gated recursive convolutional network learns a grammatical structure of a sentence automatically.},
	journaltitle = {{arXiv}:1409.1259 [cs, stat]},
	author = {Cho, Kyunghyun and van Merrienboer, Bart and Bahdanau, Dzmitry and Bengio, Yoshua},
	urldate = {2020-06-28},
	date = {2014-10-07},
	eprinttype = {arxiv},
	eprint = {1409.1259},
	keywords = {⛔ No {DOI} found},
	file = {Cho et al_2014_On the Properties of Neural Machine Translation.pdf:/data/zotero/storage/HKBG6TN2/Cho et al_2014_On the Properties of Neural Machine Translation.pdf:application/pdf;arXiv.org Snapshot:/data/zotero/storage/BQJFKP7H/1409.html:text/html}
}

@article{cho_learning_2014,
	title = {Learning Phrase Representations using {RNN} Encoder-Decoder for Statistical Machine Translation},
	url = {http://arxiv.org/abs/1406.1078},
	abstract = {In this paper, we propose a novel neural network model called {RNN} Encoder-Decoder that consists of two recurrent neural networks ({RNN}). One {RNN} encodes a sequence of symbols into a fixed-length vector representation, and the other decodes the representation into another sequence of symbols. The encoder and decoder of the proposed model are jointly trained to maximize the conditional probability of a target sequence given a source sequence. The performance of a statistical machine translation system is empirically found to improve by using the conditional probabilities of phrase pairs computed by the {RNN} Encoder-Decoder as an additional feature in the existing log-linear model. Qualitatively, we show that the proposed model learns a semantically and syntactically meaningful representation of linguistic phrases.},
	journaltitle = {{arXiv}:1406.1078 [cs, stat]},
	author = {Cho, Kyunghyun and van Merrienboer, Bart and Gulcehre, Caglar and Bahdanau, Dzmitry and Bougares, Fethi and Schwenk, Holger and Bengio, Yoshua},
	urldate = {2020-06-28},
	date = {2014-09-02},
	eprinttype = {arxiv},
	eprint = {1406.1078},
	keywords = {⛔ No {DOI} found},
	file = {Cho et al_2014_Learning Phrase Representations using RNN Encoder-Decoder for Statistical.pdf:/data/zotero/storage/R45VZFRT/Cho et al_2014_Learning Phrase Representations using RNN Encoder-Decoder for Statistical.pdf:application/pdf;arXiv.org Snapshot:/data/zotero/storage/I9U7GK6P/1406.html:text/html}
}

@inproceedings{cvitkovic_open_2019,
	title = {Open Vocabulary Learning on Source Code with a Graph-Structured Cache},
	url = {http://proceedings.mlr.press/v97/cvitkovic19b.html},
	abstract = {Machine learning models that take computer program source code as input typically use Natural Language Processing ({NLP}) techniques. However, a major challenge is that code is written using an open,...},
	eventtitle = {International Conference on Machine Learning},
	pages = {1475--1485},
	booktitle = {International Conference on Machine Learning},
	author = {Cvitkovic, Milan and Singh, Badal and Anandkumar, Animashree},
	urldate = {2020-06-28},
	date = {2019-05-24},
	langid = {english},
	note = {{ISSN}: 1938-7228
Section: Machine Learning},
	file = {Cvitkovic et al_2019_Open Vocabulary Learning on Source Code with a Graph-Structured Cache.pdf:/data/zotero/storage/9CP4EVJF/Cvitkovic et al_2019_Open Vocabulary Learning on Source Code with a Graph-Structured Cache.pdf:application/pdf;Snapshot:/data/zotero/storage/52MIZLE4/cvitkovic19b.html:text/html}
}

@article{li_gated_2017,
	title = {Gated Graph Sequence Neural Networks},
	url = {http://arxiv.org/abs/1511.05493},
	abstract = {Graph-structured data appears frequently in domains including chemistry, natural language semantics, social networks, and knowledge bases. In this work, we study feature learning techniques for graph-structured inputs. Our starting point is previous work on Graph Neural Networks (Scarselli et al., 2009), which we modify to use gated recurrent units and modern optimization techniques and then extend to output sequences. The result is a flexible and broadly useful class of neural network models that has favorable inductive biases relative to purely sequence-based models (e.g., {LSTMs}) when the problem is graph-structured. We demonstrate the capabilities on some simple {AI} ({bAbI}) and graph algorithm learning tasks. We then show it achieves state-of-the-art performance on a problem from program verification, in which subgraphs need to be matched to abstract data structures.},
	journaltitle = {{arXiv}:1511.05493 [cs, stat]},
	author = {Li, Yujia and Tarlow, Daniel and Brockschmidt, Marc and Zemel, Richard},
	urldate = {2020-06-29},
	date = {2017-09-22},
	eprinttype = {arxiv},
	eprint = {1511.05493},
	keywords = {⛔ No {DOI} found},
	file = {Li et al_2017_Gated Graph Sequence Neural Networks.pdf:/data/zotero/storage/9VGW4RVZ/Li et al_2017_Gated Graph Sequence Neural Networks.pdf:application/pdf;arXiv.org Snapshot:/data/zotero/storage/CSZ8QBR2/1511.html:text/html}
}

@article{velickovic_graph_2018,
	title = {Graph Attention Networks},
	url = {http://arxiv.org/abs/1710.10903},
	abstract = {We present graph attention networks ({GATs}), novel neural network architectures that operate on graph-structured data, leveraging masked self-attentional layers to address the shortcomings of prior methods based on graph convolutions or their approximations. By stacking layers in which nodes are able to attend over their neighborhoods' features, we enable (implicitly) specifying different weights to different nodes in a neighborhood, without requiring any kind of costly matrix operation (such as inversion) or depending on knowing the graph structure upfront. In this way, we address several key challenges of spectral-based graph neural networks simultaneously, and make our model readily applicable to inductive as well as transductive problems. Our {GAT} models have achieved or matched state-of-the-art results across four established transductive and inductive graph benchmarks: the Cora, Citeseer and Pubmed citation network datasets, as well as a protein-protein interaction dataset (wherein test graphs remain unseen during training).},
	journaltitle = {{arXiv}:1710.10903 [cs, stat]},
	author = {Veličković, Petar and Cucurull, Guillem and Casanova, Arantxa and Romero, Adriana and Liò, Pietro and Bengio, Yoshua},
	urldate = {2020-06-29},
	date = {2018-02-04},
	eprinttype = {arxiv},
	eprint = {1710.10903},
	keywords = {⛔ No {DOI} found},
	file = {Veličković et al_2018_Graph Attention Networks.pdf:/data/zotero/storage/J2EGEYQ3/Veličković et al_2018_Graph Attention Networks.pdf:application/pdf;arXiv.org Snapshot:/data/zotero/storage/8WYQG8PV/1710.html:text/html}
}

@inproceedings{malik_nl2type_2019,
	title = {{NL}2Type: Inferring {JavaScript} Function Types from Natural Language Information},
	doi = {10/gg3j6v},
	shorttitle = {{NL}2Type},
	abstract = {{JavaScript} is dynamically typed and hence lacks the type safety of statically typed languages, leading to suboptimal {IDE} support, difficult to understand {APIs}, and unexpected runtime behavior. Several gradual type systems have been proposed, e.g., Flow and {TypeScript}, but they rely on developers to annotate code with types. This paper presents {NL}2Type, a learning-based approach for predicting likely type signatures of {JavaScript} functions. The key idea is to exploit natural language information in source code, such as comments, function names, and parameter names, a rich source of knowledge that is typically ignored by type inference algorithms. We formulate the problem of predicting types as a classification problem and train a recurrent, {LSTM}-based neural model that, after learning from an annotated code base, predicts function types for unannotated code. We evaluate the approach with a corpus of 162,673 {JavaScript} files from real-world projects. {NL}2Type predicts types with a precision of 84.1\% and a recall of 78.9\% when considering only the top-most suggestion, and with a precision of 95.5\% and a recall of 89.6\% when considering the top-5 suggestions. The approach outperforms both {JSNice}, a state-of-the-art approach that analyzes implementations of functions instead of natural language information, and {DeepTyper}, a recent type prediction approach that is also based on deep learning. Beyond predicting types, {NL}2Type serves as a consistency checker for existing type annotations. We show that it discovers 39 inconsistencies that deserve developer attention (from a manual analysis of 50 warnings), most of which are due to incorrect type annotations.},
	eventtitle = {2019 {IEEE}/{ACM} 41st International Conference on Software Engineering ({ICSE})},
	pages = {304--315},
	booktitle = {2019 {IEEE}/{ACM} 41st International Conference on Software Engineering ({ICSE})},
	author = {Malik, Rabee Sohail and Patra, Jibesh and Pradel, Michael},
	date = {2019-05},
	note = {{ISSN}: 1558-1225},
	keywords = {skimmed},
	file = {Malik et al_2019_NL2Type.pdf:/data/zotero/storage/SJ96GUCG/Malik et al_2019_NL2Type.pdf:application/pdf;IEEE Xplore Abstract Record:/data/zotero/storage/6AJAV9TV/8811893.html:text/html}
}

@inproceedings{raychev_learning_2016,
	location = {St. Petersburg, {FL}, {USA}},
	title = {Learning programs from noisy data},
	isbn = {978-1-4503-3549-2},
	url = {https://doi.org/10.1145/2837614.2837671},
	doi = {10/gg3j6x},
	series = {{POPL} '16},
	abstract = {We present a new approach for learning programs from noisy datasets. Our approach is based on two new concepts: a regularized program generator which produces a candidate program based on a small sample of the entire dataset while avoiding overfitting, and a dataset sampler which carefully samples the dataset by leveraging the candidate program's score on that dataset. The two components are connected in a continuous feedback-directed loop. We show how to apply this approach to two settings: one where the dataset has a bound on the noise, and another without a noise bound. The second setting leads to a new way of performing approximate empirical risk minimization on hypotheses classes formed by a discrete search space. We then present two new kinds of program synthesizers which target the two noise settings. First, we introduce a novel regularized bitstream synthesizer that successfully generates programs even in the presence of incorrect examples. We show that the synthesizer can detect errors in the examples while combating overfitting -- a major problem in existing synthesis techniques. We also show how the approach can be used in a setting where the dataset grows dynamically via new examples (e.g., provided by a human). Second, we present a novel technique for constructing statistical code completion systems. These are systems trained on massive datasets of open source programs, also known as ``Big Code''. The key idea is to introduce a domain specific language ({DSL}) over trees and to learn functions in that {DSL} directly from the dataset. These learned functions then condition the predictions made by the system. This is a flexible and powerful technique which generalizes several existing works as we no longer need to decide a priori on what the prediction should be conditioned (another benefit is that the learned functions are a natural mechanism for explaining the prediction). As a result, our code completion system surpasses the prediction capabilities of existing, hard-wired systems.},
	pages = {761--774},
	booktitle = {Proceedings of the 43rd Annual {ACM} {SIGPLAN}-{SIGACT} Symposium on Principles of Programming Languages},
	publisher = {Association for Computing Machinery},
	author = {Raychev, Veselin and Bielik, Pavol and Vechev, Martin and Krause, Andreas},
	urldate = {2020-06-29},
	date = {2016-01-11},
	keywords = {read},
	file = {Raychev et al_2016_Learning programs from noisy data.pdf:/data/zotero/storage/2YTEDNMS/Raychev et al_2016_Learning programs from noisy data.pdf:application/pdf}
}

@inproceedings{hellendoorn_are_2017,
	location = {Paderborn, Germany},
	title = {Are deep neural networks the best choice for modeling source code?},
	isbn = {978-1-4503-5105-8},
	url = {https://doi.org/10.1145/3106237.3106290},
	doi = {10/gg3j53},
	series = {{ESEC}/{FSE} 2017},
	abstract = {Current statistical language modeling techniques, including deep-learning based models, have proven to be quite effective for source code. We argue here that the special properties of source code can be exploited for further improvements. In this work, we enhance established language modeling approaches to handle the special challenges of modeling source code, such as: frequent changes, larger, changing vocabularies, deeply nested scopes, etc. We present a fast, nested language modeling toolkit specifically designed for software, with the ability to add \& remove text, and mix \& swap out many models. Specifically, we improve upon prior cache-modeling work and present a model with a much more expansive, multi-level notion of locality that we show to be well-suited for modeling software. We present results on varying corpora in comparison with traditional N-gram, as well as {RNN}, and {LSTM} deep-learning language models, and release all our source code for public use. Our evaluations suggest that carefully adapting N-gram models for source code can yield performance that surpasses even {RNN} and {LSTM} based deep-learning models.},
	pages = {763--773},
	booktitle = {Proceedings of the 2017 11th Joint Meeting on Foundations of Software Engineering},
	publisher = {Association for Computing Machinery},
	author = {Hellendoorn, Vincent J. and Devanbu, Premkumar},
	urldate = {2020-06-29},
	date = {2017-08-21},
	keywords = {read},
	file = {Hellendoorn_Devanbu_2017_Are deep neural networks the best choice for modeling source code.pdf:/data/zotero/storage/W9PCB95A/Hellendoorn_Devanbu_2017_Are deep neural networks the best choice for modeling source code.pdf:application/pdf}
}

@inproceedings{sen_jalangi_2013,
	location = {Saint Petersburg, Russia},
	title = {Jalangi: a selective record-replay and dynamic analysis framework for {JavaScript}},
	isbn = {978-1-4503-2237-9},
	url = {https://doi.org/10.1145/2491411.2491447},
	doi = {10/gg3j6q},
	series = {{ESEC}/{FSE} 2013},
	shorttitle = {Jalangi},
	abstract = {{JavaScript} is widely used for writing client-side web applications and is getting increasingly popular for writing mobile applications. However, unlike C, C++, and Java, there are not that many tools available for analysis and testing of {JavaScript} applications. In this paper, we present a simple yet powerful framework, called Jalangi, for writing heavy-weight dynamic analyses. Our framework incorporates two key techniques: 1) selective record-replay, a technique which enables to record and to faithfully replay a user-selected part of the program, and 2) shadow values and shadow execution, which enables easy implementation of heavy-weight dynamic analyses. Our implementation makes no special assumption about {JavaScript}, which makes it applicable to real-world {JavaScript} programs running on multiple platforms. We have implemented concolic testing, an analysis to track origins of nulls and undefined, a simple form of taint analysis, an analysis to detect likely type inconsistencies, and an object allocation profiler in Jalangi. Our evaluation of Jalangi on the {SunSpider} benchmark suite and on five web applications shows that Jalangi has an average slowdown of 26X during recording and 30X slowdown during replay and analysis. The slowdowns are comparable with slowdowns reported for similar tools, such as {PIN} and Valgrind for x86 binaries. We believe that the techniques proposed in this paper are applicable to other dynamic languages.},
	pages = {488--498},
	booktitle = {Proceedings of the 2013 9th Joint Meeting on Foundations of Software Engineering},
	publisher = {Association for Computing Machinery},
	author = {Sen, Koushik and Kalasapur, Swaroop and Brutch, Tasneem and Gibbs, Simon},
	urldate = {2020-06-29},
	date = {2013-08-18},
	file = {Sen et al_2013_Jalangi.pdf:/data/zotero/storage/2NUS865H/Sen et al_2013_Jalangi.pdf:application/pdf}
}

@article{andreasen_survey_2017,
	title = {A Survey of Dynamic Analysis and Test Generation for {JavaScript}},
	volume = {50},
	issn = {0360-0300},
	url = {https://doi.org/10.1145/3106739},
	doi = {10/gc5phq},
	abstract = {{JavaScript} has become one of the most prevalent programming languages. Unfortunately, some of the unique properties that contribute to this popularity also make {JavaScript} programs prone to errors and difficult for program analyses to reason about. These properties include the highly dynamic nature of the language, a set of unusual language features, a lack of encapsulation mechanisms, and the “no crash” philosophy. This article surveys dynamic program analysis and test generation techniques for {JavaScript} targeted at improving the correctness, reliability, performance, security, and privacy of {JavaScript}-based software.},
	pages = {66:1--66:36},
	number = {5},
	journaltitle = {{ACM} Computing Surveys},
	shortjournal = {{ACM} Comput. Surv.},
	author = {Andreasen, Esben and Gong, Liang and Møller, Anders and Pradel, Michael and Selakovic, Marija and Sen, Koushik and Staicu, Cristian-Alexandru},
	urldate = {2020-06-29},
	date = {2017-09-26},
	file = {Andreasen et al_2017_A Survey of Dynamic Analysis and Test Generation for JavaScript.pdf:/data/zotero/storage/EZPNN4CE/Andreasen et al_2017_A Survey of Dynamic Analysis and Test Generation for JavaScript.pdf:application/pdf}
}

@inproceedings{pradel_typedevil_2015,
	title = {{TypeDevil}: Dynamic Type Inconsistency Analysis for {JavaScript}},
	volume = {1},
	doi = {10/gg3j7j},
	shorttitle = {{TypeDevil}},
	abstract = {Dynamic languages, such as {JavaScript}, give programmers the freedom to ignore types, and enable them to write concise code in short time. Despite this freedom, many programs follow implicit type rules, for example, that a function has a particular signature or that a property has a particular type. Violations of such implicit type rules often correlate with problems in the program. This paper presents Type Devil, a mostly dynamic analysis that warns developers about inconsistent types. The key idea is to assign a set of observed types to each variable, property, and function, to merge types based in their structure, and to warn developers about variables, properties, and functions that have inconsistent types. To deal with the pervasiveness of polymorphic behavior in real-world {JavaScript} programs, we present a set of techniques to remove spurious warnings and to merge related warnings. Applying Type Devil to widely used benchmark suites and real-world web applications reveals 15 problematic type inconsistencies, including correctness problems, performance problems, and dangerous coding practices.},
	eventtitle = {2015 {IEEE}/{ACM} 37th {IEEE} International Conference on Software Engineering},
	pages = {314--324},
	booktitle = {2015 {IEEE}/{ACM} 37th {IEEE} International Conference on Software Engineering},
	author = {Pradel, Michael and Schuh, Parker and Sen, Koushik},
	date = {2015-05},
	note = {{ISSN}: 1558-1225},
	file = {Pradel et al_2015_TypeDevil.pdf:/data/zotero/storage/CEBQEVJL/Pradel et al_2015_TypeDevil.pdf:application/pdf;IEEE Xplore Abstract Record:/data/zotero/storage/KWK7IJGJ/7194584.html:text/html}
}

@inproceedings{andreasen_trace_2016,
	location = {Dagstuhl, Germany},
	title = {Trace Typing: An Approach for Evaluating Retrofitted Type Systems},
	volume = {56},
	isbn = {978-3-95977-014-9},
	url = {http://drops.dagstuhl.de/opus/volltexte/2016/6095},
	doi = {10/gg3j7g},
	series = {Leibniz International Proceedings in Informatics ({LIPIcs})},
	shorttitle = {Trace Typing},
	pages = {1:1--1:26},
	booktitle = {30th European Conference on Object-Oriented Programming ({ECOOP} 2016)},
	publisher = {Schloss Dagstuhl–Leibniz-Zentrum fuer Informatik},
	author = {Andreasen, Esben and Gordon, Colin S. and Chandra, Satish and Sridharan, Manu and Tip, Frank and Sen, Koushik},
	editor = {Krishnamurthi, Shriram and Lerner, Benjamin S.},
	urldate = {2020-06-29},
	date = {2016},
	note = {{ISSN}: 1868-8969},
	file = {Snapshot:/data/zotero/storage/J9I33BF8/6095.html:text/html;Andreasen et al_2016_Trace Typing.pdf:/data/zotero/storage/D59NKHFM/Andreasen et al_2016_Trace Typing.pdf:application/pdf}
}

@inproceedings{brockschmidt_learning_2017,
	location = {Cham},
	title = {Learning Shape Analysis},
	isbn = {978-3-319-66706-5},
	doi = {10/gg3j6r},
	series = {Lecture Notes in Computer Science},
	abstract = {We present a data-driven verification framework to automatically prove memory safety of heap-manipulating programs. Our core contribution is a novel statistical machine learning technique that maps observed program states to (possibly disjunctive) separation logic formulas describing the invariant shape of (possibly nested) data structures at relevant program locations. We then attempt to verify these predictions using a program verifier, where counterexamples to a predicted invariant are used as additional input to the shape predictor in a refinement loop. We have implemented our techniques in Locust, an extension of the {GRASShopper} verification tool. Locust is able to automatically prove memory safety of implementations of classical heap-manipulating programs such as insertionsort, quicksort and traversals of nested data structures.},
	pages = {66--87},
	booktitle = {Static Analysis},
	publisher = {Springer International Publishing},
	author = {Brockschmidt, Marc and Chen, Yuxin and Kohli, Pushmeet and Krishna, Siddharth and Tarlow, Daniel},
	editor = {Ranzato, Francesco},
	date = {2017},
	langid = {english},
	file = {Brockschmidt et al_2017_Learning Shape Analysis.pdf:/data/zotero/storage/2QZC3A6B/Brockschmidt et al_2017_Learning Shape Analysis.pdf:application/pdf}
}

@article{chistyakov_semantic_2018,
	title = {Semantic embeddings for program behavior patterns},
	url = {http://arxiv.org/abs/1804.03635},
	abstract = {In this paper, we propose a new feature extraction technique for program execution logs. First, we automatically extract complex patterns from a program's behavior graph. Then, we embed these patterns into a continuous space by training an autoencoder. We evaluate the proposed features on a real-world malicious software detection task. We also find that the embedding space captures interpretable structures in the space of pattern parts.},
	journaltitle = {{arXiv}:1804.03635 [cs, stat]},
	author = {Chistyakov, Alexander and Lobacheva, Ekaterina and Kuznetsov, Arseny and Romanenko, Alexey},
	urldate = {2020-06-29},
	date = {2018-04-10},
	eprinttype = {arxiv},
	eprint = {1804.03635},
	keywords = {⛔ No {DOI} found},
	file = {Chistyakov et al_2018_Semantic embeddings for program behavior patterns.pdf:/data/zotero/storage/3WHTR8SU/Chistyakov et al_2018_Semantic embeddings for program behavior patterns.pdf:application/pdf;arXiv.org Snapshot:/data/zotero/storage/BMPQ9T59/1804.html:text/html}
}

@inproceedings{allamanis_suggesting_2015,
	location = {Bergamo, Italy},
	title = {Suggesting accurate method and class names},
	isbn = {978-1-4503-3675-8},
	url = {https://doi.org/10.1145/2786805.2786849},
	doi = {10/gf8np5},
	series = {{ESEC}/{FSE} 2015},
	abstract = {Descriptive names are a vital part of readable, and hence maintainable, code. Recent progress on automatically suggesting names for local variables tantalizes with the prospect of replicating that success with method and class names. However, suggesting names for methods and classes is much more difficult. This is because good method and class names need to be functionally descriptive, but suggesting such names requires that the model goes beyond local context. We introduce a neural probabilistic language model for source code that is specifically designed for the method naming problem. Our model learns which names are semantically similar by assigning them to locations, called embeddings, in a high-dimensional continuous space, in such a way that names with similar embeddings tend to be used in similar contexts. These embeddings seem to contain semantic information about tokens, even though they are learned only from statistical co-occurrences of tokens. Furthermore, we introduce a variant of our model that is, to our knowledge, the first that can propose neologisms, names that have not appeared in the training corpus. We obtain state of the art results on the method, class, and even the simpler variable naming tasks. More broadly, the continuous embeddings that are learned by our model have the potential for wide application within software engineering.},
	pages = {38--49},
	booktitle = {Proceedings of the 2015 10th Joint Meeting on Foundations of Software Engineering},
	publisher = {Association for Computing Machinery},
	author = {Allamanis, Miltiadis and Barr, Earl T. and Bird, Christian and Sutton, Charles},
	urldate = {2020-06-29},
	date = {2015-08-30},
	file = {Allamanis et al_2015_Suggesting accurate method and class names.pdf:/data/zotero/storage/JHV9RRUX/Allamanis et al_2015_Suggesting accurate method and class names.pdf:application/pdf}
}

@inproceedings{habib_finding_2016,
	location = {Amsterdam, Netherlands},
	title = {Finding concurrency bugs using graph-based anomaly detection in big code},
	isbn = {978-1-4503-4437-1},
	url = {https://doi.org/10.1145/2984043.2998542},
	doi = {10/gg3j6j},
	series = {{SPLASH} Companion 2016},
	abstract = {Concurrency bugs are very difficult and subtle to discover, reproduce, and fix. Many techniques have been devised by the academic as well as the industry communities to find these bugs. However, most of the effective techniques tend to focus on a subset of the various concurrency bugs types. We propose a new generic concurrency bug detection technique that leverages "Big Code": millions of lines of code freely available on code repositories. Our approach tries to learn the properties of what constitutes a good and a bad synchronization pattern from hundreds of concurrent software using graph-based anomaly detection.},
	pages = {55--56},
	booktitle = {Companion Proceedings of the 2016 {ACM} {SIGPLAN} International Conference on Systems, Programming, Languages and Applications: Software for Humanity},
	publisher = {Association for Computing Machinery},
	author = {Habib, Andrew},
	urldate = {2020-06-29},
	date = {2016-10-20},
	file = {Habib_2016_Finding concurrency bugs using graph-based anomaly detection in big code.pdf:/data/zotero/storage/BHFWSR3H/Habib_2016_Finding concurrency bugs using graph-based anomaly detection in big code.pdf:application/pdf}
}

@inproceedings{he_debin_2018,
	location = {Toronto, Canada},
	title = {Debin: Predicting Debug Information in Stripped Binaries},
	isbn = {978-1-4503-5693-0},
	url = {https://doi.org/10.1145/3243734.3243866},
	doi = {10/gg3j6c},
	series = {{CCS} '18},
	shorttitle = {Debin},
	abstract = {We present a novel approach for predicting debug information in stripped binaries. Using machine learning, we first train probabilistic models on thousands of non-stripped binaries and then use these models to predict properties of meaningful elements in unseen stripped binaries. Our focus is on recovering symbol names, types and locations, which are critical source-level information wiped off during compilation and stripping. Our learning approach is able to distinguish and extract key elements such as register-allocated and memory-allocated variables usually not evident in the stripped binary. To predict names and types of extracted elements, we use scalable structured prediction algorithms in probabilistic graphical models with an extensive set of features which capture key characteristics of binary code. Based on this approach, we implemented an automated tool, called Debin, which handles {ELF} binaries on three of the most popular architectures: x86, x64 and {ARM}. Given a stripped binary, Debin outputs a binary augmented with the predicted debug information. Our experimental results indicate that Debin is practically useful: for x64, it predicts symbol names and types with 68.8\% precision and 68.3\% recall. We also show that Debin is helpful for the task of inspecting real-world malware -- it revealed suspicious library usage and behaviors such as {DNS} resolver reader.},
	pages = {1667--1680},
	booktitle = {Proceedings of the 2018 {ACM} {SIGSAC} Conference on Computer and Communications Security},
	publisher = {Association for Computing Machinery},
	author = {He, Jingxuan and Ivanov, Pesho and Tsankov, Petar and Raychev, Veselin and Vechev, Martin},
	urldate = {2020-06-29},
	date = {2018-01-15},
	file = {He et al_2018_Debin.pdf:/data/zotero/storage/SEX6U6DY/He et al_2018_Debin.pdf:application/pdf}
}

@article{heo_adaptive_2018,
	title = {Adaptive Static Analysis via Learning with Bayesian Optimization},
	volume = {40},
	issn = {0164-0925},
	url = {https://doi.org/10.1145/3121135},
	doi = {10/gg3j5x},
	abstract = {Building a cost-effective static analyzer for real-world programs is still regarded an art. One key contributor to this grim reputation is the difficulty in balancing the cost and the precision of an analyzer. An ideal analyzer should be adaptive to a given analysis task and avoid using techniques that unnecessarily improve precision and increase analysis cost. However, achieving this ideal is highly nontrivial, and it requires a large amount of engineering efforts. In this article, we present a new learning-based approach for adaptive static analysis. In our approach, the analysis includes a sophisticated parameterized strategy that decides, for each part of a given program, whether to apply a precision-improving technique to that part or not. We present a method for learning a good parameter for such a strategy from an existing codebase via Bayesian optimization. The learnt strategy is then used for new, unseen programs. Using our approach, we developed partially flow- and context-sensitive variants of a realistic C static analyzer. The experimental results demonstrate that using Bayesian optimization is crucial for learning from an existing codebase. Also, they show that among all program queries that require flow- or context-sensitivity, our partially flow- and context-sensitive analysis answers 75\% of them, while increasing the analysis cost only by 3.3× of the baseline flow- and context-insensitive analysis, rather than 40× or more of the fully sensitive version.},
	pages = {14:1--14:37},
	number = {4},
	journaltitle = {{ACM} Transactions on Programming Languages and Systems},
	shortjournal = {{ACM} Trans. Program. Lang. Syst.},
	author = {Heo, Kihong and Oh, Hakjoo and Yang, Hongseok and Yi, Kwangkeun},
	urldate = {2020-06-29},
	date = {2018-11-16},
	file = {Heo et al_2018_Adaptive Static Analysis via Learning with Bayesian Optimization.pdf:/data/zotero/storage/RY5DW2UG/Heo et al_2018_Adaptive Static Analysis via Learning with Bayesian Optimization.pdf:application/pdf}
}

@inproceedings{katz_estimating_2016,
	location = {St. Petersburg, {FL}, {USA}},
	title = {Estimating types in binaries using predictive modeling},
	isbn = {978-1-4503-3549-2},
	url = {https://doi.org/10.1145/2837614.2837674},
	doi = {10/gg3j6g},
	series = {{POPL} '16},
	abstract = {Reverse engineering is an important tool in mitigating vulnerabilities in binaries. As a lot of software is developed in object-oriented languages, reverse engineering of object-oriented code is of critical importance. One of the major hurdles in reverse engineering binaries compiled from object-oriented code is the use of dynamic dispatch. In the absence of debug information, any dynamic dispatch may seem to jump to many possible targets, posing a significant challenge to a reverse engineer trying to track the program flow. We present a novel technique that allows us to statically determine the likely targets of virtual function calls. Our technique uses object tracelets – statically constructed sequences of operations performed on an object – to capture potential runtime behaviors of the object. Our analysis automatically pre-labels some of the object tracelets by relying on instances where the type of an object is known. The resulting type-labeled tracelets are then used to train a statistical language model ({SLM}) for each type.We then use the resulting ensemble of {SLMs} over unlabeled tracelets to generate a ranking of their most likely types, from which we deduce the likely targets of dynamic dispatches.We have implemented our technique and evaluated it over real-world C++ binaries. Our evaluation shows that when there are multiple alternative targets, our approach can drastically reduce the number of targets that have to be considered by a reverse engineer.},
	pages = {313--326},
	booktitle = {Proceedings of the 43rd Annual {ACM} {SIGPLAN}-{SIGACT} Symposium on Principles of Programming Languages},
	publisher = {Association for Computing Machinery},
	author = {Katz, Omer and El-Yaniv, Ran and Yahav, Eran},
	urldate = {2020-06-29},
	date = {2016-01-11},
	file = {Katz et al_2016_Estimating types in binaries using predictive modeling.pdf:/data/zotero/storage/6BDEFPNB/Katz et al_2016_Estimating types in binaries using predictive modeling.pdf:application/pdf}
}

@inproceedings{katz_statistical_2018,
	location = {Williamsburg, {VA}, {USA}},
	title = {Statistical Reconstruction of Class Hierarchies in Binaries},
	isbn = {978-1-4503-4911-6},
	url = {https://doi.org/10.1145/3173162.3173202},
	doi = {10/gg3j7f},
	series = {{ASPLOS} '18},
	abstract = {We address a fundamental problem in reverse engineering of object-oriented code: the reconstruction of a program's class hierarchy from its stripped binary. Existing approaches rely heavily on structural information that is not always available, e.g., calls to parent constructors. As a result, these approaches often leave gaps in the hierarchies they construct, or fail to construct them altogether. Our main insight is that behavioral information can be used to infer subclass/superclass relations, supplementing any missing structural information. Thus, we propose the first statistical approach for static reconstruction of class hierarchies based on behavioral similarity. We capture the behavior of each type using a statistical language model ({SLM}), define a metric for pairwise similarity between types based on the Kullback-Leibler divergence between their {SLMs}, and lift it to determine the most likely class hierarchy. We implemented our approach in a tool called {ROCK} and used it to automatically reconstruct the class hierarchies of several real-world stripped C++ binaries. Our results demonstrate that {ROCK} obtained significantly more accurate class hierarchies than those obtained using structural analysis alone.},
	pages = {363--376},
	booktitle = {Proceedings of the Twenty-Third International Conference on Architectural Support for Programming Languages and Operating Systems},
	publisher = {Association for Computing Machinery},
	author = {Katz, Omer and Rinetzky, Noam and Yahav, Eran},
	urldate = {2020-06-29},
	date = {2018-03-19},
	file = {Katz et al_2018_Statistical Reconstruction of Class Hierarchies in Binaries.pdf:/data/zotero/storage/AXP6TBEI/Katz et al_2018_Statistical Reconstruction of Class Hierarchies in Binaries.pdf:application/pdf}
}

@inproceedings{li_detecting_2016,
	location = {Cham},
	title = {Detecting Similar Programs via The Weisfeiler-Leman Graph Kernel},
	isbn = {978-3-319-35122-3},
	doi = {10/gg3j6d},
	series = {Lecture Notes in Computer Science},
	abstract = {With the increasing availability of source code on the Internet, many new approaches to retrieve, repair, and reuse code have emerged that rely on the ability to efficiently compute the similarity of two pieces of code. The meaning of similarity, however, heavily depends on the application domain. For predicting {API} calls, for example, programs can be considered similar if they call a specific set of functions in a similar way, while for automated bug fixing, it is important that similar programs share a similar data-flow.In this paper, we propose an algorithm to compute program similarity based on the Weisfeiler-Leman graph kernel. Our algorithm is able to operate on different graph-based representations of programs and thus can be applied in different domains. We show the usefulness of our approach in two experiments using data-flow similarity and {API}-call similarity.},
	pages = {315--330},
	booktitle = {Software Reuse: Bridging with Social-Awareness},
	publisher = {Springer International Publishing},
	author = {Li, Wenchao and Saidi, Hassen and Sanchez, Huascar and Schäf, Martin and Schweitzer, Pascal},
	editor = {Kapitsaki, Georgia M. and Santana de Almeida, Eduardo},
	date = {2016},
	langid = {english},
	keywords = {out-of-scope},
	file = {Li et al_2016_Detecting Similar Programs via The Weisfeiler-Leman Graph Kernel.pdf:/data/zotero/storage/CSMC4ZDP/Li et al_2016_Detecting Similar Programs via The Weisfeiler-Leman Graph Kernel.pdf:application/pdf}
}

@online{bourgeois_learning_2019,
	title = {Learning Representations of Source Code from Structure and Context},
	url = {https://infoscience.epfl.ch/record/277163},
	abstract = {Large codebases are routinely indexed by standard Information Retrieval systems, starting from the assumption that code written by humans shows similar statistical properties to written text [Hindle et al., 2012]. While those {IR} systems are still relatively successful inside companies to help developers search on their proprietary codebase, the same cannot be said about most of public platforms: throughout the years many notable names (Google Code Search, Koders, Ohloh, etc.) have been shut down. The limited functionalities offered, combined with the low quality of the results, did not attract a critical mass of users to justify running those services. To this date, even {GitHub} (arguably the largest code repository in the world) offers search functionalities that are not more innovative than those present in platforms from the past decade. We argue that the reason why this happens has happened can be imputed to the fundamental limitation of mining information exclusively from the textual representation of the code. Developing a more powerful representation of code will not only enable a new generation of search systems, but will also allow us to explore code by functional similarity, i.e., searching for blocks of code which accomplish similar (and not strictly equivalent) tasks. In this thesis, we want to explore the opportunities provided by a multimodal representation of code: (1) hierarchical (both in terms of object and package hierarchy), (2) syntactical (leveraging the Abstract Syntax Tree representation of code), (3) distributional (embedding by means of co-occurrences), and (4) textual (mining the code documentation). Our goal is to distill as much information as possible from the complex nature of code. Recent advances in deep learning are providing a new set of techniques that we plan to employ for the different modes, for instance Poincaré Embeddings [Nickel and Kiela, 2017] for (1) hierarchical, and Gated Graph {NNs} [Li et al., 2016] for (2) syntactical. Last but not the least, learning multimodal similarity [{McFee} and Lanckriet, 2011] is an ulterior research challenge, especially at the scale of large codebases – we will explore the opportunities offered by a framework like {GraphSAGE} [Hamilton et al., 2017] to harmonize a large graph with rich feature information.},
	titleaddon = {Infoscience},
	author = {Bourgeois, Dylan},
	urldate = {2020-06-29},
	date = {2019-03-15},
	langid = {english},
	note = {Library Catalog: infoscience.epfl.ch
Number: {STUDENT}},
	file = {Bourgeois_2019_Learning Representations of Source Code from Structure and Context.pdf:/data/zotero/storage/AEVV85EZ/Bourgeois_2019_Learning Representations of Source Code from Structure and Context.pdf:application/pdf;Snapshot:/data/zotero/storage/M6NZWADR/277163.pdf:application/pdf}
}

@incollection{gupta_neural_2019,
	title = {Neural Attribution for Semantic Bug-Localization in Student Programs},
	url = {http://papers.nips.cc/paper/9358-neural-attribution-for-semantic-bug-localization-in-student-programs.pdf},
	pages = {11884--11894},
	booktitle = {Advances in Neural Information Processing Systems 32},
	publisher = {Curran Associates, Inc.},
	author = {Gupta, Rahul and Kanade, Aditya and Shevade, Shirish},
	editor = {Wallach, H. and Larochelle, H. and Beygelzimer, A. and Alché-Buc, F. d\{{\textbackslash}textbackslash\}textquotesingle and Fox, E. and Garnett, R.},
	urldate = {2020-06-29},
	date = {2019},
	file = {Gupta et al_2019_Neural Attribution for Semantic Bug-Localization in Student Programs.pdf:/data/zotero/storage/QS65Q2NC/Gupta et al_2019_Neural Attribution for Semantic Bug-Localization in Student Programs.pdf:application/pdf;NIPS Snapshot:/data/zotero/storage/KXBDBBMF/9358-neural-attribution-for-semantic-bug-localization-in-student-programs.html:text/html}
}

@inproceedings{azcona_user2code2vec_2019,
	location = {Tempe, {AZ}, {USA}},
	title = {user2code2vec: Embeddings for Profiling Students Based on Distributional Representations of Source Code},
	isbn = {978-1-4503-6256-6},
	url = {https://doi.org/10.1145/3303772.3303813},
	doi = {10/gg3j7k},
	series = {{LAK}19},
	shorttitle = {user2code2vec},
	abstract = {In this work, we propose a new methodology to profile individual students of computer science based on their programming design using a technique called embeddings. We investigate different approaches to analyze user source code submissions in the Python language. We compare the performances of different source code vectorization techniques to predict the correctness of a code submission. In addition, we propose a new mechanism to represent students based on their code submissions for a given set of laboratory tasks on a particular course. This way, we can make deeper recommendations for programming solutions and pathways to support student learning and progression in computer programming modules effectively at a Higher Education Institution. Recent work using Deep Learning tends to work better when more and more data is provided. However, in Learning Analytics, the number of students in a course is an unavoidable limit. Thus we cannot simply generate more data as is done in other domains such as {FinTech} or Social Network Analysis. Our findings indicate there is a need to learn and develop better mechanisms to extract and learn effective data features from students so as to analyze the students' progression and performance effectively.},
	pages = {86--95},
	booktitle = {Proceedings of the 9th International Conference on Learning Analytics \& Knowledge},
	publisher = {Association for Computing Machinery},
	author = {Azcona, David and Arora, Piyush and Hsiao, I-Han and Smeaton, Alan},
	urldate = {2020-06-29},
	date = {2019-03-04},
	file = {Azcona et al_2019_user2code2vec.pdf:/data/zotero/storage/VW653GRD/Azcona et al_2019_user2code2vec.pdf:application/pdf}
}

@article{arabshahi_combining_2018,
	title = {Combining Symbolic Expressions and Black-box Function Evaluations in Neural Programs},
	url = {https://arxiv.org/abs/1801.04342v3},
	abstract = {Neural programming involves training neural networks to learn programs,
mathematics, or logic from data. Previous works have failed to achieve good
generalization performance, especially on problems and programs with high
complexity or on large domains. This is because they mostly rely either on
black-box function evaluations that do not capture the structure of the
program, or on detailed execution traces that are expensive to obtain, and
hence the training data has poor coverage of the domain under consideration. We
present a novel framework that utilizes black-box function evaluations, in
conjunction with symbolic expressions that define relationships between the
given functions. We employ tree {LSTMs} to incorporate the structure of the
symbolic expression trees. We use tree encoding for numbers present in function
evaluation data, based on their decimal representation. We present an
evaluation benchmark for this task to demonstrate our proposed model combines
symbolic reasoning and function evaluation in a fruitful manner, obtaining high
accuracies in our experiments. Our framework generalizes significantly better
to expressions of higher depth and is able to fill partial equations with valid
completions.},
	author = {Arabshahi, Forough and Singh, Sameer and Anandkumar, Animashree},
	urldate = {2020-06-29},
	date = {2018-01-12},
	langid = {english},
	keywords = {theory, ⛔ No {DOI} found},
	file = {Arabshahi et al_2018_Combining Symbolic Expressions and Black-box Function Evaluations in Neural.pdf:/data/zotero/storage/C3WANNVS/Arabshahi et al_2018_Combining Symbolic Expressions and Black-box Function Evaluations in Neural.pdf:application/pdf;Snapshot:/data/zotero/storage/7ZWVGPZJ/1801.html:text/html}
}

@inproceedings{allamanis_learning_2017,
	title = {Learning Continuous Semantic Representations of Symbolic Expressions},
	url = {http://proceedings.mlr.press/v70/allamanis17a.html},
	abstract = {Combining abstract, symbolic reasoning with continuous neural reasoning is a grand challenge of representation learning. As a step in this direction, we propose a new architecture, called neural eq...},
	eventtitle = {International Conference on Machine Learning},
	pages = {80--88},
	booktitle = {International Conference on Machine Learning},
	author = {Allamanis, Miltiadis and Chanthirasegaran, Pankajan and Kohli, Pushmeet and Sutton, Charles},
	urldate = {2020-06-29},
	date = {2017-07-17},
	langid = {english},
	note = {{ISSN}: 1938-7228
Section: Machine Learning},
	file = {Allamanis et al_2017_Learning Continuous Semantic Representations of Symbolic Expressions.pdf:/data/zotero/storage/AATHK5H2/Allamanis et al_2017_Learning Continuous Semantic Representations of Symbolic Expressions.pdf:application/pdf;Snapshot:/data/zotero/storage/TVU7WSRE/allamanis17a.html:text/html}
}

@article{balog_deepcoder_2017,
	title = {{DeepCoder}: Learning to Write Programs},
	url = {http://arxiv.org/abs/1611.01989},
	shorttitle = {{DeepCoder}},
	abstract = {We develop a first line of attack for solving programming competition-style problems from input-output examples using deep learning. The approach is to train a neural network to predict properties of the program that generated the outputs from the inputs. We use the neural network's predictions to augment search techniques from the programming languages community, including enumerative search and an {SMT}-based solver. Empirically, we show that our approach leads to an order of magnitude speedup over the strong non-augmented baselines and a Recurrent Neural Network approach, and that we are able to solve problems of difficulty comparable to the simplest problems on programming competition websites.},
	journaltitle = {{arXiv}:1611.01989 [cs]},
	author = {Balog, Matej and Gaunt, Alexander L. and Brockschmidt, Marc and Nowozin, Sebastian and Tarlow, Daniel},
	urldate = {2020-06-29},
	date = {2017-03-08},
	eprinttype = {arxiv},
	eprint = {1611.01989},
	keywords = {⛔ No {DOI} found},
	file = {Balog et al_2017_DeepCoder.pdf:/data/zotero/storage/JZBYXY4D/Balog et al_2017_DeepCoder.pdf:application/pdf;arXiv.org Snapshot:/data/zotero/storage/MTIWTEB5/1611.html:text/html}
}

@article{solar-lezama_combinatorial_2006,
	title = {Combinatorial sketching for finite programs},
	volume = {34},
	issn = {0163-5964},
	url = {https://doi.org/10.1145/1168919.1168907},
	doi = {10/dz9ppd},
	abstract = {Sketching is a software synthesis approach where the programmer develops a partial implementation - a sketch - and a separate specification of the desired functionality. The synthesizer then completes the sketch to behave like the specification. The correctness of the synthesized implementation is guaranteed by the compiler, which allows, among other benefits, rapid development of highly tuned implementations without the fear of introducing bugs.We develop {SKETCH}, a language for finite programs with linguistic support for sketching. Finite programs include many highperformance kernels, including cryptocodes. In contrast to prior synthesizers, which had to be equipped with domain-specific rules, {SKETCH} completes sketches by means of a combinatorial search based on generalized boolean satisfiability. Consequently, our combinatorial synthesizer is complete for the class of finite programs: it is guaranteed to complete any sketch in theory, and in practice has scaled to realistic programming problems.Freed from domain rules, we can now write sketches as simpleto-understand partial programs, which are regular programs in which difficult code fragments are replaced with holes to be filled by the synthesizer. Holes may stand for index expressions, lookup tables, or bitmasks, but the programmer can easily define new kinds of holes using a single versatile synthesis operator.We have used {SKETCH} to synthesize an efficient implementation of the {AES} cipher standard. The synthesizer produces the most complex part of the implementation and runs in about an hour.},
	pages = {404--415},
	number = {5},
	journaltitle = {{ACM} {SIGARCH} Computer Architecture News},
	shortjournal = {{SIGARCH} Comput. Archit. News},
	author = {Solar-Lezama, Armando and Tancau, Liviu and Bodik, Rastislav and Seshia, Sanjit and Saraswat, Vijay},
	urldate = {2020-06-30},
	date = {2006-10-20},
	file = {Solar-Lezama et al_2006_Combinatorial sketching for finite programs.pdf:/data/zotero/storage/CYDMNI5N/Solar-Lezama et al_2006_Combinatorial sketching for finite programs.pdf:application/pdf}
}

@article{padhi_loopinvgen_2019,
	title = {{LoopInvGen}: A Loop Invariant Generator based on Precondition Inference},
	url = {http://arxiv.org/abs/1707.02029},
	shorttitle = {{LoopInvGen}},
	abstract = {We describe the {LoopInvGen} tool for generating loop invariants that can provably guarantee correctness of a program with respect to a given specification. {LoopInvGen} is an efficient implementation of the inference technique originally proposed in our earlier work on {PIE} (https://doi.org/10.1145/2908080.2908099). In contrast to existing techniques, {LoopInvGen} is not restricted to a fixed set of features -- atomic predicates that are composed together to build complex loop invariants. Instead, we start with no initial features, and use program synthesis techniques to grow the set on demand. This not only enables a less onerous and more expressive approach, but also appears to be significantly faster than the existing tools over the {SyGuS}-{COMP} 2018 benchmarks from the {INV} track.},
	journaltitle = {{arXiv}:1707.02029 [cs]},
	author = {Padhi, Saswat and Sharma, Rahul and Millstein, Todd},
	urldate = {2020-06-30},
	date = {2019-10-31},
	eprinttype = {arxiv},
	eprint = {1707.02029},
	keywords = {⛔ No {DOI} found},
	file = {Padhi et al_2019_LoopInvGen.pdf:/data/zotero/storage/N9WPAYND/Padhi et al_2019_LoopInvGen.pdf:application/pdf;arXiv.org Snapshot:/data/zotero/storage/3868M8R2/1707.html:text/html}
}

@inproceedings{allamanis_convolutional_2016,
	title = {A convolutional attention network for extreme summarization of source code},
	pages = {2091--2100},
	booktitle = {International conference on machine learning},
	author = {Allamanis, Miltiadis and Peng, Hao and Sutton, Charles},
	date = {2016},
	keywords = {⛔ No {DOI} found},
	file = {Allamanis et al_2016_A convolutional attention network for extreme summarization of source code.pdf:/data/zotero/storage/RD4PS2JY/Allamanis et al_2016_A convolutional attention network for extreme summarization of source code.pdf:application/pdf}
}